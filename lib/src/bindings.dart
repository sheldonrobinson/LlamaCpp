// ignore_for_file: always_specify_types
// ignore_for_file: camel_case_types
// ignore_for_file: non_constant_identifier_names

// AUTO GENERATED FILE, DO NOT EDIT.
//
// Generated by `package:ffigen`.
// ignore_for_file: type=lint
@ffi.DefaultAsset('package:lib/src/bindings.dart')
library;

import 'dart:ffi' as ffi;

@ffi.Native<ffi.Void Function(ffi.Pointer<va_list>)>()
external void __va_start(
  ffi.Pointer<va_list> arg0,
);

@ffi.Native<ffi.Void Function()>()
external void __security_init_cookie();

@ffi.Native<ffi.Void Function(ffi.UintPtr)>()
external void __security_check_cookie(
  int _StackCookie,
);

@ffi.Native<ffi.Void Function(ffi.UintPtr)>()
external void __report_gsfailure(
  int _StackCookie,
);

@ffi.Native<ffi.UintPtr>()
external int __security_cookie;

@ffi.Native<ffi.Void Function()>()
external void _invalid_parameter_noinfo();

@ffi.Native<ffi.Void Function()>()
external void _invalid_parameter_noinfo_noreturn();

@ffi.Native<
    ffi.Void Function(ffi.Pointer<ffi.WChar>, ffi.Pointer<ffi.WChar>,
        ffi.Pointer<ffi.WChar>, ffi.UnsignedInt, ffi.UintPtr)>()
external void _invoke_watson(
  ffi.Pointer<ffi.WChar> _Expression,
  ffi.Pointer<ffi.WChar> _FunctionName,
  ffi.Pointer<ffi.WChar> _FileName,
  int _LineNo,
  int _Reserved,
);

@ffi.Native<ffi.Pointer<ffi.Int> Function()>()
external ffi.Pointer<ffi.Int> _errno();

@ffi.Native<errno_t Function(ffi.Int)>()
external int _set_errno(
  int _Value,
);

@ffi.Native<errno_t Function(ffi.Pointer<ffi.Int>)>()
external int _get_errno(
  ffi.Pointer<ffi.Int> _Value,
);

@ffi.Native<ffi.UnsignedLong Function()>()
external int __threadid();

@ffi.Native<ffi.UintPtr Function()>()
external int __threadhandle();

@ffi.Native<ffi.Pointer<FILE> Function(ffi.UnsignedInt)>()
external ffi.Pointer<FILE> __acrt_iob_func(
  int _Ix,
);

@ffi.Native<wint_t Function(ffi.Pointer<FILE>)>()
external int fgetwc(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function()>()
external int _fgetwchar();

@ffi.Native<wint_t Function(ffi.WChar, ffi.Pointer<FILE>)>()
external int fputwc(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function(ffi.WChar)>()
external int _fputwchar(
  int _Character,
);

@ffi.Native<wint_t Function(ffi.Pointer<FILE>)>()
external int getwc(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function()>()
external int getwchar();

@ffi.Native<
    ffi.Pointer<ffi.WChar> Function(
        ffi.Pointer<ffi.WChar>, ffi.Int, ffi.Pointer<FILE>)>()
external ffi.Pointer<ffi.WChar> fgetws(
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.WChar>, ffi.Pointer<FILE>)>()
external int fputws(
  ffi.Pointer<ffi.WChar> _Buffer,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Pointer<ffi.WChar> Function(ffi.Pointer<ffi.WChar>, ffi.Size)>()
external ffi.Pointer<ffi.WChar> _getws_s(
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
);

@ffi.Native<wint_t Function(ffi.WChar, ffi.Pointer<FILE>)>()
external int putwc(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function(ffi.WChar)>()
external int putwchar(
  int _Character,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.WChar>)>()
external int _putws(
  ffi.Pointer<ffi.WChar> _Buffer,
);

@ffi.Native<wint_t Function(wint_t, ffi.Pointer<FILE>)>()
external int ungetwc(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Pointer<FILE> Function(ffi.Int, ffi.Pointer<ffi.WChar>)>()
external ffi.Pointer<FILE> _wfdopen(
  int _FileHandle,
  ffi.Pointer<ffi.WChar> _Mode,
);

@ffi.Native<
    ffi.Pointer<FILE> Function(
        ffi.Pointer<ffi.WChar>, ffi.Pointer<ffi.WChar>)>()
external ffi.Pointer<FILE> _wfopen(
  ffi.Pointer<ffi.WChar> _FileName,
  ffi.Pointer<ffi.WChar> _Mode,
);

@ffi.Native<
    errno_t Function(ffi.Pointer<ffi.Pointer<FILE>>, ffi.Pointer<ffi.WChar>,
        ffi.Pointer<ffi.WChar>)>()
external int _wfopen_s(
  ffi.Pointer<ffi.Pointer<FILE>> _Stream,
  ffi.Pointer<ffi.WChar> _FileName,
  ffi.Pointer<ffi.WChar> _Mode,
);

@ffi.Native<
    ffi.Pointer<FILE> Function(
        ffi.Pointer<ffi.WChar>, ffi.Pointer<ffi.WChar>, ffi.Pointer<FILE>)>()
external ffi.Pointer<FILE> _wfreopen(
  ffi.Pointer<ffi.WChar> _FileName,
  ffi.Pointer<ffi.WChar> _Mode,
  ffi.Pointer<FILE> _OldStream,
);

@ffi.Native<
    errno_t Function(ffi.Pointer<ffi.Pointer<FILE>>, ffi.Pointer<ffi.WChar>,
        ffi.Pointer<ffi.WChar>, ffi.Pointer<FILE>)>()
external int _wfreopen_s(
  ffi.Pointer<ffi.Pointer<FILE>> _Stream,
  ffi.Pointer<ffi.WChar> _FileName,
  ffi.Pointer<ffi.WChar> _Mode,
  ffi.Pointer<FILE> _OldStream,
);

@ffi.Native<
    ffi.Pointer<FILE> Function(
        ffi.Pointer<ffi.WChar>, ffi.Pointer<ffi.WChar>, ffi.Int)>()
external ffi.Pointer<FILE> _wfsopen(
  ffi.Pointer<ffi.WChar> _FileName,
  ffi.Pointer<ffi.WChar> _Mode,
  int _ShFlag,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ffi.WChar>)>()
external void _wperror(
  ffi.Pointer<ffi.WChar> _ErrorMessage,
);

@ffi.Native<
    ffi.Pointer<FILE> Function(
        ffi.Pointer<ffi.WChar>, ffi.Pointer<ffi.WChar>)>()
external ffi.Pointer<FILE> _wpopen(
  ffi.Pointer<ffi.WChar> _Command,
  ffi.Pointer<ffi.WChar> _Mode,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.WChar>)>()
external int _wremove(
  ffi.Pointer<ffi.WChar> _FileName,
);

@ffi.Native<
    ffi.Pointer<ffi.WChar> Function(
        ffi.Pointer<ffi.WChar>, ffi.Pointer<ffi.WChar>)>()
external ffi.Pointer<ffi.WChar> _wtempnam(
  ffi.Pointer<ffi.WChar> _Directory,
  ffi.Pointer<ffi.WChar> _FilePrefix,
);

@ffi.Native<errno_t Function(ffi.Pointer<ffi.WChar>, ffi.Size)>()
external int _wtmpnam_s(
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
);

@ffi.Native<ffi.Pointer<ffi.WChar> Function(ffi.Pointer<ffi.WChar>)>()
external ffi.Pointer<ffi.WChar> _wtmpnam(
  ffi.Pointer<ffi.WChar> _Buffer,
);

@ffi.Native<wint_t Function(ffi.Pointer<FILE>)>()
external int _fgetwc_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function(ffi.WChar, ffi.Pointer<FILE>)>()
external int _fputwc_nolock(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function(ffi.Pointer<FILE>)>()
external int _getwc_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function(ffi.WChar, ffi.Pointer<FILE>)>()
external int _putwc_nolock(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function(wint_t, ffi.Pointer<FILE>)>()
external int _ungetwc_nolock(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<FILE>,
        ffi.Pointer<ffi.WChar>, _locale_t, va_list)>()
external int __stdio_common_vfwprintf(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<FILE>,
        ffi.Pointer<ffi.WChar>, _locale_t, va_list)>()
external int __stdio_common_vfwprintf_s(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<FILE>,
        ffi.Pointer<ffi.WChar>, _locale_t, va_list)>()
external int __stdio_common_vfwprintf_p(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<FILE>,
        ffi.Pointer<ffi.WChar>, _locale_t, va_list)>()
external int __stdio_common_vfwscanf(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<ffi.WChar>, ffi.Size,
        ffi.Pointer<ffi.WChar>, _locale_t, va_list)>()
external int __stdio_common_vswprintf(
  int _Options,
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<ffi.WChar>, ffi.Size,
        ffi.Pointer<ffi.WChar>, _locale_t, va_list)>()
external int __stdio_common_vswprintf_s(
  int _Options,
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<ffi.WChar>, ffi.Size,
        ffi.Size, ffi.Pointer<ffi.WChar>, _locale_t, va_list)>()
external int __stdio_common_vsnwprintf_s(
  int _Options,
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
  int _MaxCount,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<ffi.WChar>, ffi.Size,
        ffi.Pointer<ffi.WChar>, _locale_t, va_list)>()
external int __stdio_common_vswprintf_p(
  int _Options,
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<ffi.WChar>, ffi.Size,
        ffi.Pointer<ffi.WChar>, _locale_t, va_list)>()
external int __stdio_common_vswscanf(
  int _Options,
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    errno_t Function(
        ffi.Pointer<FILE>,
        ffi.Pointer<ffi.Pointer<ffi.Pointer<ffi.Char>>>,
        ffi.Pointer<ffi.Pointer<ffi.Pointer<ffi.Char>>>,
        ffi.Pointer<ffi.Pointer<ffi.Int>>)>()
external int _get_stream_buffer_pointers(
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Pointer<ffi.Pointer<ffi.Char>>> _Base,
  ffi.Pointer<ffi.Pointer<ffi.Pointer<ffi.Char>>> _Pointer,
  ffi.Pointer<ffi.Pointer<ffi.Int>> _Count,
);

@ffi.Native<errno_t Function(ffi.Pointer<FILE>)>()
external int clearerr_s(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
    errno_t Function(ffi.Pointer<ffi.Pointer<FILE>>, ffi.Pointer<ffi.Char>,
        ffi.Pointer<ffi.Char>)>()
external int fopen_s(
  ffi.Pointer<ffi.Pointer<FILE>> _Stream,
  ffi.Pointer<ffi.Char> _FileName,
  ffi.Pointer<ffi.Char> _Mode,
);

@ffi.Native<
    ffi.Size Function(ffi.Pointer<ffi.Void>, ffi.Size, ffi.Size, ffi.Size,
        ffi.Pointer<FILE>)>()
external int fread_s(
  ffi.Pointer<ffi.Void> _Buffer,
  int _BufferSize,
  int _ElementSize,
  int _ElementCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
    errno_t Function(ffi.Pointer<ffi.Pointer<FILE>>, ffi.Pointer<ffi.Char>,
        ffi.Pointer<ffi.Char>, ffi.Pointer<FILE>)>()
external int freopen_s(
  ffi.Pointer<ffi.Pointer<FILE>> _Stream,
  ffi.Pointer<ffi.Char> _FileName,
  ffi.Pointer<ffi.Char> _Mode,
  ffi.Pointer<FILE> _OldStream,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Pointer<ffi.Char>, rsize_t)>()
external ffi.Pointer<ffi.Char> gets_s(
  ffi.Pointer<ffi.Char> _Buffer,
  int _Size,
);

@ffi.Native<errno_t Function(ffi.Pointer<ffi.Pointer<FILE>>)>()
external int tmpfile_s(
  ffi.Pointer<ffi.Pointer<FILE>> _Stream,
);

@ffi.Native<errno_t Function(ffi.Pointer<ffi.Char>, rsize_t)>()
external int tmpnam_s(
  ffi.Pointer<ffi.Char> _Buffer,
  int _Size,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<FILE>)>()
external void clearerr(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int fclose(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int _fcloseall();

@ffi.Native<ffi.Pointer<FILE> Function(ffi.Int, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<FILE> _fdopen(
  int _FileHandle,
  ffi.Pointer<ffi.Char> _Mode,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int feof(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int ferror(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int fflush(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int fgetc(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int _fgetchar();

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>, ffi.Pointer<fpos_t>)>()
external int fgetpos(
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<fpos_t> _Position,
);

@ffi.Native<
    ffi.Pointer<ffi.Char> Function(
        ffi.Pointer<ffi.Char>, ffi.Int, ffi.Pointer<FILE>)>()
external ffi.Pointer<ffi.Char> fgets(
  ffi.Pointer<ffi.Char> _Buffer,
  int _MaxCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _fileno(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int _flushall();

@ffi.Native<
    ffi.Pointer<FILE> Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<FILE> fopen(
  ffi.Pointer<ffi.Char> _FileName,
  ffi.Pointer<ffi.Char> _Mode,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int fputc(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Int)>()
external int _fputchar(
  int _Character,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.Char>, ffi.Pointer<FILE>)>()
external int fputs(
  ffi.Pointer<ffi.Char> _Buffer,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
    ffi.UnsignedLongLong Function(
        ffi.Pointer<ffi.Void>, ffi.Size, ffi.Size, ffi.Pointer<FILE>)>()
external int fread(
  ffi.Pointer<ffi.Void> _Buffer,
  int _ElementSize,
  int _ElementCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
    ffi.Pointer<FILE> Function(
        ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>, ffi.Pointer<FILE>)>()
external ffi.Pointer<FILE> freopen(
  ffi.Pointer<ffi.Char> _FileName,
  ffi.Pointer<ffi.Char> _Mode,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
    ffi.Pointer<FILE> Function(
        ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>, ffi.Int)>()
external ffi.Pointer<FILE> _fsopen(
  ffi.Pointer<ffi.Char> _FileName,
  ffi.Pointer<ffi.Char> _Mode,
  int _ShFlag,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>, ffi.Pointer<fpos_t>)>()
external int fsetpos(
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<fpos_t> _Position,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>, ffi.Long, ffi.Int)>()
external int fseek(
  ffi.Pointer<FILE> _Stream,
  int _Offset,
  int _Origin,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>, ffi.LongLong, ffi.Int)>()
external int _fseeki64(
  ffi.Pointer<FILE> _Stream,
  int _Offset,
  int _Origin,
);

@ffi.Native<ffi.Long Function(ffi.Pointer<FILE>)>()
external int ftell(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.LongLong Function(ffi.Pointer<FILE>)>()
external int _ftelli64(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
    ffi.UnsignedLongLong Function(
        ffi.Pointer<ffi.Void>, ffi.Size, ffi.Size, ffi.Pointer<FILE>)>()
external int fwrite(
  ffi.Pointer<ffi.Void> _Buffer,
  int _ElementSize,
  int _ElementCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int getc(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int getchar();

@ffi.Native<ffi.Int Function()>()
external int _getmaxstdio();

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _getw(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ffi.Char>)>()
external void perror(
  ffi.Pointer<ffi.Char> _ErrorMessage,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _pclose(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
    ffi.Pointer<FILE> Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<FILE> _popen(
  ffi.Pointer<ffi.Char> _Command,
  ffi.Pointer<ffi.Char> _Mode,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int putc(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Int)>()
external int putchar(
  int _Character,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.Char>)>()
external int puts(
  ffi.Pointer<ffi.Char> _Buffer,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int _putw(
  int _Word,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.Char>)>()
external int remove(
  ffi.Pointer<ffi.Char> _FileName,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>()
external int rename(
  ffi.Pointer<ffi.Char> _OldFileName,
  ffi.Pointer<ffi.Char> _NewFileName,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.Char>)>()
external int _unlink(
  ffi.Pointer<ffi.Char> _FileName,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.Char>)>()
external int unlink(
  ffi.Pointer<ffi.Char> _FileName,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<FILE>)>()
external void rewind(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int _rmtmp();

@ffi.Native<ffi.Void Function(ffi.Pointer<FILE>, ffi.Pointer<ffi.Char>)>()
external void setbuf(
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Char> _Buffer,
);

@ffi.Native<ffi.Int Function(ffi.Int)>()
external int _setmaxstdio(
  int _Maximum,
);

@ffi.Native<
    ffi.Int Function(
        ffi.Pointer<FILE>, ffi.Pointer<ffi.Char>, ffi.Int, ffi.Size)>()
external int setvbuf(
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Char> _Buffer,
  int _Mode,
  int _Size,
);

@ffi.Native<
    ffi.Pointer<ffi.Char> Function(
        ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<ffi.Char> _tempnam(
  ffi.Pointer<ffi.Char> _DirectoryName,
  ffi.Pointer<ffi.Char> _FilePrefix,
);

@ffi.Native<ffi.Pointer<FILE> Function()>()
external ffi.Pointer<FILE> tmpfile();

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<ffi.Char> tmpnam(
  ffi.Pointer<ffi.Char> _Buffer,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int ungetc(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<FILE>)>()
external void _lock_file(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<FILE>)>()
external void _unlock_file(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _fclose_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _fflush_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _fgetc_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int _fputc_nolock(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
    ffi.Size Function(
        ffi.Pointer<ffi.Void>, ffi.Size, ffi.Size, ffi.Pointer<FILE>)>()
external int _fread_nolock(
  ffi.Pointer<ffi.Void> _Buffer,
  int _ElementSize,
  int _ElementCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
    ffi.Size Function(ffi.Pointer<ffi.Void>, ffi.Size, ffi.Size, ffi.Size,
        ffi.Pointer<FILE>)>()
external int _fread_nolock_s(
  ffi.Pointer<ffi.Void> _Buffer,
  int _BufferSize,
  int _ElementSize,
  int _ElementCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>, ffi.Long, ffi.Int)>()
external int _fseek_nolock(
  ffi.Pointer<FILE> _Stream,
  int _Offset,
  int _Origin,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>, ffi.LongLong, ffi.Int)>()
external int _fseeki64_nolock(
  ffi.Pointer<FILE> _Stream,
  int _Offset,
  int _Origin,
);

@ffi.Native<ffi.Long Function(ffi.Pointer<FILE>)>()
external int _ftell_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.LongLong Function(ffi.Pointer<FILE>)>()
external int _ftelli64_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
    ffi.Size Function(
        ffi.Pointer<ffi.Void>, ffi.Size, ffi.Size, ffi.Pointer<FILE>)>()
external int _fwrite_nolock(
  ffi.Pointer<ffi.Void> _Buffer,
  int _ElementSize,
  int _ElementCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _getc_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int _putc_nolock(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int _ungetc_nolock(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Pointer<ffi.Int> Function()>()
external ffi.Pointer<ffi.Int> __p__commode();

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<FILE>,
        ffi.Pointer<ffi.Char>, _locale_t, va_list)>()
external int __stdio_common_vfprintf(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<FILE>,
        ffi.Pointer<ffi.Char>, _locale_t, va_list)>()
external int __stdio_common_vfprintf_s(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<FILE>,
        ffi.Pointer<ffi.Char>, _locale_t, va_list)>()
external int __stdio_common_vfprintf_p(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<ffi.Int Function(ffi.Int)>()
external int _set_printf_count_output(
  int _Value,
);

@ffi.Native<ffi.Int Function()>()
external int _get_printf_count_output();

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<FILE>,
        ffi.Pointer<ffi.Char>, _locale_t, va_list)>()
external int __stdio_common_vfscanf(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _Arglist,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<ffi.Char>, ffi.Size,
        ffi.Pointer<ffi.Char>, _locale_t, va_list)>()
external int __stdio_common_vsprintf(
  int _Options,
  ffi.Pointer<ffi.Char> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<ffi.Char>, ffi.Size,
        ffi.Pointer<ffi.Char>, _locale_t, va_list)>()
external int __stdio_common_vsprintf_s(
  int _Options,
  ffi.Pointer<ffi.Char> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<ffi.Char>, ffi.Size,
        ffi.Size, ffi.Pointer<ffi.Char>, _locale_t, va_list)>()
external int __stdio_common_vsnprintf_s(
  int _Options,
  ffi.Pointer<ffi.Char> _Buffer,
  int _BufferCount,
  int _MaxCount,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<ffi.Char>, ffi.Size,
        ffi.Pointer<ffi.Char>, _locale_t, va_list)>()
external int __stdio_common_vsprintf_p(
  int _Options,
  ffi.Pointer<ffi.Char> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Int Function(ffi.UnsignedLongLong, ffi.Pointer<ffi.Char>, ffi.Size,
        ffi.Pointer<ffi.Char>, _locale_t, va_list)>()
external int __stdio_common_vsscanf(
  int _Options,
  ffi.Pointer<ffi.Char> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
    ffi.Pointer<ffi.Char> Function(
        ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<ffi.Char> tempnam(
  ffi.Pointer<ffi.Char> _Directory,
  ffi.Pointer<ffi.Char> _FilePrefix,
);

@ffi.Native<ffi.Int Function()>()
external int fcloseall();

@ffi.Native<ffi.Pointer<FILE> Function(ffi.Int, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<FILE> fdopen(
  int _FileHandle,
  ffi.Pointer<ffi.Char> _Format,
);

@ffi.Native<ffi.Int Function()>()
external int fgetchar();

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int fileno(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int flushall();

@ffi.Native<ffi.Int Function(ffi.Int)>()
external int fputchar(
  int _Ch,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int getw(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int putw(
  int _Ch,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int rmtmp();

@ffi.Native<
    ffi.Void Function(ffi.Pointer<ffi.Char>, ffi.Int, ffi.Pointer<ffi.Char>)>()
external void ggml_abort(
  ffi.Pointer<ffi.Char> file,
  int line,
  ffi.Pointer<ffi.Char> fmt,
);

/// get ggml_status name string
@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Int)>(
    symbol: 'ggml_status_to_string')
external ffi.Pointer<ffi.Char> _ggml_status_to_string(
  int status,
);

ffi.Pointer<ffi.Char> ggml_status_to_string(
  ggml_status status,
) =>
    _ggml_status_to_string(
      status.value,
    );

@ffi.Native<ffi.Float Function(ggml_fp16_t)>()
external double ggml_fp16_to_fp32(
  int arg0,
);

@ffi.Native<ggml_fp16_t Function(ffi.Float)>()
external int ggml_fp32_to_fp16(
  double arg0,
);

@ffi.Native<
    ffi.Void Function(
        ffi.Pointer<ggml_fp16_t>, ffi.Pointer<ffi.Float>, ffi.Int64)>()
external void ggml_fp16_to_fp32_row(
  ffi.Pointer<ggml_fp16_t> arg0,
  ffi.Pointer<ffi.Float> arg1,
  int arg2,
);

@ffi.Native<
    ffi.Void Function(
        ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_fp16_t>, ffi.Int64)>()
external void ggml_fp32_to_fp16_row(
  ffi.Pointer<ffi.Float> arg0,
  ffi.Pointer<ggml_fp16_t> arg1,
  int arg2,
);

@ffi.Native<ggml_bf16_t Function(ffi.Float)>()
external ggml_bf16_t ggml_fp32_to_bf16(
  double arg0,
);

@ffi.Native<ffi.Float Function(ggml_bf16_t)>()
external double ggml_bf16_to_fp32(
  ggml_bf16_t arg0,
);

@ffi.Native<
    ffi.Void Function(
        ffi.Pointer<ggml_bf16_t>, ffi.Pointer<ffi.Float>, ffi.Int64)>()
external void ggml_bf16_to_fp32_row(
  ffi.Pointer<ggml_bf16_t> arg0,
  ffi.Pointer<ffi.Float> arg1,
  int arg2,
);

@ffi.Native<
    ffi.Void Function(
        ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_bf16_t>, ffi.Int64)>()
external void ggml_fp32_to_bf16_row_ref(
  ffi.Pointer<ffi.Float> arg0,
  ffi.Pointer<ggml_bf16_t> arg1,
  int arg2,
);

@ffi.Native<
    ffi.Void Function(
        ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_bf16_t>, ffi.Int64)>()
external void ggml_fp32_to_bf16_row(
  ffi.Pointer<ffi.Float> arg0,
  ffi.Pointer<ggml_bf16_t> arg1,
  int arg2,
);

@ffi.Native<ffi.Size>()
external final int GGML_TENSOR_SIZE;

@ffi.Native<ffi.Bool Function(ggml_guid_t, ggml_guid_t)>()
external bool ggml_guid_matches(
  ggml_guid_t guid_a,
  ggml_guid_t guid_b,
);

/// misc
@ffi.Native<ffi.Void Function()>()
external void ggml_time_init();

@ffi.Native<ffi.Int64 Function()>()
external int ggml_time_ms();

@ffi.Native<ffi.Int64 Function()>()
external int ggml_time_us();

@ffi.Native<ffi.Int64 Function()>()
external int ggml_cycles();

@ffi.Native<ffi.Int64 Function()>()
external int ggml_cycles_per_ms();

/// accepts a UTF-8 path, even on Windows
@ffi.Native<
    ffi.Pointer<FILE> Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<FILE> ggml_fopen(
  ffi.Pointer<ffi.Char> fname,
  ffi.Pointer<ffi.Char> mode,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_object>)>()
external void ggml_print_object(
  ffi.Pointer<ggml_object> obj,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_context>)>()
external void ggml_print_objects(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Int64 Function(ffi.Pointer<ggml_tensor>)>()
external int ggml_nelements(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Int64 Function(ffi.Pointer<ggml_tensor>)>()
external int ggml_nrows(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<ggml_tensor>)>()
external int ggml_nbytes(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<ggml_tensor>)>()
external int ggml_nbytes_pad(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Int64 Function(ffi.UnsignedInt)>(symbol: 'ggml_blck_size')
external int _ggml_blck_size(
  int type,
);

int ggml_blck_size(
  ggml_type type,
) =>
    _ggml_blck_size(
      type.value,
    );

@ffi.Native<ffi.Size Function(ffi.UnsignedInt)>(symbol: 'ggml_type_size')
external int _ggml_type_size(
  int type,
);

int ggml_type_size(
  ggml_type type,
) =>
    _ggml_type_size(
      type.value,
    );

@ffi.Native<ffi.Size Function(ffi.UnsignedInt, ffi.Int64)>(
    symbol: 'ggml_row_size')
external int _ggml_row_size(
  int type,
  int ne,
);

int ggml_row_size(
  ggml_type type,
  int ne,
) =>
    _ggml_row_size(
      type.value,
      ne,
    );

@ffi.Native<ffi.Double Function(ffi.UnsignedInt)>(symbol: 'ggml_type_sizef')
external double _ggml_type_sizef(
  int type,
);

double ggml_type_sizef(
  ggml_type type,
) =>
    _ggml_type_sizef(
      type.value,
    );

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>(
    symbol: 'ggml_type_name')
external ffi.Pointer<ffi.Char> _ggml_type_name(
  int type,
);

ffi.Pointer<ffi.Char> ggml_type_name(
  ggml_type type,
) =>
    _ggml_type_name(
      type.value,
    );

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>(
    symbol: 'ggml_op_name')
external ffi.Pointer<ffi.Char> _ggml_op_name(
  int op,
);

ffi.Pointer<ffi.Char> ggml_op_name(
  ggml_op op,
) =>
    _ggml_op_name(
      op.value,
    );

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>(
    symbol: 'ggml_op_symbol')
external ffi.Pointer<ffi.Char> _ggml_op_symbol(
  int op,
);

ffi.Pointer<ffi.Char> ggml_op_symbol(
  ggml_op op,
) =>
    _ggml_op_symbol(
      op.value,
    );

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>(
    symbol: 'ggml_unary_op_name')
external ffi.Pointer<ffi.Char> _ggml_unary_op_name(
  int op,
);

ffi.Pointer<ffi.Char> ggml_unary_op_name(
  ggml_unary_op op,
) =>
    _ggml_unary_op_name(
      op.value,
    );

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ffi.Char> ggml_op_desc(
  ffi.Pointer<ggml_tensor> t,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<ggml_tensor>)>()
external int ggml_element_size(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.UnsignedInt)>(symbol: 'ggml_is_quantized')
external bool _ggml_is_quantized(
  int type,
);

bool ggml_is_quantized(
  ggml_type type,
) =>
    _ggml_is_quantized(
      type.value,
    );

/// TODO: temporary until model loading of ggml examples is refactored
@ffi.Native<ffi.UnsignedInt Function(ffi.Int)>(
    symbol: 'ggml_ftype_to_ggml_type')
external int _ggml_ftype_to_ggml_type(
  int ftype,
);

ggml_type ggml_ftype_to_ggml_type(
  ggml_ftype ftype,
) =>
    ggml_type.fromValue(_ggml_ftype_to_ggml_type(
      ftype.value,
    ));

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_transposed(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_permuted(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_empty(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_scalar(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_vector(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_matrix(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_3d(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ggml_tensor>)>()
external int ggml_n_dims(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_contiguous(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_contiguous_0(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_contiguous_1(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_contiguous_2(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<
    ffi.Bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external bool ggml_are_same_shape(
  ffi.Pointer<ggml_tensor> t0,
  ffi.Pointer<ggml_tensor> t1,
);

@ffi.Native<
    ffi.Bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external bool ggml_are_same_stride(
  ffi.Pointer<ggml_tensor> t0,
  ffi.Pointer<ggml_tensor> t1,
);

@ffi.Native<
    ffi.Bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external bool ggml_can_repeat(
  ffi.Pointer<ggml_tensor> t0,
  ffi.Pointer<ggml_tensor> t1,
);

/// use this to compute the memory overhead of a tensor
@ffi.Native<ffi.Size Function()>()
external int ggml_tensor_overhead();

@ffi.Native<
        ffi.Bool Function(ffi.UnsignedInt, ffi.Pointer<ffi.Void>, ffi.Size)>(
    symbol: 'ggml_validate_row_data')
external bool _ggml_validate_row_data(
  int type,
  ffi.Pointer<ffi.Void> data,
  int nbytes,
);

bool ggml_validate_row_data(
  ggml_type type,
  ffi.Pointer<ffi.Void> data,
  int nbytes,
) =>
    _ggml_validate_row_data(
      type.value,
      data,
      nbytes,
    );

/// main
@ffi.Native<ffi.Pointer<ggml_context> Function(ggml_init_params)>()
external ffi.Pointer<ggml_context> ggml_init(
  ggml_init_params params,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_context>)>()
external void ggml_reset(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_context>)>()
external void ggml_free(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<ggml_context>)>()
external int ggml_used_mem(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_context>)>()
external bool ggml_get_no_alloc(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_context>, ffi.Bool)>()
external void ggml_set_no_alloc(
  ffi.Pointer<ggml_context> ctx,
  bool no_alloc,
);

@ffi.Native<ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_context>)>()
external ffi.Pointer<ffi.Void> ggml_get_mem_buffer(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<ggml_context>)>()
external int ggml_get_mem_size(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<ggml_context>)>()
external int ggml_get_max_tensor_size(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.UnsignedInt,
        ffi.Int,
        ffi.Pointer<ffi.Int64>)>(symbol: 'ggml_new_tensor')
external ffi.Pointer<ggml_tensor> _ggml_new_tensor(
  ffi.Pointer<ggml_context> ctx,
  int type,
  int n_dims,
  ffi.Pointer<ffi.Int64> ne,
);

ffi.Pointer<ggml_tensor> ggml_new_tensor(
  ffi.Pointer<ggml_context> ctx,
  ggml_type type,
  int n_dims,
  ffi.Pointer<ffi.Int64> ne,
) =>
    _ggml_new_tensor(
      ctx,
      type.value,
      n_dims,
      ne,
    );

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.UnsignedInt, ffi.Int64)>(symbol: 'ggml_new_tensor_1d')
external ffi.Pointer<ggml_tensor> _ggml_new_tensor_1d(
  ffi.Pointer<ggml_context> ctx,
  int type,
  int ne0,
);

ffi.Pointer<ggml_tensor> ggml_new_tensor_1d(
  ffi.Pointer<ggml_context> ctx,
  ggml_type type,
  int ne0,
) =>
    _ggml_new_tensor_1d(
      ctx,
      type.value,
      ne0,
    );

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.UnsignedInt, ffi.Int64, ffi.Int64)>(symbol: 'ggml_new_tensor_2d')
external ffi.Pointer<ggml_tensor> _ggml_new_tensor_2d(
  ffi.Pointer<ggml_context> ctx,
  int type,
  int ne0,
  int ne1,
);

ffi.Pointer<ggml_tensor> ggml_new_tensor_2d(
  ffi.Pointer<ggml_context> ctx,
  ggml_type type,
  int ne0,
  int ne1,
) =>
    _ggml_new_tensor_2d(
      ctx,
      type.value,
      ne0,
      ne1,
    );

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.UnsignedInt,
        ffi.Int64,
        ffi.Int64,
        ffi.Int64)>(symbol: 'ggml_new_tensor_3d')
external ffi.Pointer<ggml_tensor> _ggml_new_tensor_3d(
  ffi.Pointer<ggml_context> ctx,
  int type,
  int ne0,
  int ne1,
  int ne2,
);

ffi.Pointer<ggml_tensor> ggml_new_tensor_3d(
  ffi.Pointer<ggml_context> ctx,
  ggml_type type,
  int ne0,
  int ne1,
  int ne2,
) =>
    _ggml_new_tensor_3d(
      ctx,
      type.value,
      ne0,
      ne1,
      ne2,
    );

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.UnsignedInt,
        ffi.Int64,
        ffi.Int64,
        ffi.Int64,
        ffi.Int64)>(symbol: 'ggml_new_tensor_4d')
external ffi.Pointer<ggml_tensor> _ggml_new_tensor_4d(
  ffi.Pointer<ggml_context> ctx,
  int type,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
);

ffi.Pointer<ggml_tensor> ggml_new_tensor_4d(
  ffi.Pointer<ggml_context> ctx,
  ggml_type type,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
) =>
    _ggml_new_tensor_4d(
      ctx,
      type.value,
      ne0,
      ne1,
      ne2,
      ne3,
    );

@ffi.Native<
    ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_context>, ffi.Size)>()
external ffi.Pointer<ffi.Void> ggml_new_buffer(
  ffi.Pointer<ggml_context> ctx,
  int nbytes,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_dup_tensor(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> src,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_view_tensor(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> src,
);

/// Context tensor enumeration and lookup
@ffi.Native<ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>)>()
external ffi.Pointer<ggml_tensor> ggml_get_first_tensor(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_get_next_tensor(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<ggml_tensor> ggml_get_tensor(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ffi.Char> name,
);

/// Converts a flat index into coordinates
@ffi.Native<
    ffi.Void Function(
        ffi.Pointer<ggml_tensor>,
        ffi.Int64,
        ffi.Pointer<ffi.Int64>,
        ffi.Pointer<ffi.Int64>,
        ffi.Pointer<ffi.Int64>,
        ffi.Pointer<ffi.Int64>)>()
external void ggml_unravel_index(
  ffi.Pointer<ggml_tensor> tensor,
  int i,
  ffi.Pointer<ffi.Int64> i0,
  ffi.Pointer<ffi.Int64> i1,
  ffi.Pointer<ffi.Int64> i2,
  ffi.Pointer<ffi.Int64> i3,
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<ggml_tensor>)>(
    symbol: 'ggml_get_unary_op')
external int _ggml_get_unary_op(
  ffi.Pointer<ggml_tensor> tensor,
);

ggml_unary_op ggml_get_unary_op(
  ffi.Pointer<ggml_tensor> tensor,
) =>
    ggml_unary_op.fromValue(_ggml_get_unary_op(
      tensor,
    ));

@ffi.Native<ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ffi.Void> ggml_get_data(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Pointer<ffi.Float> Function(ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ffi.Float> ggml_get_data_f32(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ffi.Char> ggml_get_name(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<ggml_tensor> ggml_set_name(
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Char> name,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<ggml_tensor> ggml_format_name(
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Char> fmt,
);

/// Tensor flags
@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>)>()
external void ggml_set_input(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>)>()
external void ggml_set_output(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<
    ffi.Void Function(ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external void ggml_set_param(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>)>()
external void ggml_set_loss(
  ffi.Pointer<ggml_tensor> tensor,
);

/// operations on tensors with backpropagation
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_dup(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// in-place, returns view(a)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_dup_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_add(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_add_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.UnsignedInt)>(symbol: 'ggml_add_cast')
external ffi.Pointer<ggml_tensor> _ggml_add_cast(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int type,
);

ffi.Pointer<ggml_tensor> ggml_add_cast(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ggml_type type,
) =>
    _ggml_add_cast(
      ctx,
      a,
      b,
      type.value,
    );

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_add1(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_add1_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// dst = a
/// view(dst, nb1, nb2, nb3, offset) += b
/// return dst
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Size,
        ffi.Size,
        ffi.Size,
        ffi.Size)>()
external ffi.Pointer<ggml_tensor> ggml_acc(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int nb1,
  int nb2,
  int nb3,
  int offset,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Size,
        ffi.Size,
        ffi.Size,
        ffi.Size)>()
external ffi.Pointer<ggml_tensor> ggml_acc_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int nb1,
  int nb2,
  int nb3,
  int offset,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sub(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sub_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_mul(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_mul_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_div(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_div_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sqr(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sqr_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sqrt(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sqrt_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_log(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_log_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sin(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sin_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_cos(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_cos_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// return scalar
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sum(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// sums along rows, with input shape [a,b,c,d] return shape [1,b,c,d]
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sum_rows(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// mean along rows
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_mean(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// argmax along rows
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_argmax(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// count number of equal elements in a and b
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_count_equal(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// if a is the same shape as b, and a is not parameter, return a
/// otherwise, return a new tensor: repeat(a) to fit in b
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_repeat(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// sums repetitions in a into shape of b
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_repeat_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// concat a and b along dim
/// used in stable-diffusion
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_concat(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int dim,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_abs(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_abs_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sgn(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sgn_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_neg(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_neg_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_step(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_step_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_tanh(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_tanh_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_elu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_elu_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_relu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Float, ffi.Bool)>()
external ffi.Pointer<ggml_tensor> ggml_leaky_relu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double negative_slope,
  bool inplace,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_relu_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sigmoid(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_sigmoid_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_gelu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_gelu_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_gelu_quick(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_gelu_quick_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_silu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_silu_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// a - x
/// b - dy
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_silu_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// hardswish(x) = x * relu6(x + 3) / 6
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_hardswish(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// hardsigmoid(x) = relu6(x + 3) / 6
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_hardsigmoid(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_exp(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_exp_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// normalize along rows
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_norm(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double eps,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_norm_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double eps,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_rms_norm(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double eps,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_rms_norm_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double eps,
);

/// group normalize along ne0*ne1*n_groups
/// used in stable-diffusion
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_group_norm(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int n_groups,
  double eps,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_group_norm_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int n_groups,
  double eps,
);

/// a - x
/// b - dy
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_rms_norm_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  double eps,
);

/// A: k columns, n rows => [ne03, ne02, n, k]
/// B: k columns, m rows  (i.e. we transpose it internally) => [ne03 * x, ne02 * y, m, k]
/// result is n columns, m rows => [ne03 * x, ne02 * y, m, n]
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_mul_mat(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// change the precision of a matrix multiplication
/// set to GGML_PREC_F32 for higher precision (useful for phi-2)
@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.UnsignedInt)>(
    symbol: 'ggml_mul_mat_set_prec')
external void _ggml_mul_mat_set_prec(
  ffi.Pointer<ggml_tensor> a,
  int prec,
);

void ggml_mul_mat_set_prec(
  ffi.Pointer<ggml_tensor> a,
  ggml_prec prec,
) =>
    _ggml_mul_mat_set_prec(
      a,
      prec.value,
    );

/// indirect matrix multiplication
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_mul_mat_id(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> as$,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> ids,
);

/// A: m columns, n rows,
/// B: p columns, n rows,
/// result is m columns, p rows
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_out_prod(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// operations on tensors without backpropagation
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_scale(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double s,
);

/// in-place, returns view(a)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_scale_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double s,
);

/// b -> view(a,offset,nb1,nb2,3), return modified a
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Size,
        ffi.Size,
        ffi.Size,
        ffi.Size)>()
external ffi.Pointer<ggml_tensor> ggml_set(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int nb1,
  int nb2,
  int nb3,
  int offset,
);

/// b -> view(a,offset,nb1,nb2,3), return view(a)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Size,
        ffi.Size,
        ffi.Size,
        ffi.Size)>()
external ffi.Pointer<ggml_tensor> ggml_set_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int nb1,
  int nb2,
  int nb3,
  int offset,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, ffi.Size)>()
external ffi.Pointer<ggml_tensor> ggml_set_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int offset,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, ffi.Size)>()
external ffi.Pointer<ggml_tensor> ggml_set_1d_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int offset,
);

/// b -> view(a,offset,nb1,nb2,3), return modified a
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Size,
        ffi.Size)>()
external ffi.Pointer<ggml_tensor> ggml_set_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int nb1,
  int offset,
);

/// b -> view(a,offset,nb1,nb2,3), return view(a)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Size,
        ffi.Size)>()
external ffi.Pointer<ggml_tensor> ggml_set_2d_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int nb1,
  int offset,
);

/// a -> b, return view(b)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_cpy(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.UnsignedInt)>(symbol: 'ggml_cast')
external ffi.Pointer<ggml_tensor> _ggml_cast(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int type,
);

ffi.Pointer<ggml_tensor> ggml_cast(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_type type,
) =>
    _ggml_cast(
      ctx,
      a,
      type.value,
    );

/// make contiguous
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_cont(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// make contiguous, with new shape
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Int64)>()
external ffi.Pointer<ggml_tensor> ggml_cont_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int64, ffi.Int64)>()
external ffi.Pointer<ggml_tensor> ggml_cont_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int64, ffi.Int64, ffi.Int64)>()
external ffi.Pointer<ggml_tensor> ggml_cont_3d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int64, ffi.Int64, ffi.Int64, ffi.Int64)>()
external ffi.Pointer<ggml_tensor> ggml_cont_4d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
);

/// return view(a), b specifies the new shape
/// TODO: when we start computing gradient, make a copy instead of view
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_reshape(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// return view(a)
/// TODO: when we start computing gradient, make a copy instead of view
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Int64)>()
external ffi.Pointer<ggml_tensor> ggml_reshape_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int64, ffi.Int64)>()
external ffi.Pointer<ggml_tensor> ggml_reshape_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
);

/// return view(a)
/// TODO: when we start computing gradient, make a copy instead of view
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int64, ffi.Int64, ffi.Int64)>()
external ffi.Pointer<ggml_tensor> ggml_reshape_3d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int64, ffi.Int64, ffi.Int64, ffi.Int64)>()
external ffi.Pointer<ggml_tensor> ggml_reshape_4d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
);

/// offset in bytes
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int64, ffi.Size)>()
external ffi.Pointer<ggml_tensor> ggml_view_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int offset,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int64, ffi.Int64, ffi.Size, ffi.Size)>()
external ffi.Pointer<ggml_tensor> ggml_view_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int nb1,
  int offset,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int64,
        ffi.Int64,
        ffi.Int64,
        ffi.Size,
        ffi.Size,
        ffi.Size)>()
external ffi.Pointer<ggml_tensor> ggml_view_3d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int nb1,
  int nb2,
  int offset,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int64,
        ffi.Int64,
        ffi.Int64,
        ffi.Int64,
        ffi.Size,
        ffi.Size,
        ffi.Size,
        ffi.Size)>()
external ffi.Pointer<ggml_tensor> ggml_view_4d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
  int nb1,
  int nb2,
  int nb3,
  int offset,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int, ffi.Int, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_permute(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int axis0,
  int axis1,
  int axis2,
  int axis3,
);

/// alias for ggml_permute(ctx, a, 1, 0, 2, 3)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_transpose(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// supports 3D: a->ne[2] == b->ne[1]
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_get_rows(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_get_rows_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_diag(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// set elements above the diagonal to -INF
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_diag_mask_inf(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int n_past,
);

/// in-place, returns view(a)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_diag_mask_inf_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int n_past,
);

/// set elements above the diagonal to 0
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_diag_mask_zero(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int n_past,
);

/// in-place, returns view(a)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_diag_mask_zero_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int n_past,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_soft_max(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// in-place, returns view(a)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_soft_max_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// fused soft_max(a*scale + mask*(ALiBi slope))
/// mask is optional
/// max_bias = 0.0f for no ALiBi
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Float,
        ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_soft_max_ext(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> mask,
  double scale,
  double max_bias,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Float,
        ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_soft_max_ext_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  double scale,
  double max_bias,
);

/// in-place, returns view(a)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Float,
        ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_soft_max_ext_back_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  double scale,
  double max_bias,
);

/// rotary position embedding
/// if (mode & 1) - skip n_past elements (NOT SUPPORTED)
/// if (mode & GGML_ROPE_TYPE_NEOX) - GPT-NeoX style
///
/// b is an int32 vector with size a->ne[2], it contains the positions
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_rope(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int n_dims,
  int mode,
);

/// in-place, returns view(a)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_rope_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int n_dims,
  int mode,
);

/// custom RoPE
/// c is freq factors (e.g. phi3-128k), (optional)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_rope_ext(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  int n_dims,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Pointer<ffi.Int>,
        ffi.Int,
        ffi.Int,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_rope_multi(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  int n_dims,
  ffi.Pointer<ffi.Int> sections,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

/// in-place, returns view(a)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_rope_ext_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  int n_dims,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_rope_custom(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int n_dims,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_rope_custom_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int n_dims,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

/// compute correction dims for YaRN RoPE scaling
@ffi.Native<
    ffi.Void Function(ffi.Int, ffi.Int, ffi.Float, ffi.Float, ffi.Float,
        ffi.Pointer<ffi.Float>)>()
external void ggml_rope_yarn_corr_dims(
  int n_dims,
  int n_ctx_orig,
  double freq_base,
  double beta_fast,
  double beta_slow,
  ffi.Pointer<ffi.Float> dims,
);

/// rotary position embedding backward, i.e compute dx from dy
/// a - dy
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_rope_ext_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  int n_dims,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Pointer<ffi.Int>,
        ffi.Int,
        ffi.Int,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float,
        ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_rope_multi_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  int n_dims,
  ffi.Pointer<ffi.Int> sections,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

/// clamp
/// in-place, returns view(a)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Float, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_clamp(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double min,
  double max,
);

/// im2col
/// converts data into a format that effectively results in a convolution when combined with matrix multiplication
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Bool,
        ffi.UnsignedInt)>(symbol: 'ggml_im2col')
external ffi.Pointer<ggml_tensor> _ggml_im2col(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int s1,
  int p0,
  int p1,
  int d0,
  int d1,
  bool is_2D,
  int dst_type,
);

ffi.Pointer<ggml_tensor> ggml_im2col(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int s1,
  int p0,
  int p1,
  int d0,
  int d1,
  bool is_2D,
  ggml_type dst_type,
) =>
    _ggml_im2col(
      ctx,
      a,
      b,
      s0,
      s1,
      p0,
      p1,
      d0,
      d1,
      is_2D,
      dst_type.value,
    );

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ffi.Int64>,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Bool)>()
external ffi.Pointer<ggml_tensor> ggml_im2col_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ffi.Int64> ne,
  int s0,
  int s1,
  int p0,
  int p1,
  int d0,
  int d1,
  bool is_2D,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Int,
        ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_conv_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int p0,
  int d0,
);

/// conv_1d with padding = half
/// alias for ggml_conv_1d(a, b, s, a->ne[0]/2, d)
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_conv_1d_ph(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s,
  int d,
);

/// depthwise
/// TODO: this is very likely wrong for some cases! - needs more testing
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Int,
        ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_conv_1d_dw(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int p0,
  int d0,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_conv_1d_dw_ph(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int d0,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Int,
        ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_conv_transpose_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int p0,
  int d0,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_conv_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int s1,
  int p0,
  int p1,
  int d0,
  int d1,
);

/// kernel size is a->ne[0] x a->ne[1]
/// stride is equal to kernel size
/// padding is zero
/// example:
/// a:     16   16    3  768
/// b:   1024 1024    3    1
/// res:   64   64  768    1
/// used in sam
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_conv_2d_sk_p0(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// kernel size is a->ne[0] x a->ne[1]
/// stride is 1
/// padding is half
/// example:
/// a:      3    3    256  256
/// b:     64   64    256    1
/// res:   64   64    256    1
/// used in sam
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_conv_2d_s1_ph(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// depthwise
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_conv_2d_dw(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int s1,
  int p0,
  int p1,
  int d0,
  int d1,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_conv_transpose_2d_p0(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int stride,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.UnsignedInt,
        ffi.Int,
        ffi.Int,
        ffi.Int)>(symbol: 'ggml_pool_1d')
external ffi.Pointer<ggml_tensor> _ggml_pool_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int op,
  int k0,
  int s0,
  int p0,
);

ffi.Pointer<ggml_tensor> ggml_pool_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_op_pool op,
  int k0,
  int s0,
  int p0,
) =>
    _ggml_pool_1d(
      ctx,
      a,
      op.value,
      k0,
      s0,
      p0,
    );

/// the result will have 2*p0 padding for the first dimension
/// and 2*p1 padding for the second dimension
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.UnsignedInt,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Float,
        ffi.Float)>(symbol: 'ggml_pool_2d')
external ffi.Pointer<ggml_tensor> _ggml_pool_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int op,
  int k0,
  int k1,
  int s0,
  int s1,
  double p0,
  double p1,
);

ffi.Pointer<ggml_tensor> ggml_pool_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_op_pool op,
  int k0,
  int k1,
  int s0,
  int s1,
  double p0,
  double p1,
) =>
    _ggml_pool_2d(
      ctx,
      a,
      op.value,
      k0,
      k1,
      s0,
      s1,
      p0,
      p1,
    );

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.UnsignedInt,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Int,
        ffi.Float,
        ffi.Float)>(symbol: 'ggml_pool_2d_back')
external ffi.Pointer<ggml_tensor> _ggml_pool_2d_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> af,
  int op,
  int k0,
  int k1,
  int s0,
  int s1,
  double p0,
  double p1,
);

ffi.Pointer<ggml_tensor> ggml_pool_2d_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> af,
  ggml_op_pool op,
  int k0,
  int k1,
  int s0,
  int s1,
  double p0,
  double p1,
) =>
    _ggml_pool_2d_back(
      ctx,
      a,
      af,
      op.value,
      k0,
      k1,
      s0,
      s1,
      p0,
      p1,
    );

/// nearest interpolate
/// multiplies ne0 and ne1 by scale factor
/// used in stable-diffusion
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_upscale(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int scale_factor,
);

/// nearest interpolate
/// nearest interpolate to specified dimensions
/// used in tortoise.cpp
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int, ffi.Int, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_upscale_ext(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
);

/// pad each dimension with zeros: [x, ..., x] -> [x, ..., x, 0, ..., 0]
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int, ffi.Int, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_pad(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int p0,
  int p1,
  int p2,
  int p3,
);

/// pad each dimension with reflection: [a, b, c, d] -> [b, a, b, c, d, c]
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_pad_reflect_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int p0,
  int p1,
);

/// Ref: https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/util.py#L151
/// timesteps: [N,]
/// return: [N, dim]
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_timestep_embedding(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> timesteps,
  int dim,
  int max_period,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.UnsignedInt)>(symbol: 'ggml_argsort')
external ffi.Pointer<ggml_tensor> _ggml_argsort(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int order,
);

ffi.Pointer<ggml_tensor> ggml_argsort(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_sort_order order,
) =>
    _ggml_argsort(
      ctx,
      a,
      order.value,
    );

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Float, ffi.Float, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_arange(
  ffi.Pointer<ggml_context> ctx,
  double start,
  double stop,
  double step,
);

/// top k elements per row
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_top_k(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int k,
);

/// q:    [n_embd, n_batch,     n_head,    1]
/// k:    [n_embd, n_kv,        n_head_kv, 1]
/// v:    [n_embd, n_kv,        n_head_kv, 1] !! not transposed !!
/// mask: [n_kv,   n_batch_pad, 1,         1] !! n_batch_pad = GGML_PAD(n_batch, GGML_KQ_MASK_PAD) !!
/// res:  [n_embd, n_head,      n_batch,   1] !! permuted !!
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Float,
        ffi.Float,
        ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_flash_attn_ext(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> q,
  ffi.Pointer<ggml_tensor> k,
  ffi.Pointer<ggml_tensor> v,
  ffi.Pointer<ggml_tensor> mask,
  double scale,
  double max_bias,
  double logit_softcap,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.UnsignedInt)>(
    symbol: 'ggml_flash_attn_ext_set_prec')
external void _ggml_flash_attn_ext_set_prec(
  ffi.Pointer<ggml_tensor> a,
  int prec,
);

void ggml_flash_attn_ext_set_prec(
  ffi.Pointer<ggml_tensor> a,
  ggml_prec prec,
) =>
    _ggml_flash_attn_ext_set_prec(
      a,
      prec.value,
    );

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<ggml_tensor>)>(
    symbol: 'ggml_flash_attn_ext_get_prec')
external int _ggml_flash_attn_ext_get_prec(
  ffi.Pointer<ggml_tensor> a,
);

ggml_prec ggml_flash_attn_ext_get_prec(
  ffi.Pointer<ggml_tensor> a,
) =>
    ggml_prec.fromValue(_ggml_flash_attn_ext_get_prec(
      a,
    ));

/// TODO: needs to be adapted to ggml_flash_attn_ext
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Bool)>()
external ffi.Pointer<ggml_tensor> ggml_flash_attn_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> q,
  ffi.Pointer<ggml_tensor> k,
  ffi.Pointer<ggml_tensor> v,
  ffi.Pointer<ggml_tensor> d,
  bool masked,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_ssm_conv(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> sx,
  ffi.Pointer<ggml_tensor> c,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_ssm_scan(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> s,
  ffi.Pointer<ggml_tensor> x,
  ffi.Pointer<ggml_tensor> dt,
  ffi.Pointer<ggml_tensor> A,
  ffi.Pointer<ggml_tensor> B,
  ffi.Pointer<ggml_tensor> C,
);

/// partition into non-overlapping windows with padding if needed
/// example:
/// a:   768   64   64    1
/// w:    14
/// res: 768   14   14    25
/// used in sam
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_win_part(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int w,
);

/// reverse of ggml_win_part
/// used in sam
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_win_unpart(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int w0,
  int h0,
  int w,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.UnsignedInt)>(symbol: 'ggml_unary')
external ffi.Pointer<ggml_tensor> _ggml_unary(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int op,
);

ffi.Pointer<ggml_tensor> ggml_unary(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_unary_op op,
) =>
    _ggml_unary(
      ctx,
      a,
      op.value,
    );

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.UnsignedInt)>(symbol: 'ggml_unary_inplace')
external ffi.Pointer<ggml_tensor> _ggml_unary_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int op,
);

ffi.Pointer<ggml_tensor> ggml_unary_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_unary_op op,
) =>
    _ggml_unary_inplace(
      ctx,
      a,
      op.value,
    );

/// used in sam
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_get_rel_pos(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int qh,
  int kh,
);

/// used in sam
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_add_rel_pos(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> pw,
  ffi.Pointer<ggml_tensor> ph,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_add_rel_pos_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> pw,
  ffi.Pointer<ggml_tensor> ph,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_rwkv_wkv6(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> k,
  ffi.Pointer<ggml_tensor> v,
  ffi.Pointer<ggml_tensor> r,
  ffi.Pointer<ggml_tensor> tf,
  ffi.Pointer<ggml_tensor> td,
  ffi.Pointer<ggml_tensor> state,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_gated_linear_attn(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> k,
  ffi.Pointer<ggml_tensor> v,
  ffi.Pointer<ggml_tensor> q,
  ffi.Pointer<ggml_tensor> g,
  ffi.Pointer<ggml_tensor> state,
  double scale,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ggml_unary_op_f32_t)>()
external ffi.Pointer<ggml_tensor> ggml_map_unary_f32(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_unary_op_f32_t fun,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ggml_unary_op_f32_t)>()
external ffi.Pointer<ggml_tensor> ggml_map_unary_inplace_f32(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_unary_op_f32_t fun,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ggml_binary_op_f32_t)>()
external ffi.Pointer<ggml_tensor> ggml_map_binary_f32(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ggml_binary_op_f32_t fun,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ggml_binary_op_f32_t)>()
external ffi.Pointer<ggml_tensor> ggml_map_binary_inplace_f32(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ggml_binary_op_f32_t fun,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ggml_custom1_op_f32_t)>()
external ffi.Pointer<ggml_tensor> ggml_map_custom1_f32(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_custom1_op_f32_t fun,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ggml_custom1_op_f32_t)>()
external ffi.Pointer<ggml_tensor> ggml_map_custom1_inplace_f32(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_custom1_op_f32_t fun,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ggml_custom2_op_f32_t)>()
external ffi.Pointer<ggml_tensor> ggml_map_custom2_f32(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ggml_custom2_op_f32_t fun,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ggml_custom2_op_f32_t)>()
external ffi.Pointer<ggml_tensor> ggml_map_custom2_inplace_f32(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ggml_custom2_op_f32_t fun,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ggml_custom3_op_f32_t)>()
external ffi.Pointer<ggml_tensor> ggml_map_custom3_f32(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  ggml_custom3_op_f32_t fun,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ggml_custom3_op_f32_t)>()
external ffi.Pointer<ggml_tensor> ggml_map_custom3_inplace_f32(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  ggml_custom3_op_f32_t fun,
);

/// n_tasks == GGML_N_TASKS_MAX means to use max number of tasks
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ggml_custom1_op_t,
        ffi.Int,
        ffi.Pointer<ffi.Void>)>()
external ffi.Pointer<ggml_tensor> ggml_map_custom1(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_custom1_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ggml_custom1_op_t,
        ffi.Int,
        ffi.Pointer<ffi.Void>)>()
external ffi.Pointer<ggml_tensor> ggml_map_custom1_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_custom1_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ggml_custom2_op_t,
        ffi.Int,
        ffi.Pointer<ffi.Void>)>()
external ffi.Pointer<ggml_tensor> ggml_map_custom2(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ggml_custom2_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ggml_custom2_op_t,
        ffi.Int,
        ffi.Pointer<ffi.Void>)>()
external ffi.Pointer<ggml_tensor> ggml_map_custom2_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ggml_custom2_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ggml_custom3_op_t,
        ffi.Int,
        ffi.Pointer<ffi.Void>)>()
external ffi.Pointer<ggml_tensor> ggml_map_custom3(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  ggml_custom3_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ggml_custom3_op_t,
        ffi.Int,
        ffi.Pointer<ffi.Void>)>()
external ffi.Pointer<ggml_tensor> ggml_map_custom3_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  ggml_custom3_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

/// loss function
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_cross_entropy_loss(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_cross_entropy_loss_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
);

/// AdamW optimizer step
/// Paper: https://arxiv.org/pdf/1711.05101v3.pdf
/// PyTorch: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html
@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_opt_step_adamw(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> grad,
  ffi.Pointer<ggml_tensor> m,
  ffi.Pointer<ggml_tensor> v,
  ffi.Pointer<ggml_tensor> adamw_params,
);

/// automatic differentiation
@ffi.Native<
    ffi.Void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)>()
external void ggml_build_forward_expand(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<
    ffi.Void Function(ffi.Pointer<ggml_context>, ffi.Pointer<ggml_context>,
        ffi.Pointer<ggml_cgraph>, ffi.Bool)>()
external void ggml_build_backward_expand(
  ffi.Pointer<ggml_context> ctx_static,
  ffi.Pointer<ggml_context> ctx_compute,
  ffi.Pointer<ggml_cgraph> cgraph,
  bool accumulate,
);

/// graph allocation in a context
@ffi.Native<ffi.Pointer<ggml_cgraph> Function(ffi.Pointer<ggml_context>)>()
external ffi.Pointer<ggml_cgraph> ggml_new_graph(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<
    ffi.Pointer<ggml_cgraph> Function(
        ffi.Pointer<ggml_context>, ffi.Size, ffi.Bool)>()
external ffi.Pointer<ggml_cgraph> ggml_new_graph_custom(
  ffi.Pointer<ggml_context> ctx,
  int size,
  bool grads,
);

@ffi.Native<
    ffi.Pointer<ggml_cgraph> Function(
        ffi.Pointer<ggml_context>, ffi.Pointer<ggml_cgraph>)>()
external ffi.Pointer<ggml_cgraph> ggml_graph_dup(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<
    ffi.Void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cgraph>)>()
external void ggml_graph_cpy(
  ffi.Pointer<ggml_cgraph> src,
  ffi.Pointer<ggml_cgraph> dst,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_cgraph>)>()
external void ggml_graph_reset(
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_cgraph>)>()
external void ggml_graph_clear(
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ggml_cgraph>)>()
external int ggml_graph_size(
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_cgraph>, ffi.Int)>()
external ffi.Pointer<ggml_tensor> ggml_graph_node(
  ffi.Pointer<ggml_cgraph> cgraph,
  int i,
);

@ffi.Native<
    ffi.Pointer<ffi.Pointer<ggml_tensor>> Function(ffi.Pointer<ggml_cgraph>)>()
external ffi.Pointer<ffi.Pointer<ggml_tensor>> ggml_graph_nodes(
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ggml_cgraph>)>()
external int ggml_graph_n_nodes(
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<
    ffi.Void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)>()
external void ggml_graph_add_node(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Size Function()>()
external int ggml_graph_overhead();

@ffi.Native<ffi.Size Function(ffi.Size, ffi.Bool)>()
external int ggml_graph_overhead_custom(
  int size,
  bool grads,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_cgraph>, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<ggml_tensor> ggml_graph_get_tensor(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ffi.Char> name,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_graph_get_grad(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ggml_tensor> node,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(
        ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_graph_get_grad_acc(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ggml_tensor> node,
);

@ffi.Native<
    ffi.Void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ffi.Char>)>()
external void ggml_graph_export(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ffi.Char> fname,
);

@ffi.Native<
    ffi.Pointer<ggml_cgraph> Function(
        ffi.Pointer<ffi.Char>,
        ffi.Pointer<ffi.Pointer<ggml_context>>,
        ffi.Pointer<ffi.Pointer<ggml_context>>)>()
external ffi.Pointer<ggml_cgraph> ggml_graph_import(
  ffi.Pointer<ffi.Char> fname,
  ffi.Pointer<ffi.Pointer<ggml_context>> ctx_data,
  ffi.Pointer<ffi.Pointer<ggml_context>> ctx_eval,
);

/// print info and performance information for the graph
@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_cgraph>)>()
external void ggml_graph_print(
  ffi.Pointer<ggml_cgraph> cgraph,
);

/// dump the graph into a file using the dot format
@ffi.Native<
    ffi.Void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cgraph>,
        ffi.Pointer<ffi.Char>)>()
external void ggml_graph_dump_dot(
  ffi.Pointer<ggml_cgraph> gb,
  ffi.Pointer<ggml_cgraph> gf,
  ffi.Pointer<ffi.Char> filename,
);

/// Set callback for all future logging events.
/// If this is not called, or NULL is supplied, everything is output on stderr.
@ffi.Native<ffi.Void Function(ggml_log_callback, ffi.Pointer<ffi.Void>)>()
external void ggml_log_set(
  ggml_log_callback log_callback,
  ffi.Pointer<ffi.Void> user_data,
);

@ffi.Native<ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_set_zero(
  ffi.Pointer<ggml_tensor> tensor,
);

/// - ggml_quantize_init can be called multiple times with the same type
/// it will only initialize the quantization tables for the first call or after ggml_quantize_free
/// automatically called by ggml_quantize_chunk for convenience
///
/// - ggml_quantize_free will free any memory allocated by ggml_quantize_init
/// call this at the end of the program to avoid memory leaks
///
/// note: these are thread-safe
@ffi.Native<ffi.Void Function(ffi.UnsignedInt)>(symbol: 'ggml_quantize_init')
external void _ggml_quantize_init(
  int type,
);

void ggml_quantize_init(
  ggml_type type,
) =>
    _ggml_quantize_init(
      type.value,
    );

@ffi.Native<ffi.Void Function()>()
external void ggml_quantize_free();

/// some quantization type cannot be used without an importance matrix
@ffi.Native<ffi.Bool Function(ffi.UnsignedInt)>(
    symbol: 'ggml_quantize_requires_imatrix')
external bool _ggml_quantize_requires_imatrix(
  int type,
);

bool ggml_quantize_requires_imatrix(
  ggml_type type,
) =>
    _ggml_quantize_requires_imatrix(
      type.value,
    );

/// calls ggml_quantize_init internally (i.e. can allocate memory)
@ffi.Native<
    ffi.Size Function(
        ffi.UnsignedInt,
        ffi.Pointer<ffi.Float>,
        ffi.Pointer<ffi.Void>,
        ffi.Int64,
        ffi.Int64,
        ffi.Int64,
        ffi.Pointer<ffi.Float>)>(symbol: 'ggml_quantize_chunk')
external int _ggml_quantize_chunk(
  int type,
  ffi.Pointer<ffi.Float> src,
  ffi.Pointer<ffi.Void> dst,
  int start,
  int nrows,
  int n_per_row,
  ffi.Pointer<ffi.Float> imatrix,
);

int ggml_quantize_chunk(
  ggml_type type,
  ffi.Pointer<ffi.Float> src,
  ffi.Pointer<ffi.Void> dst,
  int start,
  int nrows,
  int n_per_row,
  ffi.Pointer<ffi.Float> imatrix,
) =>
    _ggml_quantize_chunk(
      type.value,
      src,
      dst,
      start,
      nrows,
      n_per_row,
      imatrix,
    );

@ffi.Native<ffi.Pointer<ggml_type_traits> Function(ffi.UnsignedInt)>(
    symbol: 'ggml_get_type_traits')
external ffi.Pointer<ggml_type_traits> _ggml_get_type_traits(
  int type,
);

ffi.Pointer<ggml_type_traits> ggml_get_type_traits(
  ggml_type type,
) =>
    _ggml_get_type_traits(
      type.value,
    );

@ffi.Native<ggml_threadpool_params Function(ffi.Int)>()
external ggml_threadpool_params ggml_threadpool_params_default(
  int n_threads,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_threadpool_params>, ffi.Int)>()
external void ggml_threadpool_params_init(
  ffi.Pointer<ggml_threadpool_params> p,
  int n_threads,
);

@ffi.Native<
    ffi.Bool Function(ffi.Pointer<ggml_threadpool_params>,
        ffi.Pointer<ggml_threadpool_params>)>()
external bool ggml_threadpool_params_match(
  ffi.Pointer<ggml_threadpool_params> p0,
  ffi.Pointer<ggml_threadpool_params> p1,
);

@ffi.Native<ggml_tallocr Function(ggml_backend_buffer_t)>()
external ggml_tallocr ggml_tallocr_new(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<
        ffi.Int Function(ffi.Pointer<ggml_tallocr>, ffi.Pointer<ggml_tensor>)>(
    symbol: 'ggml_tallocr_alloc')
external int _ggml_tallocr_alloc(
  ffi.Pointer<ggml_tallocr> talloc,
  ffi.Pointer<ggml_tensor> tensor,
);

ggml_status ggml_tallocr_alloc(
  ffi.Pointer<ggml_tallocr> talloc,
  ffi.Pointer<ggml_tensor> tensor,
) =>
    ggml_status.fromValue(_ggml_tallocr_alloc(
      talloc,
      tensor,
    ));

@ffi.Native<ggml_gallocr_t Function(ggml_backend_buffer_type_t)>()
external ggml_gallocr_t ggml_gallocr_new(
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<
    ggml_gallocr_t Function(ffi.Pointer<ggml_backend_buffer_type_t>, ffi.Int)>()
external ggml_gallocr_t ggml_gallocr_new_n(
  ffi.Pointer<ggml_backend_buffer_type_t> bufts,
  int n_bufs,
);

@ffi.Native<ffi.Void Function(ggml_gallocr_t)>()
external void ggml_gallocr_free(
  ggml_gallocr_t galloc,
);

/// pre-allocate buffers from a measure graph - does not allocate or modify the graph
/// call with a worst-case graph to avoid buffer reallocations
/// not strictly required for single buffer usage: ggml_gallocr_alloc_graph will reallocate the buffers automatically if needed
/// returns false if the buffer allocation failed
@ffi.Native<ffi.Bool Function(ggml_gallocr_t, ffi.Pointer<ggml_cgraph>)>()
external bool ggml_gallocr_reserve(
  ggml_gallocr_t galloc,
  ffi.Pointer<ggml_cgraph> graph,
);

@ffi.Native<
    ffi.Bool Function(ggml_gallocr_t, ffi.Pointer<ggml_cgraph>,
        ffi.Pointer<ffi.Int>, ffi.Pointer<ffi.Int>)>()
external bool ggml_gallocr_reserve_n(
  ggml_gallocr_t galloc,
  ffi.Pointer<ggml_cgraph> graph,
  ffi.Pointer<ffi.Int> node_buffer_ids,
  ffi.Pointer<ffi.Int> leaf_buffer_ids,
);

/// automatic reallocation if the topology changes when using a single buffer
/// returns false if using multiple buffers and a re-allocation is needed (call ggml_gallocr_reserve_n first to set the node buffers)
@ffi.Native<ffi.Bool Function(ggml_gallocr_t, ffi.Pointer<ggml_cgraph>)>()
external bool ggml_gallocr_alloc_graph(
  ggml_gallocr_t galloc,
  ffi.Pointer<ggml_cgraph> graph,
);

@ffi.Native<ffi.Size Function(ggml_gallocr_t, ffi.Int)>()
external int ggml_gallocr_get_buffer_size(
  ggml_gallocr_t galloc,
  int buffer_id,
);

/// Utils
/// Create a buffer and allocate all the tensors in a ggml_context
@ffi.Native<
    ffi.Pointer<ggml_backend_buffer> Function(
        ffi.Pointer<ggml_context>, ggml_backend_buffer_type_t)>()
external ffi.Pointer<ggml_backend_buffer>
    ggml_backend_alloc_ctx_tensors_from_buft(
  ffi.Pointer<ggml_context> ctx,
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<
    ffi.Pointer<ggml_backend_buffer> Function(
        ffi.Pointer<ggml_context>, ggml_backend_t)>()
external ffi.Pointer<ggml_backend_buffer> ggml_backend_alloc_ctx_tensors(
  ffi.Pointer<ggml_context> ctx,
  ggml_backend_t backend,
);

/// Backend buffer type
@ffi.Native<ffi.Pointer<ffi.Char> Function(ggml_backend_buffer_type_t)>()
external ffi.Pointer<ffi.Char> ggml_backend_buft_name(
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<
    ggml_backend_buffer_t Function(ggml_backend_buffer_type_t, ffi.Size)>()
external ggml_backend_buffer_t ggml_backend_buft_alloc_buffer(
  ggml_backend_buffer_type_t buft,
  int size,
);

@ffi.Native<ffi.Size Function(ggml_backend_buffer_type_t)>()
external int ggml_backend_buft_get_alignment(
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<ffi.Size Function(ggml_backend_buffer_type_t)>()
external int ggml_backend_buft_get_max_size(
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<
    ffi.Size Function(ggml_backend_buffer_type_t, ffi.Pointer<ggml_tensor>)>()
external int ggml_backend_buft_get_alloc_size(
  ggml_backend_buffer_type_t buft,
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ggml_backend_buffer_type_t)>()
external bool ggml_backend_buft_is_host(
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<ggml_backend_dev_t Function(ggml_backend_buffer_type_t)>()
external ggml_backend_dev_t ggml_backend_buft_get_device(
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ggml_backend_buffer_t)>()
external ffi.Pointer<ffi.Char> ggml_backend_buffer_name(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Void Function(ggml_backend_buffer_t)>()
external void ggml_backend_buffer_free(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Pointer<ffi.Void> Function(ggml_backend_buffer_t)>()
external ffi.Pointer<ffi.Void> ggml_backend_buffer_get_base(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Size Function(ggml_backend_buffer_t)>()
external int ggml_backend_buffer_get_size(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Int Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>)>(
    symbol: 'ggml_backend_buffer_init_tensor')
external int _ggml_backend_buffer_init_tensor(
  ggml_backend_buffer_t buffer,
  ffi.Pointer<ggml_tensor> tensor,
);

ggml_status ggml_backend_buffer_init_tensor(
  ggml_backend_buffer_t buffer,
  ffi.Pointer<ggml_tensor> tensor,
) =>
    ggml_status.fromValue(_ggml_backend_buffer_init_tensor(
      buffer,
      tensor,
    ));

@ffi.Native<ffi.Size Function(ggml_backend_buffer_t)>()
external int ggml_backend_buffer_get_alignment(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Size Function(ggml_backend_buffer_t)>()
external int ggml_backend_buffer_get_max_size(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<
    ffi.Size Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>)>()
external int ggml_backend_buffer_get_alloc_size(
  ggml_backend_buffer_t buffer,
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Void Function(ggml_backend_buffer_t, ffi.Uint8)>()
external void ggml_backend_buffer_clear(
  ggml_backend_buffer_t buffer,
  int value,
);

@ffi.Native<ffi.Bool Function(ggml_backend_buffer_t)>()
external bool ggml_backend_buffer_is_host(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Void Function(ggml_backend_buffer_t, ffi.UnsignedInt)>(
    symbol: 'ggml_backend_buffer_set_usage')
external void _ggml_backend_buffer_set_usage(
  ggml_backend_buffer_t buffer,
  int usage,
);

void ggml_backend_buffer_set_usage(
  ggml_backend_buffer_t buffer,
  ggml_backend_buffer_usage usage,
) =>
    _ggml_backend_buffer_set_usage(
      buffer,
      usage.value,
    );

@ffi.Native<ffi.UnsignedInt Function(ggml_backend_buffer_t)>(
    symbol: 'ggml_backend_buffer_get_usage')
external int _ggml_backend_buffer_get_usage(
  ggml_backend_buffer_t buffer,
);

ggml_backend_buffer_usage ggml_backend_buffer_get_usage(
  ggml_backend_buffer_t buffer,
) =>
    ggml_backend_buffer_usage.fromValue(_ggml_backend_buffer_get_usage(
      buffer,
    ));

@ffi.Native<ggml_backend_buffer_type_t Function(ggml_backend_buffer_t)>()
external ggml_backend_buffer_type_t ggml_backend_buffer_get_type(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Void Function(ggml_backend_buffer_t)>()
external void ggml_backend_buffer_reset(
  ggml_backend_buffer_t buffer,
);

/// tensor copy between different backends
@ffi.Native<
    ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>()
external void ggml_backend_tensor_copy(
  ffi.Pointer<ggml_tensor> src,
  ffi.Pointer<ggml_tensor> dst,
);

/// Backend (stream)
@ffi.Native<ggml_guid_t Function(ggml_backend_t)>()
external ggml_guid_t ggml_backend_guid(
  ggml_backend_t backend,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ggml_backend_t)>()
external ffi.Pointer<ffi.Char> ggml_backend_name(
  ggml_backend_t backend,
);

@ffi.Native<ffi.Void Function(ggml_backend_t)>()
external void ggml_backend_free(
  ggml_backend_t backend,
);

@ffi.Native<ggml_backend_buffer_type_t Function(ggml_backend_t)>()
external ggml_backend_buffer_type_t ggml_backend_get_default_buffer_type(
  ggml_backend_t backend,
);

@ffi.Native<ggml_backend_buffer_t Function(ggml_backend_t, ffi.Size)>()
external ggml_backend_buffer_t ggml_backend_alloc_buffer(
  ggml_backend_t backend,
  int size,
);

@ffi.Native<ffi.Size Function(ggml_backend_t)>()
external int ggml_backend_get_alignment(
  ggml_backend_t backend,
);

@ffi.Native<ffi.Size Function(ggml_backend_t)>()
external int ggml_backend_get_max_size(
  ggml_backend_t backend,
);

@ffi.Native<
    ffi.Void Function(ggml_backend_t, ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ffi.Void>, ffi.Size, ffi.Size)>()
external void ggml_backend_tensor_set_async(
  ggml_backend_t backend,
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> data,
  int offset,
  int size,
);

@ffi.Native<
    ffi.Void Function(ggml_backend_t, ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ffi.Void>, ffi.Size, ffi.Size)>()
external void ggml_backend_tensor_get_async(
  ggml_backend_t backend,
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> data,
  int offset,
  int size,
);

/// "offset" refers to the offset in tensor->data for setting/getting data
@ffi.Native<
    ffi.Void Function(
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Void>, ffi.Size, ffi.Size)>()
external void ggml_backend_tensor_set(
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> data,
  int offset,
  int size,
);

@ffi.Native<
    ffi.Void Function(
        ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Void>, ffi.Size, ffi.Size)>()
external void ggml_backend_tensor_get(
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> data,
  int offset,
  int size,
);

@ffi.Native<
    ffi.Void Function(
        ffi.Pointer<ggml_tensor>, ffi.Uint8, ffi.Size, ffi.Size)>()
external void ggml_backend_tensor_memset(
  ffi.Pointer<ggml_tensor> tensor,
  int value,
  int offset,
  int size,
);

@ffi.Native<ffi.Void Function(ggml_backend_t)>()
external void ggml_backend_synchronize(
  ggml_backend_t backend,
);

@ffi.Native<
    ggml_backend_graph_plan_t Function(
        ggml_backend_t, ffi.Pointer<ggml_cgraph>)>()
external ggml_backend_graph_plan_t ggml_backend_graph_plan_create(
  ggml_backend_t backend,
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<ffi.Void Function(ggml_backend_t, ggml_backend_graph_plan_t)>()
external void ggml_backend_graph_plan_free(
  ggml_backend_t backend,
  ggml_backend_graph_plan_t plan,
);

@ffi.Native<ffi.Int Function(ggml_backend_t, ggml_backend_graph_plan_t)>(
    symbol: 'ggml_backend_graph_plan_compute')
external int _ggml_backend_graph_plan_compute(
  ggml_backend_t backend,
  ggml_backend_graph_plan_t plan,
);

ggml_status ggml_backend_graph_plan_compute(
  ggml_backend_t backend,
  ggml_backend_graph_plan_t plan,
) =>
    ggml_status.fromValue(_ggml_backend_graph_plan_compute(
      backend,
      plan,
    ));

@ffi.Native<ffi.Int Function(ggml_backend_t, ffi.Pointer<ggml_cgraph>)>(
    symbol: 'ggml_backend_graph_compute')
external int _ggml_backend_graph_compute(
  ggml_backend_t backend,
  ffi.Pointer<ggml_cgraph> cgraph,
);

ggml_status ggml_backend_graph_compute(
  ggml_backend_t backend,
  ffi.Pointer<ggml_cgraph> cgraph,
) =>
    ggml_status.fromValue(_ggml_backend_graph_compute(
      backend,
      cgraph,
    ));

@ffi.Native<ffi.Int Function(ggml_backend_t, ffi.Pointer<ggml_cgraph>)>(
    symbol: 'ggml_backend_graph_compute_async')
external int _ggml_backend_graph_compute_async(
  ggml_backend_t backend,
  ffi.Pointer<ggml_cgraph> cgraph,
);

ggml_status ggml_backend_graph_compute_async(
  ggml_backend_t backend,
  ffi.Pointer<ggml_cgraph> cgraph,
) =>
    ggml_status.fromValue(_ggml_backend_graph_compute_async(
      backend,
      cgraph,
    ));

/// NOTE: will be removed, use device version instead
@ffi.Native<ffi.Bool Function(ggml_backend_t, ffi.Pointer<ggml_tensor>)>()
external bool ggml_backend_supports_op(
  ggml_backend_t backend,
  ffi.Pointer<ggml_tensor> op,
);

@ffi.Native<ffi.Bool Function(ggml_backend_t, ggml_backend_buffer_type_t)>()
external bool ggml_backend_supports_buft(
  ggml_backend_t backend,
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<ffi.Bool Function(ggml_backend_t, ffi.Pointer<ggml_tensor>)>()
external bool ggml_backend_offload_op(
  ggml_backend_t backend,
  ffi.Pointer<ggml_tensor> op,
);

/// asynchronous copy
/// the copy is performed after all the currently queued operations in backend_src
/// backend_dst will wait for the copy to complete before performing other operations
/// automatic fallback to sync copy if async is not supported
@ffi.Native<
    ffi.Void Function(ggml_backend_t, ggml_backend_t, ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ggml_tensor>)>()
external void ggml_backend_tensor_copy_async(
  ggml_backend_t backend_src,
  ggml_backend_t backend_dst,
  ffi.Pointer<ggml_tensor> src,
  ffi.Pointer<ggml_tensor> dst,
);

@ffi.Native<ggml_backend_dev_t Function(ggml_backend_t)>()
external ggml_backend_dev_t ggml_backend_get_device(
  ggml_backend_t backend,
);

/// Events
@ffi.Native<ggml_backend_event_t Function(ggml_backend_dev_t)>()
external ggml_backend_event_t ggml_backend_event_new(
  ggml_backend_dev_t device,
);

@ffi.Native<ffi.Void Function(ggml_backend_event_t)>()
external void ggml_backend_event_free(
  ggml_backend_event_t event,
);

@ffi.Native<ffi.Void Function(ggml_backend_event_t, ggml_backend_t)>()
external void ggml_backend_event_record(
  ggml_backend_event_t event,
  ggml_backend_t backend,
);

@ffi.Native<ffi.Void Function(ggml_backend_event_t)>()
external void ggml_backend_event_synchronize(
  ggml_backend_event_t event,
);

@ffi.Native<ffi.Void Function(ggml_backend_t, ggml_backend_event_t)>()
external void ggml_backend_event_wait(
  ggml_backend_t backend,
  ggml_backend_event_t event,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ggml_backend_dev_t)>()
external ffi.Pointer<ffi.Char> ggml_backend_dev_name(
  ggml_backend_dev_t device,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ggml_backend_dev_t)>()
external ffi.Pointer<ffi.Char> ggml_backend_dev_description(
  ggml_backend_dev_t device,
);

@ffi.Native<
    ffi.Void Function(
        ggml_backend_dev_t, ffi.Pointer<ffi.Size>, ffi.Pointer<ffi.Size>)>()
external void ggml_backend_dev_memory(
  ggml_backend_dev_t device,
  ffi.Pointer<ffi.Size> free,
  ffi.Pointer<ffi.Size> total,
);

@ffi.Native<ffi.UnsignedInt Function(ggml_backend_dev_t)>(
    symbol: 'ggml_backend_dev_type')
external int _ggml_backend_dev_type$1(
  ggml_backend_dev_t device,
);

ggml_backend_dev_type ggml_backend_dev_type$1(
  ggml_backend_dev_t device,
) =>
    ggml_backend_dev_type.fromValue(_ggml_backend_dev_type$1(
      device,
    ));

@ffi.Native<
    ffi.Void Function(
        ggml_backend_dev_t, ffi.Pointer<ggml_backend_dev_props>)>()
external void ggml_backend_dev_get_props(
  ggml_backend_dev_t device,
  ffi.Pointer<ggml_backend_dev_props> props,
);

@ffi.Native<ggml_backend_reg_t Function(ggml_backend_dev_t)>()
external ggml_backend_reg_t ggml_backend_dev_backend_reg(
  ggml_backend_dev_t device,
);

@ffi.Native<
    ggml_backend_t Function(ggml_backend_dev_t, ffi.Pointer<ffi.Char>)>()
external ggml_backend_t ggml_backend_dev_init(
  ggml_backend_dev_t device,
  ffi.Pointer<ffi.Char> params,
);

@ffi.Native<ggml_backend_buffer_type_t Function(ggml_backend_dev_t)>()
external ggml_backend_buffer_type_t ggml_backend_dev_buffer_type(
  ggml_backend_dev_t device,
);

@ffi.Native<ggml_backend_buffer_type_t Function(ggml_backend_dev_t)>()
external ggml_backend_buffer_type_t ggml_backend_dev_host_buffer_type(
  ggml_backend_dev_t device,
);

@ffi.Native<
    ggml_backend_buffer_t Function(
        ggml_backend_dev_t, ffi.Pointer<ffi.Void>, ffi.Size, ffi.Size)>()
external ggml_backend_buffer_t ggml_backend_dev_buffer_from_host_ptr(
  ggml_backend_dev_t device,
  ffi.Pointer<ffi.Void> ptr,
  int size,
  int max_tensor_size,
);

@ffi.Native<ffi.Bool Function(ggml_backend_dev_t, ffi.Pointer<ggml_tensor>)>()
external bool ggml_backend_dev_supports_op(
  ggml_backend_dev_t device,
  ffi.Pointer<ggml_tensor> op,
);

@ffi.Native<ffi.Bool Function(ggml_backend_dev_t, ggml_backend_buffer_type_t)>()
external bool ggml_backend_dev_supports_buft(
  ggml_backend_dev_t device,
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<ffi.Bool Function(ggml_backend_dev_t, ffi.Pointer<ggml_tensor>)>()
external bool ggml_backend_dev_offload_op(
  ggml_backend_dev_t device,
  ffi.Pointer<ggml_tensor> op,
);

/// Backend (reg)
@ffi.Native<ffi.Pointer<ffi.Char> Function(ggml_backend_reg_t)>()
external ffi.Pointer<ffi.Char> ggml_backend_reg_name(
  ggml_backend_reg_t reg,
);

@ffi.Native<ffi.Size Function(ggml_backend_reg_t)>()
external int ggml_backend_reg_dev_count(
  ggml_backend_reg_t reg,
);

@ffi.Native<ggml_backend_dev_t Function(ggml_backend_reg_t, ffi.Size)>()
external ggml_backend_dev_t ggml_backend_reg_dev_get(
  ggml_backend_reg_t reg,
  int index,
);

@ffi.Native<
    ffi.Pointer<ffi.Void> Function(ggml_backend_reg_t, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<ffi.Void> ggml_backend_reg_get_proc_address(
  ggml_backend_reg_t reg,
  ffi.Pointer<ffi.Char> name,
);

/// Backend registry
@ffi.Native<ffi.Void Function(ggml_backend_dev_t)>()
external void ggml_backend_device_register(
  ggml_backend_dev_t device,
);

/// Backend (reg) enumeration
@ffi.Native<ffi.Size Function()>()
external int ggml_backend_reg_count();

@ffi.Native<ggml_backend_reg_t Function(ffi.Size)>()
external ggml_backend_reg_t ggml_backend_reg_get(
  int index,
);

@ffi.Native<ggml_backend_reg_t Function(ffi.Pointer<ffi.Char>)>()
external ggml_backend_reg_t ggml_backend_reg_by_name(
  ffi.Pointer<ffi.Char> name,
);

/// Device enumeration
@ffi.Native<ffi.Size Function()>()
external int ggml_backend_dev_count();

@ffi.Native<ggml_backend_dev_t Function(ffi.Size)>()
external ggml_backend_dev_t ggml_backend_dev_get(
  int index,
);

@ffi.Native<ggml_backend_dev_t Function(ffi.Pointer<ffi.Char>)>()
external ggml_backend_dev_t ggml_backend_dev_by_name(
  ffi.Pointer<ffi.Char> name,
);

@ffi.Native<ggml_backend_dev_t Function(ffi.UnsignedInt)>(
    symbol: 'ggml_backend_dev_by_type')
external ggml_backend_dev_t _ggml_backend_dev_by_type(
  int type,
);

ggml_backend_dev_t ggml_backend_dev_by_type(
  ggml_backend_dev_type type,
) =>
    _ggml_backend_dev_by_type(
      type.value,
    );

/// Direct backend (stream) initialization
/// = ggml_backend_dev_init(ggml_backend_dev_by_name(name), params)
@ffi.Native<
    ggml_backend_t Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>()
external ggml_backend_t ggml_backend_init_by_name(
  ffi.Pointer<ffi.Char> name,
  ffi.Pointer<ffi.Char> params,
);

/// = ggml_backend_dev_init(ggml_backend_dev_by_type(type), params)
@ffi.Native<ggml_backend_t Function(ffi.UnsignedInt, ffi.Pointer<ffi.Char>)>(
    symbol: 'ggml_backend_init_by_type')
external ggml_backend_t _ggml_backend_init_by_type(
  int type,
  ffi.Pointer<ffi.Char> params,
);

ggml_backend_t ggml_backend_init_by_type(
  ggml_backend_dev_type type,
  ffi.Pointer<ffi.Char> params,
) =>
    _ggml_backend_init_by_type(
      type.value,
      params,
    );

/// = ggml_backend_dev_init(ggml_backend_dev_by_type(GPU) OR ggml_backend_dev_by_type(CPU), NULL)
@ffi.Native<ggml_backend_t Function()>()
external ggml_backend_t ggml_backend_init_best();

/// Load a backend from a dynamic library and register it
@ffi.Native<ggml_backend_reg_t Function(ffi.Pointer<ffi.Char>)>()
external ggml_backend_reg_t ggml_backend_load(
  ffi.Pointer<ffi.Char> path,
);

/// Unload a backend if loaded dynamically and unregister it
@ffi.Native<ffi.Void Function(ggml_backend_reg_t)>()
external void ggml_backend_unload(
  ggml_backend_reg_t reg,
);

/// Load all known backends from dynamic libraries
@ffi.Native<ffi.Void Function()>()
external void ggml_backend_load_all();

@ffi.Native<ffi.Void Function(ffi.Pointer<ffi.Char>)>()
external void ggml_backend_load_all_from_path(
  ffi.Pointer<ffi.Char> dir_path,
);

/// Initialize a backend scheduler, backends with low index are given priority over backends with high index
@ffi.Native<
    ggml_backend_sched_t Function(ffi.Pointer<ggml_backend_t>,
        ffi.Pointer<ggml_backend_buffer_type_t>, ffi.Int, ffi.Size, ffi.Bool)>()
external ggml_backend_sched_t ggml_backend_sched_new(
  ffi.Pointer<ggml_backend_t> backends,
  ffi.Pointer<ggml_backend_buffer_type_t> bufts,
  int n_backends,
  int graph_size,
  bool parallel,
);

@ffi.Native<ffi.Void Function(ggml_backend_sched_t)>()
external void ggml_backend_sched_free(
  ggml_backend_sched_t sched,
);

/// Initialize backend buffers from a measure graph
@ffi.Native<ffi.Bool Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>()
external bool ggml_backend_sched_reserve(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> measure_graph,
);

@ffi.Native<ffi.Int Function(ggml_backend_sched_t)>()
external int ggml_backend_sched_get_n_backends(
  ggml_backend_sched_t sched,
);

@ffi.Native<ggml_backend_t Function(ggml_backend_sched_t, ffi.Int)>()
external ggml_backend_t ggml_backend_sched_get_backend(
  ggml_backend_sched_t sched,
  int i,
);

/// Get the number of splits of the last graph
@ffi.Native<ffi.Int Function(ggml_backend_sched_t)>()
external int ggml_backend_sched_get_n_splits(
  ggml_backend_sched_t sched,
);

@ffi.Native<ffi.Int Function(ggml_backend_sched_t)>()
external int ggml_backend_sched_get_n_copies(
  ggml_backend_sched_t sched,
);

@ffi.Native<ffi.Size Function(ggml_backend_sched_t, ggml_backend_t)>()
external int ggml_backend_sched_get_buffer_size(
  ggml_backend_sched_t sched,
  ggml_backend_t backend,
);

@ffi.Native<
    ffi.Void Function(
        ggml_backend_sched_t, ffi.Pointer<ggml_tensor>, ggml_backend_t)>()
external void ggml_backend_sched_set_tensor_backend(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_tensor> node,
  ggml_backend_t backend,
);

@ffi.Native<
    ggml_backend_t Function(ggml_backend_sched_t, ffi.Pointer<ggml_tensor>)>()
external ggml_backend_t ggml_backend_sched_get_tensor_backend(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_tensor> node,
);

/// Allocate and compute graph on the backend scheduler
@ffi.Native<ffi.Bool Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>()
external bool ggml_backend_sched_alloc_graph(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> graph,
);

@ffi.Native<ffi.Int Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>(
    symbol: 'ggml_backend_sched_graph_compute')
external int _ggml_backend_sched_graph_compute(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> graph,
);

ggml_status ggml_backend_sched_graph_compute(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> graph,
) =>
    ggml_status.fromValue(_ggml_backend_sched_graph_compute(
      sched,
      graph,
    ));

@ffi.Native<ffi.Int Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>(
    symbol: 'ggml_backend_sched_graph_compute_async')
external int _ggml_backend_sched_graph_compute_async(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> graph,
);

ggml_status ggml_backend_sched_graph_compute_async(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> graph,
) =>
    ggml_status.fromValue(_ggml_backend_sched_graph_compute_async(
      sched,
      graph,
    ));

@ffi.Native<ffi.Void Function(ggml_backend_sched_t)>()
external void ggml_backend_sched_synchronize(
  ggml_backend_sched_t sched,
);

/// Reset all assignments and allocators - must be called before changing the node backends or allocating a new graph.
/// This in effect deallocates all tensors that were previously allocated and leaves them with dangling pointers.
/// The correct way to use this API is to discard the deallocated tensors and create new ones.
@ffi.Native<ffi.Void Function(ggml_backend_sched_t)>()
external void ggml_backend_sched_reset(
  ggml_backend_sched_t sched,
);

/// Set a callback to be called for each resulting node during graph compute
@ffi.Native<
    ffi.Void Function(ggml_backend_sched_t, ggml_backend_sched_eval_callback,
        ffi.Pointer<ffi.Void>)>()
external void ggml_backend_sched_set_eval_callback(
  ggml_backend_sched_t sched,
  ggml_backend_sched_eval_callback callback,
  ffi.Pointer<ffi.Void> user_data,
);

/// Copy a graph to a different backend
@ffi.Native<
    ggml_backend_graph_copy Function(ggml_backend_t,
        ffi.Pointer<ggml_cgraph>)>(symbol: 'ggml_backend_graph_copy')
external ggml_backend_graph_copy ggml_backend_graph_copy$1(
  ggml_backend_t backend,
  ffi.Pointer<ggml_cgraph> graph,
);

@ffi.Native<ffi.Void Function(ggml_backend_graph_copy)>()
external void ggml_backend_graph_copy_free(
  ggml_backend_graph_copy copy,
);

/// Compare the output of two backends
@ffi.Native<
    ffi.Bool Function(ggml_backend_t, ggml_backend_t, ffi.Pointer<ggml_cgraph>,
        ggml_backend_eval_callback, ffi.Pointer<ffi.Void>)>()
external bool ggml_backend_compare_graph_backend(
  ggml_backend_t backend1,
  ggml_backend_t backend2,
  ffi.Pointer<ggml_cgraph> graph,
  ggml_backend_eval_callback callback,
  ffi.Pointer<ffi.Void> user_data,
);

/// Tensor initialization
@ffi.Native<
    ffi.Int Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>,
        ffi.Pointer<ffi.Void>)>(symbol: 'ggml_backend_tensor_alloc')
external int _ggml_backend_tensor_alloc(
  ggml_backend_buffer_t buffer,
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> addr,
);

ggml_status ggml_backend_tensor_alloc(
  ggml_backend_buffer_t buffer,
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> addr,
) =>
    ggml_status.fromValue(_ggml_backend_tensor_alloc(
      buffer,
      tensor,
      addr,
    ));

@ffi.Native<ffi.Int Function(ffi.Pointer<ggml_tensor>)>(
    symbol: 'ggml_backend_view_init')
external int _ggml_backend_view_init(
  ffi.Pointer<ggml_tensor> tensor,
);

ggml_status ggml_backend_view_init(
  ffi.Pointer<ggml_tensor> tensor,
) =>
    ggml_status.fromValue(_ggml_backend_view_init(
      tensor,
    ));

/// CPU buffer types are always available
@ffi.Native<ggml_backend_buffer_t Function(ffi.Pointer<ffi.Void>, ffi.Size)>()
external ggml_backend_buffer_t ggml_backend_cpu_buffer_from_ptr(
  ffi.Pointer<ffi.Void> ptr,
  int size,
);

@ffi.Native<ggml_backend_buffer_type_t Function()>()
external ggml_backend_buffer_type_t ggml_backend_cpu_buffer_type();

@ffi.Native<ffi.Void Function(ffi.UnsignedInt)>(symbol: 'ggml_numa_init')
external void _ggml_numa_init(
  int numa,
);

void ggml_numa_init(
  ggml_numa_strategy numa,
) =>
    _ggml_numa_init(
      numa.value,
    );

@ffi.Native<ffi.Bool Function()>()
external bool ggml_is_numa();

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>, ffi.Int32)>()
external ffi.Pointer<ggml_tensor> ggml_new_i32(
  ffi.Pointer<ggml_context> ctx,
  int value,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_new_f32(
  ffi.Pointer<ggml_context> ctx,
  double value,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>, ffi.Int32)>()
external ffi.Pointer<ggml_tensor> ggml_set_i32(
  ffi.Pointer<ggml_tensor> tensor,
  int value,
);

@ffi.Native<
    ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>, ffi.Float)>()
external ffi.Pointer<ggml_tensor> ggml_set_f32(
  ffi.Pointer<ggml_tensor> tensor,
  double value,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<ggml_tensor>, ffi.Int)>()
external int ggml_get_i32_1d(
  ffi.Pointer<ggml_tensor> tensor,
  int i,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int32)>()
external void ggml_set_i32_1d(
  ffi.Pointer<ggml_tensor> tensor,
  int i,
  int value,
);

@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int, ffi.Int, ffi.Int)>()
external int ggml_get_i32_nd(
  ffi.Pointer<ggml_tensor> tensor,
  int i0,
  int i1,
  int i2,
  int i3,
);

@ffi.Native<
    ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int, ffi.Int,
        ffi.Int, ffi.Int32)>()
external void ggml_set_i32_nd(
  ffi.Pointer<ggml_tensor> tensor,
  int i0,
  int i1,
  int i2,
  int i3,
  int value,
);

@ffi.Native<ffi.Float Function(ffi.Pointer<ggml_tensor>, ffi.Int)>()
external double ggml_get_f32_1d(
  ffi.Pointer<ggml_tensor> tensor,
  int i,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Float)>()
external void ggml_set_f32_1d(
  ffi.Pointer<ggml_tensor> tensor,
  int i,
  double value,
);

@ffi.Native<
    ffi.Float Function(
        ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int, ffi.Int, ffi.Int)>()
external double ggml_get_f32_nd(
  ffi.Pointer<ggml_tensor> tensor,
  int i0,
  int i1,
  int i2,
  int i3,
);

@ffi.Native<
    ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int, ffi.Int,
        ffi.Int, ffi.Float)>()
external void ggml_set_f32_nd(
  ffi.Pointer<ggml_tensor> tensor,
  int i0,
  int i1,
  int i2,
  int i3,
  double value,
);

@ffi.Native<
    ffi.Pointer<ggml_threadpool> Function(
        ffi.Pointer<ggml_threadpool_params>)>()
external ffi.Pointer<ggml_threadpool> ggml_threadpool_new(
  ffi.Pointer<ggml_threadpool_params> params,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_threadpool>)>()
external void ggml_threadpool_free(
  ffi.Pointer<ggml_threadpool> threadpool,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ggml_threadpool>)>()
external int ggml_threadpool_get_n_threads(
  ffi.Pointer<ggml_threadpool> threadpool,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_threadpool>)>()
external void ggml_threadpool_pause(
  ffi.Pointer<ggml_threadpool> threadpool,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_threadpool>)>()
external void ggml_threadpool_resume(
  ffi.Pointer<ggml_threadpool> threadpool,
);

/// ggml_graph_plan() has to be called before ggml_graph_compute()
/// when plan.work_size > 0, caller must allocate memory for plan.work_data
@ffi.Native<
    ggml_cplan Function(
        ffi.Pointer<ggml_cgraph>, ffi.Int, ffi.Pointer<ggml_threadpool>)>()
external ggml_cplan ggml_graph_plan(
  ffi.Pointer<ggml_cgraph> cgraph,
  int n_threads,
  ffi.Pointer<ggml_threadpool> threadpool,
);

@ffi.Native<
        ffi.Int Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cplan>)>(
    symbol: 'ggml_graph_compute')
external int _ggml_graph_compute(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ggml_cplan> cplan,
);

ggml_status ggml_graph_compute(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ggml_cplan> cplan,
) =>
    ggml_status.fromValue(_ggml_graph_compute(
      cgraph,
      cplan,
    ));

/// same as ggml_graph_compute() but the work data is allocated as a part of the context
/// note: the drawback of this API is that you must have ensured that the context has enough memory for the work data
@ffi.Native<
    ffi.Int Function(ffi.Pointer<ggml_context>, ffi.Pointer<ggml_cgraph>,
        ffi.Int)>(symbol: 'ggml_graph_compute_with_ctx')
external int _ggml_graph_compute_with_ctx(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_cgraph> cgraph,
  int n_threads,
);

ggml_status ggml_graph_compute_with_ctx(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_cgraph> cgraph,
  int n_threads,
) =>
    ggml_status.fromValue(_ggml_graph_compute_with_ctx(
      ctx,
      cgraph,
      n_threads,
    ));

/// x86
@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_sse3();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_ssse3();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx_vnni();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx2();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_bmi2();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_f16c();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_fma();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx512();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx512_vbmi();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx512_vnni();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx512_bf16();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_amx_int8();

/// ARM
@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_neon();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_arm_fma();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_fp16_va();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_dotprod();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_matmul_int8();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_sve();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_get_sve_cnt();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_sme();

/// other
@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_riscv_v();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_vsx();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_vxe();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_wasm_simd();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_llamafile();

@ffi.Native<ffi.Pointer<ggml_type_traits_cpu> Function(ffi.UnsignedInt)>(
    symbol: 'ggml_get_type_traits_cpu')
external ffi.Pointer<ggml_type_traits_cpu> _ggml_get_type_traits_cpu(
  int type,
);

ffi.Pointer<ggml_type_traits_cpu> ggml_get_type_traits_cpu(
  ggml_type type,
) =>
    _ggml_get_type_traits_cpu(
      type.value,
    );

@ffi.Native<ffi.Void Function()>()
external void ggml_cpu_init();

/// CPU backend
@ffi.Native<ggml_backend_t Function()>()
external ggml_backend_t ggml_backend_cpu_init();

@ffi.Native<ffi.Bool Function(ggml_backend_t)>()
external bool ggml_backend_is_cpu(
  ggml_backend_t backend,
);

@ffi.Native<ffi.Void Function(ggml_backend_t, ffi.Int)>()
external void ggml_backend_cpu_set_n_threads(
  ggml_backend_t backend_cpu,
  int n_threads,
);

@ffi.Native<ffi.Void Function(ggml_backend_t, ggml_threadpool_t)>()
external void ggml_backend_cpu_set_threadpool(
  ggml_backend_t backend_cpu,
  ggml_threadpool_t threadpool,
);

@ffi.Native<
    ffi.Void Function(
        ggml_backend_t, ggml_abort_callback, ffi.Pointer<ffi.Void>)>()
external void ggml_backend_cpu_set_abort_callback(
  ggml_backend_t backend_cpu,
  ggml_abort_callback abort_callback,
  ffi.Pointer<ffi.Void> abort_callback_data,
);

@ffi.Native<ggml_backend_reg_t Function()>()
external ggml_backend_reg_t ggml_backend_cpu_reg();

/// Helpers for getting default parameters
/// TODO: update API to start accepting pointers to params structs (https://github.com/ggml-org/llama.cpp/discussions/9172)
@ffi.Native<llama_model_params Function()>()
external llama_model_params llama_model_default_params();

@ffi.Native<llama_context_params Function()>()
external llama_context_params llama_context_default_params();

@ffi.Native<llama_sampler_chain_params Function()>()
external llama_sampler_chain_params llama_sampler_chain_default_params();

@ffi.Native<llama_model_quantize_params Function()>()
external llama_model_quantize_params llama_model_quantize_default_params();

/// Initialize the llama + ggml backend
/// If numa is true, use NUMA optimizations
/// Call once at the start of the program
@ffi.Native<ffi.Void Function()>()
external void llama_backend_init();

/// Call once at the end of the program - currently only used for MPI
@ffi.Native<ffi.Void Function()>()
external void llama_backend_free();

/// optional:
@ffi.Native<ffi.Void Function(ffi.UnsignedInt)>(symbol: 'llama_numa_init')
external void _llama_numa_init(
  int numa,
);

void llama_numa_init(
  ggml_numa_strategy numa,
) =>
    _llama_numa_init(
      numa.value,
    );

/// Optional: an auto threadpool gets created in ggml if not passed explicitly
@ffi.Native<
    ffi.Void Function(
        ffi.Pointer<llama_context>, ggml_threadpool_t, ggml_threadpool_t)>()
external void llama_attach_threadpool(
  ffi.Pointer<llama_context> ctx,
  ggml_threadpool_t threadpool,
  ggml_threadpool_t threadpool_batch,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_detach_threadpool(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<
    ffi.Pointer<llama_model> Function(
        ffi.Pointer<ffi.Char>, llama_model_params)>()
external ffi.Pointer<llama_model> llama_load_model_from_file(
  ffi.Pointer<ffi.Char> path_model,
  llama_model_params params,
);

/// Load the model from a file
/// If the file is split into multiple parts, the file name must follow this pattern: <name>-%05d-of-%05d.gguf
/// If the split file name does not follow this pattern, use llama_model_load_from_splits
@ffi.Native<
    ffi.Pointer<llama_model> Function(
        ffi.Pointer<ffi.Char>, llama_model_params)>()
external ffi.Pointer<llama_model> llama_model_load_from_file(
  ffi.Pointer<ffi.Char> path_model,
  llama_model_params params,
);

/// Load the model from multiple splits (support custom naming scheme)
/// The paths must be in the correct order
@ffi.Native<
    ffi.Pointer<llama_model> Function(
        ffi.Pointer<ffi.Pointer<ffi.Char>>, ffi.Size, llama_model_params)>()
external ffi.Pointer<llama_model> llama_model_load_from_splits(
  ffi.Pointer<ffi.Pointer<ffi.Char>> paths,
  int n_paths,
  llama_model_params params,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_model>)>()
external void llama_free_model(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_model>)>()
external void llama_model_free(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<
    ffi.Pointer<llama_context> Function(
        ffi.Pointer<llama_model>, llama_context_params)>()
external ffi.Pointer<llama_context> llama_init_from_model(
  ffi.Pointer<llama_model> model,
  llama_context_params params,
);

@ffi.Native<
    ffi.Pointer<llama_context> Function(
        ffi.Pointer<llama_model>, llama_context_params)>()
external ffi.Pointer<llama_context> llama_new_context_with_model(
  ffi.Pointer<llama_model> model,
  llama_context_params params,
);

/// Frees all allocated memory
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_free(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Int64 Function()>()
external int llama_time_us();

@ffi.Native<ffi.Size Function()>()
external int llama_max_devices();

@ffi.Native<ffi.Bool Function()>()
external bool llama_supports_mmap();

@ffi.Native<ffi.Bool Function()>()
external bool llama_supports_mlock();

@ffi.Native<ffi.Bool Function()>()
external bool llama_supports_gpu_offload();

@ffi.Native<ffi.Bool Function()>()
external bool llama_supports_rpc();

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_ctx(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_batch(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_ubatch(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_seq_max(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_n_ctx_train(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_n_embd(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_n_layer(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_n_head(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_vocab>)>()
external int llama_n_vocab(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Pointer<llama_model> Function(ffi.Pointer<llama_context>)>()
external ffi.Pointer<llama_model> llama_get_model(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_pooling_type')
external int _llama_pooling_type$1(
  ffi.Pointer<llama_context> ctx,
);

llama_pooling_type llama_pooling_type$1(
  ffi.Pointer<llama_context> ctx,
) =>
    llama_pooling_type.fromValue(_llama_pooling_type$1(
      ctx,
    ));

@ffi.Native<ffi.Pointer<llama_vocab> Function(ffi.Pointer<llama_model>)>()
external ffi.Pointer<llama_vocab> llama_model_get_vocab(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_rope_type')
external int _llama_model_rope_type(
  ffi.Pointer<llama_model> model,
);

llama_rope_type llama_model_rope_type(
  ffi.Pointer<llama_model> model,
) =>
    llama_rope_type.fromValue(_llama_model_rope_type(
      model,
    ));

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_ctx_train(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_embd(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_layer(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_head(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_head_kv(
  ffi.Pointer<llama_model> model,
);

/// Get the model's RoPE frequency scaling factor
@ffi.Native<ffi.Float Function(ffi.Pointer<llama_model>)>()
external double llama_model_rope_freq_scale_train(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_type')
external int _llama_vocab_type$1(
  ffi.Pointer<llama_vocab> vocab,
);

llama_vocab_type llama_vocab_type$1(
  ffi.Pointer<llama_vocab> vocab,
) =>
    llama_vocab_type.fromValue(_llama_vocab_type$1(
      vocab,
    ));

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_n_tokens(
  ffi.Pointer<llama_vocab> vocab,
);

/// Get metadata value as a string by key name
@ffi.Native<
    ffi.Int32 Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>,
        ffi.Pointer<ffi.Char>, ffi.Size)>()
external int llama_model_meta_val_str(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> key,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Get the number of metadata key/value pairs
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_meta_count(
  ffi.Pointer<llama_model> model,
);

/// Get metadata key name by index
@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<llama_model>, ffi.Int32, ffi.Pointer<ffi.Char>, ffi.Size)>()
external int llama_model_meta_key_by_index(
  ffi.Pointer<llama_model> model,
  int i,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Get metadata value as a string by index
@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<llama_model>, ffi.Int32, ffi.Pointer<ffi.Char>, ffi.Size)>()
external int llama_model_meta_val_str_by_index(
  ffi.Pointer<llama_model> model,
  int i,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Get a string describing the model type
@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>, ffi.Size)>()
external int llama_model_desc(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Returns the total size of all the tensors in the model in bytes
@ffi.Native<ffi.Uint64 Function(ffi.Pointer<llama_model>)>()
external int llama_model_size(
  ffi.Pointer<llama_model> model,
);

/// Get the default chat template. Returns nullptr if not available
/// If name is NULL, returns the default chat template
@ffi.Native<
    ffi.Pointer<ffi.Char> Function(
        ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<ffi.Char> llama_model_chat_template(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> name,
);

/// Returns the total number of parameters in the model
@ffi.Native<ffi.Uint64 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_params(
  ffi.Pointer<llama_model> model,
);

/// Returns true if the model contains an encoder that requires llama_encode() call
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_model>)>()
external bool llama_model_has_encoder(
  ffi.Pointer<llama_model> model,
);

/// Returns true if the model contains a decoder that requires llama_decode() call
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_model>)>()
external bool llama_model_has_decoder(
  ffi.Pointer<llama_model> model,
);

/// For encoder-decoder models, this function returns id of the token that must be provided
/// to the decoder to start generating output sequence. For other models, it returns -1.
@ffi.Native<llama_token Function(ffi.Pointer<llama_model>)>()
external int llama_model_decoder_start_token(
  ffi.Pointer<llama_model> model,
);

/// Returns true if the model is recurrent (like Mamba, RWKV, etc.)
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_model>)>()
external bool llama_model_is_recurrent(
  ffi.Pointer<llama_model> model,
);

/// Returns 0 on success
@ffi.Native<
    ffi.Uint32 Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>,
        ffi.Pointer<llama_model_quantize_params>)>()
external int llama_model_quantize(
  ffi.Pointer<ffi.Char> fname_inp,
  ffi.Pointer<ffi.Char> fname_out,
  ffi.Pointer<llama_model_quantize_params> params,
);

/// Load a LoRA adapter from file
@ffi.Native<
    ffi.Pointer<llama_adapter_lora> Function(
        ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<llama_adapter_lora> llama_adapter_lora_init(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> path_lora,
);

/// Manually free a LoRA adapter
/// Note: loaded adapters will be free when the associated model is deleted
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_adapter_lora>)>()
external void llama_adapter_lora_free(
  ffi.Pointer<llama_adapter_lora> adapter,
);

/// Add a loaded LoRA adapter to given context
/// This will not modify model's weight
@ffi.Native<
    ffi.Int32 Function(ffi.Pointer<llama_context>,
        ffi.Pointer<llama_adapter_lora>, ffi.Float)>()
external int llama_set_adapter_lora(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<llama_adapter_lora> adapter,
  double scale,
);

/// Remove a specific LoRA adapter from given context
/// Return -1 if the adapter is not present in the context
@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<llama_context>, ffi.Pointer<llama_adapter_lora>)>()
external int llama_rm_adapter_lora(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<llama_adapter_lora> adapter,
);

/// Remove all LoRA adapters from given context
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_clear_adapter_lora(
  ffi.Pointer<llama_context> ctx,
);

/// Apply a loaded control vector to a llama_context, or if data is NULL, clear
/// the currently loaded vector.
/// n_embd should be the size of a single layer's control, and data should point
/// to an n_embd x n_layers buffer starting from layer 1.
/// il_start and il_end are the layer range the vector should apply to (both inclusive)
/// See llama_control_vector_load in common to load a control vector.
@ffi.Native<
    ffi.Int32 Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Float>,
        ffi.Size, ffi.Int32, ffi.Int32, ffi.Int32)>()
external int llama_apply_adapter_cvec(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Float> data,
  int len,
  int n_embd,
  int il_start,
  int il_end,
);

/// Create an empty KV cache view. (use only for debugging purposes)
@ffi.Native<
    llama_kv_cache_view Function(ffi.Pointer<llama_context>, ffi.Int32)>()
external llama_kv_cache_view llama_kv_cache_view_init(
  ffi.Pointer<llama_context> ctx,
  int n_seq_max,
);

/// Free a KV cache view. (use only for debugging purposes)
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_kv_cache_view>)>()
external void llama_kv_cache_view_free(
  ffi.Pointer<llama_kv_cache_view> view,
);

/// Update the KV cache view structure with the current state of the KV cache. (use only for debugging purposes)
/// TODO: change signature to llama_kv_cache_view_update(struct llama_kv_cache_view * view, const struct llama_context * ctx)
@ffi.Native<
    ffi.Void Function(
        ffi.Pointer<llama_context>, ffi.Pointer<llama_kv_cache_view>)>()
external void llama_kv_cache_view_update(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<llama_kv_cache_view> view,
);

/// Returns the number of tokens in the KV cache (slow, use only for debug)
/// If a KV cell has multiple sequences assigned to it, it will be counted multiple times
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>)>()
external int llama_get_kv_cache_token_count(
  ffi.Pointer<llama_context> ctx,
);

/// Returns the number of used KV cells (i.e. have at least one sequence assigned to them)
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>)>()
external int llama_get_kv_cache_used_cells(
  ffi.Pointer<llama_context> ctx,
);

/// Clear the KV cache - both cell info is erased and KV data is zeroed
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_kv_cache_clear(
  ffi.Pointer<llama_context> ctx,
);

/// Removes all tokens that belong to the specified sequence and have positions in [p0, p1)
/// Returns false if a partial sequence cannot be removed. Removing a whole sequence never fails
/// seq_id < 0 : match any sequence
/// p0 < 0     : [0,  p1]
/// p1 < 0     : [p0, inf)
@ffi.Native<
    ffi.Bool Function(
        ffi.Pointer<llama_context>, llama_seq_id, llama_pos, llama_pos)>()
external bool llama_kv_cache_seq_rm(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
  int p0,
  int p1,
);

/// Copy all tokens that belong to the specified sequence to another sequence
/// Note that this does not allocate extra KV cache memory - it simply assigns the tokens to the new sequence
/// p0 < 0 : [0,  p1]
/// p1 < 0 : [p0, inf)
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id, llama_seq_id,
        llama_pos, llama_pos)>()
external void llama_kv_cache_seq_cp(
  ffi.Pointer<llama_context> ctx,
  int seq_id_src,
  int seq_id_dst,
  int p0,
  int p1,
);

/// Removes all tokens that do not belong to the specified sequence
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id)>()
external void llama_kv_cache_seq_keep(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
);

/// Adds relative position "delta" to all tokens that belong to the specified sequence and have positions in [p0, p1)
/// If the KV cache is RoPEd, the KV data is updated accordingly:
/// - lazily on next llama_decode()
/// - explicitly with llama_kv_cache_update()
/// p0 < 0 : [0,  p1]
/// p1 < 0 : [p0, inf)
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id, llama_pos,
        llama_pos, llama_pos)>()
external void llama_kv_cache_seq_add(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
  int p0,
  int p1,
  int delta,
);

/// Integer division of the positions by factor of `d > 1`
/// If the KV cache is RoPEd, the KV data is updated accordingly:
/// - lazily on next llama_decode()
/// - explicitly with llama_kv_cache_update()
/// p0 < 0 : [0,  p1]
/// p1 < 0 : [p0, inf)
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id, llama_pos,
        llama_pos, ffi.Int)>()
external void llama_kv_cache_seq_div(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
  int p0,
  int p1,
  int d,
);

/// Returns the largest position present in the KV cache for the specified sequence
@ffi.Native<llama_pos Function(ffi.Pointer<llama_context>, llama_seq_id)>()
external int llama_kv_cache_seq_pos_max(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
);

/// Defragment the KV cache
/// This will be applied:
/// - lazily on next llama_decode()
/// - explicitly with llama_kv_cache_update()
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_kv_cache_defrag(
  ffi.Pointer<llama_context> ctx,
);

/// Apply the KV cache updates (such as K-shifts, defragmentation, etc.)
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_kv_cache_update(
  ffi.Pointer<llama_context> ctx,
);

/// Check if the context supports KV cache shifting
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_context>)>()
external bool llama_kv_cache_can_shift(
  ffi.Pointer<llama_context> ctx,
);

/// Returns the *actual* size in bytes of the state
/// (logits, embedding and kv_cache)
/// Only use when saving the state, not when restoring it, otherwise the size may be too small.
@ffi.Native<ffi.Size Function(ffi.Pointer<llama_context>)>()
external int llama_state_get_size(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<llama_context>)>()
external int llama_get_state_size(
  ffi.Pointer<llama_context> ctx,
);

/// Copies the state to the specified destination address.
/// Destination needs to have allocated enough memory.
/// Returns the number of bytes copied
@ffi.Native<
    ffi.Size Function(
        ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>, ffi.Size)>()
external int llama_state_get_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> dst,
  int size,
);

@ffi.Native<
    ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)>()
external int llama_copy_state_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> dst,
);

/// Set the state reading from the specified address
/// Returns the number of bytes read
@ffi.Native<
    ffi.Size Function(
        ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>, ffi.Size)>()
external int llama_state_set_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> src,
  int size,
);

@ffi.Native<
    ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)>()
external int llama_set_state_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> src,
);

/// Save/load session file
@ffi.Native<
    ffi.Bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
        ffi.Pointer<llama_token>, ffi.Size, ffi.Pointer<ffi.Size>)>()
external bool llama_state_load_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> path_session,
  ffi.Pointer<llama_token> tokens_out,
  int n_token_capacity,
  ffi.Pointer<ffi.Size> n_token_count_out,
);

@ffi.Native<
    ffi.Bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
        ffi.Pointer<llama_token>, ffi.Size, ffi.Pointer<ffi.Size>)>()
external bool llama_load_session_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> path_session,
  ffi.Pointer<llama_token> tokens_out,
  int n_token_capacity,
  ffi.Pointer<ffi.Size> n_token_count_out,
);

@ffi.Native<
    ffi.Bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
        ffi.Pointer<llama_token>, ffi.Size)>()
external bool llama_state_save_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> path_session,
  ffi.Pointer<llama_token> tokens,
  int n_token_count,
);

@ffi.Native<
    ffi.Bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
        ffi.Pointer<llama_token>, ffi.Size)>()
external bool llama_save_session_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> path_session,
  ffi.Pointer<llama_token> tokens,
  int n_token_count,
);

/// Get the exact size needed to copy the KV cache of a single sequence
@ffi.Native<ffi.Size Function(ffi.Pointer<llama_context>, llama_seq_id)>()
external int llama_state_seq_get_size(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
);

/// Copy the KV cache of a single sequence into the specified buffer
@ffi.Native<
    ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>,
        ffi.Size, llama_seq_id)>()
external int llama_state_seq_get_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> dst,
  int size,
  int seq_id,
);

/// Copy the sequence data (originally copied with `llama_state_seq_get_data`) into the specified sequence
/// Returns:
/// - Positive: Ok
/// - Zero: Failed to load
@ffi.Native<
    ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>,
        ffi.Size, llama_seq_id)>()
external int llama_state_seq_set_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> src,
  int size,
  int dest_seq_id,
);

@ffi.Native<
    ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
        llama_seq_id, ffi.Pointer<llama_token>, ffi.Size)>()
external int llama_state_seq_save_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> filepath,
  int seq_id,
  ffi.Pointer<llama_token> tokens,
  int n_token_count,
);

@ffi.Native<
    ffi.Size Function(
        ffi.Pointer<llama_context>,
        ffi.Pointer<ffi.Char>,
        llama_seq_id,
        ffi.Pointer<llama_token>,
        ffi.Size,
        ffi.Pointer<ffi.Size>)>()
external int llama_state_seq_load_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> filepath,
  int dest_seq_id,
  ffi.Pointer<llama_token> tokens_out,
  int n_token_capacity,
  ffi.Pointer<ffi.Size> n_token_count_out,
);

/// Return batch for single sequence of tokens
/// The sequence ID will be fixed to 0
/// The position of the tokens will be tracked automatically by llama_decode
///
/// NOTE: this is a helper function to facilitate transition to the new batch API - avoid using it
@ffi.Native<llama_batch Function(ffi.Pointer<llama_token>, ffi.Int32)>()
external llama_batch llama_batch_get_one(
  ffi.Pointer<llama_token> tokens,
  int n_tokens,
);

/// Allocates a batch of tokens on the heap that can hold a maximum of n_tokens
/// Each token can be assigned up to n_seq_max sequence ids
/// The batch has to be freed with llama_batch_free()
/// If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)
/// Otherwise, llama_batch.token will be allocated to store n_tokens llama_token
/// The rest of the llama_batch members are allocated with size n_tokens
/// All members are left uninitialized
@ffi.Native<llama_batch Function(ffi.Int32, ffi.Int32, ffi.Int32)>()
external llama_batch llama_batch_init(
  int n_tokens,
  int embd,
  int n_seq_max,
);

/// Frees a batch of tokens allocated with llama_batch_init()
@ffi.Native<ffi.Void Function(llama_batch)>()
external void llama_batch_free(
  llama_batch batch,
);

/// Processes a batch of tokens with the ecoder part of the encoder-decoder model.
/// Stores the encoder output internally for later use by the decoder cross-attention layers.
/// 0 - success
/// < 0 - error. the KV cache state is restored to the state before this call
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>, llama_batch)>()
external int llama_encode(
  ffi.Pointer<llama_context> ctx,
  llama_batch batch,
);

/// Positive return values does not mean a fatal error, but rather a warning.
/// 0 - success
/// 1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)
/// < 0 - error. the KV cache state is restored to the state before this call
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>, llama_batch)>()
external int llama_decode(
  ffi.Pointer<llama_context> ctx,
  llama_batch batch,
);

/// Set the number of threads used for decoding
/// n_threads is the number of threads used for generation (single token)
/// n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_context>, ffi.Int32, ffi.Int32)>()
external void llama_set_n_threads(
  ffi.Pointer<llama_context> ctx,
  int n_threads,
  int n_threads_batch,
);

/// Get the number of threads used for generation of a single token.
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_threads(
  ffi.Pointer<llama_context> ctx,
);

/// Get the number of threads used for prompt and batch processing (multiple token).
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_threads_batch(
  ffi.Pointer<llama_context> ctx,
);

/// Set whether the model is in embeddings mode or not
/// If true, embeddings will be returned but logits will not
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>, ffi.Bool)>()
external void llama_set_embeddings(
  ffi.Pointer<llama_context> ctx,
  bool embeddings,
);

/// Set whether to use causal attention or not
/// If set to true, the model will only attend to the past tokens
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>, ffi.Bool)>()
external void llama_set_causal_attn(
  ffi.Pointer<llama_context> ctx,
  bool causal_attn,
);

/// Set abort callback
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_context>, ggml_abort_callback,
        ffi.Pointer<ffi.Void>)>()
external void llama_set_abort_callback(
  ffi.Pointer<llama_context> ctx,
  ggml_abort_callback abort_callback,
  ffi.Pointer<ffi.Void> abort_callback_data,
);

/// Wait until all computations are finished
/// This is automatically done when using one of the functions below to obtain the computation results
/// and is not necessary to call it explicitly in most cases
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_synchronize(
  ffi.Pointer<llama_context> ctx,
);

/// Token logits obtained from the last call to llama_decode()
/// The logits for which llama_batch.logits[i] != 0 are stored contiguously
/// in the order they have appeared in the batch.
/// Rows: number of tokens for which llama_batch.logits[i] != 0
/// Cols: n_vocab
@ffi.Native<ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)>()
external ffi.Pointer<ffi.Float> llama_get_logits(
  ffi.Pointer<llama_context> ctx,
);

/// Logits for the ith token. For positive indices, Equivalent to:
/// llama_get_logits(ctx) + ctx->output_ids[i]*n_vocab
/// Negative indicies can be used to access logits in reverse order, -1 is the last logit.
/// returns NULL for invalid ids.
@ffi.Native<
    ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, ffi.Int32)>()
external ffi.Pointer<ffi.Float> llama_get_logits_ith(
  ffi.Pointer<llama_context> ctx,
  int i,
);

/// Get all output token embeddings.
/// when pooling_type == LLAMA_POOLING_TYPE_NONE or when using a generative model,
/// the embeddings for which llama_batch.logits[i] != 0 are stored contiguously
/// in the order they have appeared in the batch.
/// shape: [n_outputs*n_embd]
/// Otherwise, returns NULL.
@ffi.Native<ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)>()
external ffi.Pointer<ffi.Float> llama_get_embeddings(
  ffi.Pointer<llama_context> ctx,
);

/// Get the embeddings for the ith token. For positive indices, Equivalent to:
/// llama_get_embeddings(ctx) + ctx->output_ids[i]*n_embd
/// Negative indicies can be used to access embeddings in reverse order, -1 is the last embedding.
/// shape: [n_embd] (1-dimensional)
/// returns NULL for invalid ids.
@ffi.Native<
    ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, ffi.Int32)>()
external ffi.Pointer<ffi.Float> llama_get_embeddings_ith(
  ffi.Pointer<llama_context> ctx,
  int i,
);

/// Get the embeddings for a sequence id
/// Returns NULL if pooling_type is LLAMA_POOLING_TYPE_NONE
/// when pooling_type == LLAMA_POOLING_TYPE_RANK, returns float[1] with the rank of the sequence
/// otherwise: float[n_embd] (1-dimensional)
@ffi.Native<
    ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, llama_seq_id)>()
external ffi.Pointer<ffi.Float> llama_get_embeddings_seq(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
);

/// Vocab
@ffi.Native<
    ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_vocab>, llama_token)>()
external ffi.Pointer<ffi.Char> llama_vocab_get_text(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.Float Function(ffi.Pointer<llama_vocab>, llama_token)>()
external double llama_vocab_get_score(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<llama_vocab>, llama_token)>(
    symbol: 'llama_vocab_get_attr')
external int _llama_vocab_get_attr(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

llama_token_attr llama_vocab_get_attr(
  ffi.Pointer<llama_vocab> vocab,
  Dartllama_token token,
) =>
    llama_token_attr.fromValue(_llama_vocab_get_attr(
      vocab,
      token,
    ));

/// Check if the token is supposed to end generation (end-of-generation, eg. EOS, EOT, etc.)
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)>()
external bool llama_vocab_is_eog(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

/// Identify if Token Id is a control token or a render-able token
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)>()
external bool llama_vocab_is_control(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

/// Special tokens
@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_bos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_eos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_eot(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_sep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_nl(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_pad(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>()
external bool llama_vocab_get_add_bos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>()
external bool llama_vocab_get_add_eos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_fim_pre(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_fim_suf(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_fim_mid(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_fim_pad(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_fim_rep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_fim_sep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<
    ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_vocab>, llama_token)>()
external ffi.Pointer<ffi.Char> llama_token_get_text(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.Float Function(ffi.Pointer<llama_vocab>, llama_token)>()
external double llama_token_get_score(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<llama_vocab>, llama_token)>(
    symbol: 'llama_token_get_attr')
external int _llama_token_get_attr(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

llama_token_attr llama_token_get_attr(
  ffi.Pointer<llama_vocab> vocab,
  Dartllama_token token,
) =>
    llama_token_attr.fromValue(_llama_token_get_attr(
      vocab,
      token,
    ));

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)>()
external bool llama_token_is_eog(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)>()
external bool llama_token_is_control(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_bos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_eos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_eot(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_cls(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_sep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_nl(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_pad(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>()
external bool llama_add_bos_token(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>()
external bool llama_add_eos_token(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_fim_pre(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_fim_suf(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_fim_mid(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_fim_pad(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_fim_rep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_fim_sep(
  ffi.Pointer<llama_vocab> vocab,
);

/// CLS is equivalent to BOS
@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_cls(
  ffi.Pointer<llama_vocab> vocab,
);

/// @details Convert the provided text into tokens.
/// @param tokens The tokens pointer must be large enough to hold the resulting tokens.
/// @return Returns the number of tokens on success, no more than n_tokens_max
/// @return Returns a negative number on failure - the number of tokens that would have been returned
/// @param add_special Allow to add BOS and EOS tokens if model is configured to do so.
/// @param parse_special Allow tokenizing special and/or control tokens which otherwise are not exposed and treated
/// as plaintext. Does not insert a leading space.
@ffi.Native<
    ffi.Int32 Function(ffi.Pointer<llama_vocab>, ffi.Pointer<ffi.Char>,
        ffi.Int32, ffi.Pointer<llama_token>, ffi.Int32, ffi.Bool, ffi.Bool)>()
external int llama_tokenize(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<ffi.Char> text,
  int text_len,
  ffi.Pointer<llama_token> tokens,
  int n_tokens_max,
  bool add_special,
  bool parse_special,
);

/// Token Id -> Piece.
/// Uses the vocabulary in the provided context.
/// Does not write null terminator to the buffer.
/// User can skip up to 'lstrip' leading spaces before copying (useful when encoding/decoding multiple tokens with 'add_space_prefix')
/// @param special If true, special tokens are rendered in the output.
@ffi.Native<
    ffi.Int32 Function(ffi.Pointer<llama_vocab>, llama_token,
        ffi.Pointer<ffi.Char>, ffi.Int32, ffi.Int32, ffi.Bool)>()
external int llama_token_to_piece(
  ffi.Pointer<llama_vocab> vocab,
  int token,
  ffi.Pointer<ffi.Char> buf,
  int length,
  int lstrip,
  bool special,
);

/// @details Convert the provided tokens into text (inverse of llama_tokenize()).
/// @param text The char pointer must be large enough to hold the resulting text.
/// @return Returns the number of chars/bytes on success, no more than text_len_max.
/// @return Returns a negative number on failure - the number of chars/bytes that would have been returned.
/// @param remove_special Allow to remove BOS and EOS tokens if model is configured to do so.
/// @param unparse_special If true, special tokens are rendered in the output.
@ffi.Native<
    ffi.Int32 Function(ffi.Pointer<llama_vocab>, ffi.Pointer<llama_token>,
        ffi.Int32, ffi.Pointer<ffi.Char>, ffi.Int32, ffi.Bool, ffi.Bool)>()
external int llama_detokenize(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<llama_token> tokens,
  int n_tokens,
  ffi.Pointer<ffi.Char> text,
  int text_len_max,
  bool remove_special,
  bool unparse_special,
);

/// Apply chat template. Inspired by hf apply_chat_template() on python.
/// Both "model" and "custom_template" are optional, but at least one is required. "custom_template" has higher precedence than "model"
/// NOTE: This function does not use a jinja parser. It only support a pre-defined list of template. See more: https://github.com/ggml-org/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template
/// @param tmpl A Jinja template to use for this chat. If this is nullptr, the model’s default chat template will be used instead.
/// @param chat Pointer to a list of multiple llama_chat_message
/// @param n_msg Number of llama_chat_message in this chat
/// @param add_ass Whether to end the prompt with the token(s) that indicate the start of an assistant message.
/// @param buf A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)
/// @param length The size of the allocated buffer
/// @return The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template.
@ffi.Native<
    ffi.Int32 Function(ffi.Pointer<ffi.Char>, ffi.Pointer<llama_chat_message>,
        ffi.Size, ffi.Bool, ffi.Pointer<ffi.Char>, ffi.Int32)>()
external int llama_chat_apply_template(
  ffi.Pointer<ffi.Char> tmpl,
  ffi.Pointer<llama_chat_message> chat,
  int n_msg,
  bool add_ass,
  ffi.Pointer<ffi.Char> buf,
  int length,
);

/// Get list of built-in chat templates
@ffi.Native<ffi.Int32 Function(ffi.Pointer<ffi.Pointer<ffi.Char>>, ffi.Size)>()
external int llama_chat_builtin_templates(
  ffi.Pointer<ffi.Pointer<ffi.Char>> output,
  int len,
);

/// mirror of llama_sampler_i:
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Pointer<llama_sampler_i>, llama_sampler_context_t)>()
external ffi.Pointer<llama_sampler> llama_sampler_init(
  ffi.Pointer<llama_sampler_i> iface,
  llama_sampler_context_t ctx,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_sampler>)>()
external ffi.Pointer<ffi.Char> llama_sampler_name(
  ffi.Pointer<llama_sampler> smpl,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>, llama_token)>()
external void llama_sampler_accept(
  ffi.Pointer<llama_sampler> smpl,
  int token,
);

@ffi.Native<
    ffi.Void Function(
        ffi.Pointer<llama_sampler>, ffi.Pointer<llama_token_data_array>)>()
external void llama_sampler_apply(
  ffi.Pointer<llama_sampler> smpl,
  ffi.Pointer<llama_token_data_array> cur_p,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>)>()
external void llama_sampler_reset(
  ffi.Pointer<llama_sampler> smpl,
);

@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>)>()
external ffi.Pointer<llama_sampler> llama_sampler_clone(
  ffi.Pointer<llama_sampler> smpl,
);

/// important: do not free if the sampler has been added to a llama_sampler_chain (via llama_sampler_chain_add)
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>)>()
external void llama_sampler_free(
  ffi.Pointer<llama_sampler> smpl,
);

/// llama_sampler_chain
/// a type of llama_sampler that can chain multiple samplers one after another
@ffi.Native<ffi.Pointer<llama_sampler> Function(llama_sampler_chain_params)>()
external ffi.Pointer<llama_sampler> llama_sampler_chain_init(
  llama_sampler_chain_params params,
);

/// important: takes ownership of the sampler object and will free it when llama_sampler_free is called
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_sampler>, ffi.Pointer<llama_sampler>)>()
external void llama_sampler_chain_add(
  ffi.Pointer<llama_sampler> chain,
  ffi.Pointer<llama_sampler> smpl,
);

@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Pointer<llama_sampler>, ffi.Int32)>()
external ffi.Pointer<llama_sampler> llama_sampler_chain_get(
  ffi.Pointer<llama_sampler> chain,
  int i,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<llama_sampler>)>()
external int llama_sampler_chain_n(
  ffi.Pointer<llama_sampler> chain,
);

/// after removing a sampler, the chain will no longer own it, and it will not be freed when the chain is freed
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Pointer<llama_sampler>, ffi.Int32)>()
external ffi.Pointer<llama_sampler> llama_sampler_chain_remove(
  ffi.Pointer<llama_sampler> chain,
  int i,
);

/// available samplers:
@ffi.Native<ffi.Pointer<llama_sampler> Function()>()
external ffi.Pointer<llama_sampler> llama_sampler_init_greedy();

@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Uint32)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_dist(
  int seed,
);

/// @details Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.
/// NOTE: Avoid using on the full vocabulary as the sorting can become slow. For example, apply top-k or top-p sampling first.
@ffi.Native<ffi.Pointer<llama_sampler> Function()>()
external ffi.Pointer<llama_sampler> llama_sampler_init_softmax();

/// @details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Int32)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_top_k(
  int k,
);

/// @details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Size)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_top_p(
  double p,
  int min_keep,
);

/// @details Minimum P sampling as described in https://github.com/ggml-org/llama.cpp/pull/3841
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Size)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_min_p(
  double p,
  int min_keep,
);

/// @details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Size)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_typical(
  double p,
  int min_keep,
);

/// #details Updates the logits l_i` = l_i/t. When t <= 0.0f, the maximum logit is kept at it's original value, the rest are set to -inf
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_temp(
  double t,
);

/// @details Dynamic temperature implementation (a.k.a. entropy) described in the paper https://arxiv.org/abs/2309.02772.
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Float, ffi.Float)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_temp_ext(
  double t,
  double delta,
  double exponent,
);

/// @details XTC sampler as described in https://github.com/oobabooga/text-generation-webui/pull/6335
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Float, ffi.Float, ffi.Size, ffi.Uint32)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_xtc(
  double p,
  double t,
  int min_keep,
  int seed,
);

/// @details Top n sigma sampling as described in academic paper "Top-nσ: Not All Logits Are You Need" https://arxiv.org/pdf/2411.07641
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_top_n_sigma(
  double n,
);

/// @details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
/// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
/// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
/// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
/// @param m The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.
/// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Int32, ffi.Uint32, ffi.Float, ffi.Float, ffi.Int32)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_mirostat(
  int n_vocab,
  int seed,
  double tau,
  double eta,
  int m,
);

/// @details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
/// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
/// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
/// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
/// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(ffi.Uint32, ffi.Float, ffi.Float)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_mirostat_v2(
  int seed,
  double tau,
  double eta,
);

@ffi.Native<
    ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_vocab>,
        ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_grammar(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<ffi.Char> grammar_str,
  ffi.Pointer<ffi.Char> grammar_root,
);

@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Pointer<llama_vocab>,
        ffi.Pointer<ffi.Char>,
        ffi.Pointer<ffi.Char>,
        ffi.Pointer<ffi.Pointer<ffi.Char>>,
        ffi.Size,
        ffi.Pointer<llama_token>,
        ffi.Size)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_grammar_lazy(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<ffi.Char> grammar_str,
  ffi.Pointer<ffi.Char> grammar_root,
  ffi.Pointer<ffi.Pointer<ffi.Char>> trigger_words,
  int num_trigger_words,
  ffi.Pointer<llama_token> trigger_tokens,
  int num_trigger_tokens,
);

/// @details Lazy grammar sampler, introduced in https://github.com/ggml-org/llama.cpp/pull/9639
/// @param trigger_patterns A list of patterns that will trigger the grammar sampler. Pattern will be matched from the start of the generation output, and grammar sampler will be fed content starting from its first match group.
/// @param trigger_tokens A list of tokens that will trigger the grammar sampler. Grammar sampler will be fed content starting from the trigger token included.
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Pointer<llama_vocab>,
        ffi.Pointer<ffi.Char>,
        ffi.Pointer<ffi.Char>,
        ffi.Pointer<ffi.Pointer<ffi.Char>>,
        ffi.Size,
        ffi.Pointer<llama_token>,
        ffi.Size)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_grammar_lazy_patterns(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<ffi.Char> grammar_str,
  ffi.Pointer<ffi.Char> grammar_root,
  ffi.Pointer<ffi.Pointer<ffi.Char>> trigger_patterns,
  int num_trigger_patterns,
  ffi.Pointer<llama_token> trigger_tokens,
  int num_trigger_tokens,
);

/// NOTE: Avoid using on the full vocabulary as searching for repeated tokens can become slow. For example, apply top-k or top-p sampling first.
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Int32, ffi.Float, ffi.Float, ffi.Float)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_penalties(
  int penalty_last_n,
  double penalty_repeat,
  double penalty_freq,
  double penalty_present,
);

/// @details DRY sampler, designed by p-e-w, as described in: https://github.com/oobabooga/text-generation-webui/pull/5677, porting Koboldcpp implementation authored by pi6am: https://github.com/LostRuins/koboldcpp/pull/982
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Pointer<llama_vocab>,
        ffi.Int32,
        ffi.Float,
        ffi.Float,
        ffi.Int32,
        ffi.Int32,
        ffi.Pointer<ffi.Pointer<ffi.Char>>,
        ffi.Size)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_dry(
  ffi.Pointer<llama_vocab> vocab,
  int n_ctx_train,
  double dry_multiplier,
  double dry_base,
  int dry_allowed_length,
  int dry_penalty_last_n,
  ffi.Pointer<ffi.Pointer<ffi.Char>> seq_breakers,
  int num_breakers,
);

@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Int32, ffi.Int32, ffi.Pointer<llama_logit_bias>)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_logit_bias(
  int n_vocab,
  int n_logit_bias,
  ffi.Pointer<llama_logit_bias> logit_bias,
);

/// this sampler is meant to be used for fill-in-the-middle infilling
/// it's supposed to be used after top_k + top_p sampling
///
/// 1. if the sum of the EOG probs times the number of candidates is higher than the sum of the other probs -> pick EOG
/// 2. combine probs of tokens that have the same prefix
///
/// example:
///
/// - before:
/// "hel":   0.5
/// "hell":  0.2
/// "hello": 0.1
/// "dummy": 0.1
///
/// - after:
/// "hel":   0.8
/// "dummy": 0.1
///
/// 3. discard non-EOG tokens with low prob
/// 4. if no tokens are left -> pick EOT
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_vocab>)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_infill(
  ffi.Pointer<llama_vocab> vocab,
);

/// Returns the seed used by the sampler if applicable, LLAMA_DEFAULT_SEED otherwise
@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_sampler>)>()
external int llama_sampler_get_seed(
  ffi.Pointer<llama_sampler> smpl,
);

/// @details Sample and accept a token from the idx-th output of the last evaluation
///
/// Shorthand for:
/// const auto * logits = llama_get_logits_ith(ctx, idx);
/// llama_token_data_array cur_p = { ... init from logits ... };
/// llama_sampler_apply(smpl, &cur_p);
/// auto token = cur_p.data[cur_p.selected].id;
/// llama_sampler_accept(smpl, token);
/// return token;
/// Returns the sampled token
@ffi.Native<
    llama_token Function(
        ffi.Pointer<llama_sampler>, ffi.Pointer<llama_context>, ffi.Int32)>()
external int llama_sampler_sample(
  ffi.Pointer<llama_sampler> smpl,
  ffi.Pointer<llama_context> ctx,
  int idx,
);

/// @details Build a split GGUF final path for this chunk.
/// llama_split_path(split_path, sizeof(split_path), "/models/ggml-model-q4_0", 2, 4) => split_path = "/models/ggml-model-q4_0-00002-of-00004.gguf"
/// Returns the split_path length.
@ffi.Native<
    ffi.Int Function(ffi.Pointer<ffi.Char>, ffi.Size, ffi.Pointer<ffi.Char>,
        ffi.Int, ffi.Int)>()
external int llama_split_path(
  ffi.Pointer<ffi.Char> split_path,
  int maxlen,
  ffi.Pointer<ffi.Char> path_prefix,
  int split_no,
  int split_count,
);

/// @details Extract the path prefix from the split_path if and only if the split_no and split_count match.
/// llama_split_prefix(split_prefix, 64, "/models/ggml-model-q4_0-00002-of-00004.gguf", 2, 4) => split_prefix = "/models/ggml-model-q4_0"
/// Returns the split_prefix length.
@ffi.Native<
    ffi.Int Function(ffi.Pointer<ffi.Char>, ffi.Size, ffi.Pointer<ffi.Char>,
        ffi.Int, ffi.Int)>()
external int llama_split_prefix(
  ffi.Pointer<ffi.Char> split_prefix,
  int maxlen,
  ffi.Pointer<ffi.Char> split_path,
  int split_no,
  int split_count,
);

/// Print system information
@ffi.Native<ffi.Pointer<ffi.Char> Function()>()
external ffi.Pointer<ffi.Char> llama_print_system_info();

/// Set callback for all future logging events.
/// If this is not called, or NULL is supplied, everything is output on stderr.
@ffi.Native<ffi.Void Function(ggml_log_callback, ffi.Pointer<ffi.Void>)>()
external void llama_log_set(
  ggml_log_callback log_callback,
  ffi.Pointer<ffi.Void> user_data,
);

@ffi.Native<llama_perf_context_data Function(ffi.Pointer<llama_context>)>()
external llama_perf_context_data llama_perf_context(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_perf_context_print(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_perf_context_reset(
  ffi.Pointer<llama_context> ctx,
);

/// NOTE: the following work only with samplers constructed via llama_sampler_chain_init
@ffi.Native<llama_perf_sampler_data Function(ffi.Pointer<llama_sampler>)>()
external llama_perf_sampler_data llama_perf_sampler(
  ffi.Pointer<llama_sampler> chain,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>)>()
external void llama_perf_sampler_print(
  ffi.Pointer<llama_sampler> chain,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>)>()
external void llama_perf_sampler_reset(
  ffi.Pointer<llama_sampler> chain,
);

@ffi.Native<
    ffi.Int Function(
        ffi.Pointer<ffi.Pointer<lcpp_common_chat_msg_t>>, ffi.Int)>()
external int lcpp_prompt(
  ffi.Pointer<ffi.Pointer<lcpp_common_chat_msg_t>> messages,
  int n_messages,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<lcpp_common_chat_msg_t>)>()
external void lcpp_common_chat_msg_free(
  ffi.Pointer<lcpp_common_chat_msg_t> msg,
);

@ffi.Native<lcpp_params_t Function()>()
external lcpp_params_t lcpp_sampling_params_defaults();

@ffi.Native<
    ffi.Void Function(
        llama_model_params_t, llama_context_params_t, lcpp_params_t)>()
external void lcpp_reconfigure(
  llama_model_params_t model_params,
  llama_context_params_t context_params,
  lcpp_params_t lcpp_params,
);

@ffi.Native<ffi.Void Function(LppTokenStreamCallback)>()
external void lcpp_set_token_stream_callback(
  LppTokenStreamCallback newtoken_callback,
);

@ffi.Native<ffi.Void Function()>()
external void lcpp_unset_token_stream_callback();

@ffi.Native<ffi.Void Function(LppChatMessageCallback)>()
external void lcpp_set_chat_message_callback(
  LppChatMessageCallback chat_msg_callback,
);

@ffi.Native<ffi.Void Function()>()
external void lcpp_unset_chat_message_callback();

@ffi.Native<
    ffi.Int32 Function(ffi.Pointer<ffi.Char>, ffi.Int, ffi.Bool, ffi.Bool,
        ffi.Pointer<ffi.Pointer<llama_token>>)>()
external int lcpp_tokenize(
  ffi.Pointer<ffi.Char> text,
  int n_text,
  bool add_special,
  bool parse_special,
  ffi.Pointer<ffi.Pointer<llama_token>> tokens,
);

@ffi.Native<
    ffi.Void Function(ffi.Pointer<ffi.Int>, ffi.Int, ffi.Bool,
        ffi.Pointer<lcpp_data_pvalue_t>)>()
external void lcpp_detokenize(
  ffi.Pointer<ffi.Int> tokens,
  int n_tokens,
  bool special,
  ffi.Pointer<lcpp_data_pvalue_t> text,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<lcpp_data_pvalue_t>)>()
external void lcpp_model_description(
  ffi.Pointer<lcpp_data_pvalue_t> pvalue,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<lcpp_data_pvalue_t>)>()
external void lcpp_model_architecture(
  ffi.Pointer<lcpp_data_pvalue_t> pvalue,
);

@ffi.Native<ffi.Void Function(ffi.Bool)>()
external void lcpp_send_abort_signal(
  bool abort,
);

@ffi.Native<ffi.Void Function()>()
external void lcpp_reset();

@ffi.Native<ffi.Void Function()>()
external void lcpp_clear_token_stream_responses();

@ffi.Native<ffi.Void Function()>()
external void lcpp_clear_chat_msg_strings();

@ffi.Native<ffi.Void Function()>()
external void lcpp_destroy();

typedef va_list = ffi.Pointer<ffi.Char>;
typedef ptrdiff_t = ffi.LongLong;
typedef Dartptrdiff_t = int;
typedef int_least8_t = ffi.SignedChar;
typedef Dartint_least8_t = int;
typedef int_least16_t = ffi.Short;
typedef Dartint_least16_t = int;
typedef int_least32_t = ffi.Int;
typedef Dartint_least32_t = int;
typedef int_least64_t = ffi.LongLong;
typedef Dartint_least64_t = int;
typedef uint_least8_t = ffi.UnsignedChar;
typedef Dartuint_least8_t = int;
typedef uint_least16_t = ffi.UnsignedShort;
typedef Dartuint_least16_t = int;
typedef uint_least32_t = ffi.UnsignedInt;
typedef Dartuint_least32_t = int;
typedef uint_least64_t = ffi.UnsignedLongLong;
typedef Dartuint_least64_t = int;
typedef int_fast8_t = ffi.SignedChar;
typedef Dartint_fast8_t = int;
typedef int_fast16_t = ffi.Int;
typedef Dartint_fast16_t = int;
typedef int_fast32_t = ffi.Int;
typedef Dartint_fast32_t = int;
typedef int_fast64_t = ffi.LongLong;
typedef Dartint_fast64_t = int;
typedef uint_fast8_t = ffi.UnsignedChar;
typedef Dartuint_fast8_t = int;
typedef uint_fast16_t = ffi.UnsignedInt;
typedef Dartuint_fast16_t = int;
typedef uint_fast32_t = ffi.UnsignedInt;
typedef Dartuint_fast32_t = int;
typedef uint_fast64_t = ffi.UnsignedLongLong;
typedef Dartuint_fast64_t = int;
typedef intmax_t = ffi.LongLong;
typedef Dartintmax_t = int;
typedef uintmax_t = ffi.UnsignedLongLong;
typedef Dartuintmax_t = int;
typedef errno_t = ffi.Int;
typedef Darterrno_t = int;
typedef wint_t = ffi.UnsignedShort;
typedef Dartwint_t = int;
typedef wctype_t = ffi.UnsignedShort;
typedef Dartwctype_t = int;
typedef __time32_t = ffi.Long;
typedef Dart__time32_t = int;
typedef __time64_t = ffi.LongLong;
typedef Dart__time64_t = int;

final class __crt_locale_data_public extends ffi.Struct {
  external ffi.Pointer<ffi.UnsignedShort> _locale_pctype;

  @ffi.Int()
  external int _locale_mb_cur_max;

  @ffi.UnsignedInt()
  external int _locale_lc_codepage;
}

final class __crt_locale_data extends ffi.Opaque {}

final class __crt_multibyte_data extends ffi.Opaque {}

final class __crt_locale_pointers extends ffi.Struct {
  external ffi.Pointer<__crt_locale_data> locinfo;

  external ffi.Pointer<__crt_multibyte_data> mbcinfo;
}

typedef _locale_t = ffi.Pointer<__crt_locale_pointers>;

final class _Mbstatet extends ffi.Struct {
  @ffi.UnsignedLong()
  external int _Wchar;

  @ffi.UnsignedShort()
  external int _Byte;

  @ffi.UnsignedShort()
  external int _State;
}

typedef mbstate_t = _Mbstatet;
typedef time_t = __time64_t;
typedef rsize_t = ffi.Size;
typedef Dartrsize_t = int;

final class _iobuf extends ffi.Struct {
  external ffi.Pointer<ffi.Void> _Placeholder;
}

typedef FILE = _iobuf;
typedef fpos_t = ffi.LongLong;
typedef Dartfpos_t = int;

enum ggml_status {
  GGML_STATUS_ALLOC_FAILED(-2),
  GGML_STATUS_FAILED(-1),
  GGML_STATUS_SUCCESS(0),
  GGML_STATUS_ABORTED(1);

  final int value;
  const ggml_status(this.value);

  static ggml_status fromValue(int value) => switch (value) {
        -2 => GGML_STATUS_ALLOC_FAILED,
        -1 => GGML_STATUS_FAILED,
        0 => GGML_STATUS_SUCCESS,
        1 => GGML_STATUS_ABORTED,
        _ => throw ArgumentError('Unknown value for ggml_status: $value'),
      };
}

/// ieee 754-2008 half-precision float16
/// todo: make this not an integral type
typedef ggml_fp16_t = ffi.Uint16;
typedef Dartggml_fp16_t = int;

/// google brain half-precision bfloat16
final class ggml_bf16_t extends ffi.Struct {
  @ffi.Uint16()
  external int bits;
}

final class ggml_object extends ffi.Opaque {}

final class ggml_context extends ffi.Opaque {}

final class ggml_cgraph extends ffi.Opaque {}

/// NOTE: always add types at the end of the enum to keep backward compatibility
enum ggml_type {
  GGML_TYPE_F32(0),
  GGML_TYPE_F16(1),
  GGML_TYPE_Q4_0(2),
  GGML_TYPE_Q4_1(3),

  /// GGML_TYPE_Q4_2 = 4, support has been removed
  /// GGML_TYPE_Q4_3 = 5, support has been removed
  GGML_TYPE_Q5_0(6),
  GGML_TYPE_Q5_1(7),
  GGML_TYPE_Q8_0(8),
  GGML_TYPE_Q8_1(9),
  GGML_TYPE_Q2_K(10),
  GGML_TYPE_Q3_K(11),
  GGML_TYPE_Q4_K(12),
  GGML_TYPE_Q5_K(13),
  GGML_TYPE_Q6_K(14),
  GGML_TYPE_Q8_K(15),
  GGML_TYPE_IQ2_XXS(16),
  GGML_TYPE_IQ2_XS(17),
  GGML_TYPE_IQ3_XXS(18),
  GGML_TYPE_IQ1_S(19),
  GGML_TYPE_IQ4_NL(20),
  GGML_TYPE_IQ3_S(21),
  GGML_TYPE_IQ2_S(22),
  GGML_TYPE_IQ4_XS(23),
  GGML_TYPE_I8(24),
  GGML_TYPE_I16(25),
  GGML_TYPE_I32(26),
  GGML_TYPE_I64(27),
  GGML_TYPE_F64(28),
  GGML_TYPE_IQ1_M(29),
  GGML_TYPE_BF16(30),

  /// GGML_TYPE_Q4_0_4_4 = 31, support has been removed from gguf files
  /// GGML_TYPE_Q4_0_4_8 = 32,
  /// GGML_TYPE_Q4_0_8_8 = 33,
  GGML_TYPE_TQ1_0(34),
  GGML_TYPE_TQ2_0(35),

  /// GGML_TYPE_IQ4_NL_4_4 = 36,
  /// GGML_TYPE_IQ4_NL_4_8 = 37,
  /// GGML_TYPE_IQ4_NL_8_8 = 38,
  GGML_TYPE_COUNT(39);

  final int value;
  const ggml_type(this.value);

  static ggml_type fromValue(int value) => switch (value) {
        0 => GGML_TYPE_F32,
        1 => GGML_TYPE_F16,
        2 => GGML_TYPE_Q4_0,
        3 => GGML_TYPE_Q4_1,
        6 => GGML_TYPE_Q5_0,
        7 => GGML_TYPE_Q5_1,
        8 => GGML_TYPE_Q8_0,
        9 => GGML_TYPE_Q8_1,
        10 => GGML_TYPE_Q2_K,
        11 => GGML_TYPE_Q3_K,
        12 => GGML_TYPE_Q4_K,
        13 => GGML_TYPE_Q5_K,
        14 => GGML_TYPE_Q6_K,
        15 => GGML_TYPE_Q8_K,
        16 => GGML_TYPE_IQ2_XXS,
        17 => GGML_TYPE_IQ2_XS,
        18 => GGML_TYPE_IQ3_XXS,
        19 => GGML_TYPE_IQ1_S,
        20 => GGML_TYPE_IQ4_NL,
        21 => GGML_TYPE_IQ3_S,
        22 => GGML_TYPE_IQ2_S,
        23 => GGML_TYPE_IQ4_XS,
        24 => GGML_TYPE_I8,
        25 => GGML_TYPE_I16,
        26 => GGML_TYPE_I32,
        27 => GGML_TYPE_I64,
        28 => GGML_TYPE_F64,
        29 => GGML_TYPE_IQ1_M,
        30 => GGML_TYPE_BF16,
        34 => GGML_TYPE_TQ1_0,
        35 => GGML_TYPE_TQ2_0,
        39 => GGML_TYPE_COUNT,
        _ => throw ArgumentError('Unknown value for ggml_type: $value'),
      };
}

/// precision
enum ggml_prec {
  GGML_PREC_DEFAULT(0),
  GGML_PREC_F32(1);

  final int value;
  const ggml_prec(this.value);

  static ggml_prec fromValue(int value) => switch (value) {
        0 => GGML_PREC_DEFAULT,
        1 => GGML_PREC_F32,
        _ => throw ArgumentError('Unknown value for ggml_prec: $value'),
      };
}

/// model file types
enum ggml_ftype {
  GGML_FTYPE_UNKNOWN(-1),
  GGML_FTYPE_ALL_F32(0),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_F16(1),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q4_0(2),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q4_1(3),

  /// tok_embeddings.weight and output.weight are F16
  GGML_FTYPE_MOSTLY_Q4_1_SOME_F16(4),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q8_0(7),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q5_0(8),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q5_1(9),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q2_K(10),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q3_K(11),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q4_K(12),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q5_K(13),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q6_K(14),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ2_XXS(15),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ2_XS(16),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ3_XXS(17),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ1_S(18),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ4_NL(19),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ3_S(20),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ2_S(21),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ4_XS(22),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ1_M(23),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_BF16(24);

  final int value;
  const ggml_ftype(this.value);

  static ggml_ftype fromValue(int value) => switch (value) {
        -1 => GGML_FTYPE_UNKNOWN,
        0 => GGML_FTYPE_ALL_F32,
        1 => GGML_FTYPE_MOSTLY_F16,
        2 => GGML_FTYPE_MOSTLY_Q4_0,
        3 => GGML_FTYPE_MOSTLY_Q4_1,
        4 => GGML_FTYPE_MOSTLY_Q4_1_SOME_F16,
        7 => GGML_FTYPE_MOSTLY_Q8_0,
        8 => GGML_FTYPE_MOSTLY_Q5_0,
        9 => GGML_FTYPE_MOSTLY_Q5_1,
        10 => GGML_FTYPE_MOSTLY_Q2_K,
        11 => GGML_FTYPE_MOSTLY_Q3_K,
        12 => GGML_FTYPE_MOSTLY_Q4_K,
        13 => GGML_FTYPE_MOSTLY_Q5_K,
        14 => GGML_FTYPE_MOSTLY_Q6_K,
        15 => GGML_FTYPE_MOSTLY_IQ2_XXS,
        16 => GGML_FTYPE_MOSTLY_IQ2_XS,
        17 => GGML_FTYPE_MOSTLY_IQ3_XXS,
        18 => GGML_FTYPE_MOSTLY_IQ1_S,
        19 => GGML_FTYPE_MOSTLY_IQ4_NL,
        20 => GGML_FTYPE_MOSTLY_IQ3_S,
        21 => GGML_FTYPE_MOSTLY_IQ2_S,
        22 => GGML_FTYPE_MOSTLY_IQ4_XS,
        23 => GGML_FTYPE_MOSTLY_IQ1_M,
        24 => GGML_FTYPE_MOSTLY_BF16,
        _ => throw ArgumentError('Unknown value for ggml_ftype: $value'),
      };
}

/// available tensor operations:
enum ggml_op {
  GGML_OP_NONE(0),
  GGML_OP_DUP(1),
  GGML_OP_ADD(2),
  GGML_OP_ADD1(3),
  GGML_OP_ACC(4),
  GGML_OP_SUB(5),
  GGML_OP_MUL(6),
  GGML_OP_DIV(7),
  GGML_OP_SQR(8),
  GGML_OP_SQRT(9),
  GGML_OP_LOG(10),
  GGML_OP_SIN(11),
  GGML_OP_COS(12),
  GGML_OP_SUM(13),
  GGML_OP_SUM_ROWS(14),
  GGML_OP_MEAN(15),
  GGML_OP_ARGMAX(16),
  GGML_OP_COUNT_EQUAL(17),
  GGML_OP_REPEAT(18),
  GGML_OP_REPEAT_BACK(19),
  GGML_OP_CONCAT(20),
  GGML_OP_SILU_BACK(21),

  /// normalize
  GGML_OP_NORM(22),
  GGML_OP_RMS_NORM(23),
  GGML_OP_RMS_NORM_BACK(24),
  GGML_OP_GROUP_NORM(25),
  GGML_OP_MUL_MAT(26),
  GGML_OP_MUL_MAT_ID(27),
  GGML_OP_OUT_PROD(28),
  GGML_OP_SCALE(29),
  GGML_OP_SET(30),
  GGML_OP_CPY(31),
  GGML_OP_CONT(32),
  GGML_OP_RESHAPE(33),
  GGML_OP_VIEW(34),
  GGML_OP_PERMUTE(35),
  GGML_OP_TRANSPOSE(36),
  GGML_OP_GET_ROWS(37),
  GGML_OP_GET_ROWS_BACK(38),
  GGML_OP_DIAG(39),
  GGML_OP_DIAG_MASK_INF(40),
  GGML_OP_DIAG_MASK_ZERO(41),
  GGML_OP_SOFT_MAX(42),
  GGML_OP_SOFT_MAX_BACK(43),
  GGML_OP_ROPE(44),
  GGML_OP_ROPE_BACK(45),
  GGML_OP_CLAMP(46),
  GGML_OP_CONV_TRANSPOSE_1D(47),
  GGML_OP_IM2COL(48),
  GGML_OP_IM2COL_BACK(49),
  GGML_OP_CONV_TRANSPOSE_2D(50),
  GGML_OP_POOL_1D(51),
  GGML_OP_POOL_2D(52),
  GGML_OP_POOL_2D_BACK(53),

  /// nearest interpolate
  GGML_OP_UPSCALE(54),
  GGML_OP_PAD(55),
  GGML_OP_PAD_REFLECT_1D(56),
  GGML_OP_ARANGE(57),
  GGML_OP_TIMESTEP_EMBEDDING(58),
  GGML_OP_ARGSORT(59),
  GGML_OP_LEAKY_RELU(60),
  GGML_OP_FLASH_ATTN_EXT(61),
  GGML_OP_FLASH_ATTN_BACK(62),
  GGML_OP_SSM_CONV(63),
  GGML_OP_SSM_SCAN(64),
  GGML_OP_WIN_PART(65),
  GGML_OP_WIN_UNPART(66),
  GGML_OP_GET_REL_POS(67),
  GGML_OP_ADD_REL_POS(68),
  GGML_OP_RWKV_WKV6(69),
  GGML_OP_GATED_LINEAR_ATTN(70),
  GGML_OP_UNARY(71),
  GGML_OP_MAP_UNARY(72),
  GGML_OP_MAP_BINARY(73),
  GGML_OP_MAP_CUSTOM1_F32(74),
  GGML_OP_MAP_CUSTOM2_F32(75),
  GGML_OP_MAP_CUSTOM3_F32(76),
  GGML_OP_MAP_CUSTOM1(77),
  GGML_OP_MAP_CUSTOM2(78),
  GGML_OP_MAP_CUSTOM3(79),
  GGML_OP_CROSS_ENTROPY_LOSS(80),
  GGML_OP_CROSS_ENTROPY_LOSS_BACK(81),
  GGML_OP_OPT_STEP_ADAMW(82),
  GGML_OP_COUNT(83);

  final int value;
  const ggml_op(this.value);

  static ggml_op fromValue(int value) => switch (value) {
        0 => GGML_OP_NONE,
        1 => GGML_OP_DUP,
        2 => GGML_OP_ADD,
        3 => GGML_OP_ADD1,
        4 => GGML_OP_ACC,
        5 => GGML_OP_SUB,
        6 => GGML_OP_MUL,
        7 => GGML_OP_DIV,
        8 => GGML_OP_SQR,
        9 => GGML_OP_SQRT,
        10 => GGML_OP_LOG,
        11 => GGML_OP_SIN,
        12 => GGML_OP_COS,
        13 => GGML_OP_SUM,
        14 => GGML_OP_SUM_ROWS,
        15 => GGML_OP_MEAN,
        16 => GGML_OP_ARGMAX,
        17 => GGML_OP_COUNT_EQUAL,
        18 => GGML_OP_REPEAT,
        19 => GGML_OP_REPEAT_BACK,
        20 => GGML_OP_CONCAT,
        21 => GGML_OP_SILU_BACK,
        22 => GGML_OP_NORM,
        23 => GGML_OP_RMS_NORM,
        24 => GGML_OP_RMS_NORM_BACK,
        25 => GGML_OP_GROUP_NORM,
        26 => GGML_OP_MUL_MAT,
        27 => GGML_OP_MUL_MAT_ID,
        28 => GGML_OP_OUT_PROD,
        29 => GGML_OP_SCALE,
        30 => GGML_OP_SET,
        31 => GGML_OP_CPY,
        32 => GGML_OP_CONT,
        33 => GGML_OP_RESHAPE,
        34 => GGML_OP_VIEW,
        35 => GGML_OP_PERMUTE,
        36 => GGML_OP_TRANSPOSE,
        37 => GGML_OP_GET_ROWS,
        38 => GGML_OP_GET_ROWS_BACK,
        39 => GGML_OP_DIAG,
        40 => GGML_OP_DIAG_MASK_INF,
        41 => GGML_OP_DIAG_MASK_ZERO,
        42 => GGML_OP_SOFT_MAX,
        43 => GGML_OP_SOFT_MAX_BACK,
        44 => GGML_OP_ROPE,
        45 => GGML_OP_ROPE_BACK,
        46 => GGML_OP_CLAMP,
        47 => GGML_OP_CONV_TRANSPOSE_1D,
        48 => GGML_OP_IM2COL,
        49 => GGML_OP_IM2COL_BACK,
        50 => GGML_OP_CONV_TRANSPOSE_2D,
        51 => GGML_OP_POOL_1D,
        52 => GGML_OP_POOL_2D,
        53 => GGML_OP_POOL_2D_BACK,
        54 => GGML_OP_UPSCALE,
        55 => GGML_OP_PAD,
        56 => GGML_OP_PAD_REFLECT_1D,
        57 => GGML_OP_ARANGE,
        58 => GGML_OP_TIMESTEP_EMBEDDING,
        59 => GGML_OP_ARGSORT,
        60 => GGML_OP_LEAKY_RELU,
        61 => GGML_OP_FLASH_ATTN_EXT,
        62 => GGML_OP_FLASH_ATTN_BACK,
        63 => GGML_OP_SSM_CONV,
        64 => GGML_OP_SSM_SCAN,
        65 => GGML_OP_WIN_PART,
        66 => GGML_OP_WIN_UNPART,
        67 => GGML_OP_GET_REL_POS,
        68 => GGML_OP_ADD_REL_POS,
        69 => GGML_OP_RWKV_WKV6,
        70 => GGML_OP_GATED_LINEAR_ATTN,
        71 => GGML_OP_UNARY,
        72 => GGML_OP_MAP_UNARY,
        73 => GGML_OP_MAP_BINARY,
        74 => GGML_OP_MAP_CUSTOM1_F32,
        75 => GGML_OP_MAP_CUSTOM2_F32,
        76 => GGML_OP_MAP_CUSTOM3_F32,
        77 => GGML_OP_MAP_CUSTOM1,
        78 => GGML_OP_MAP_CUSTOM2,
        79 => GGML_OP_MAP_CUSTOM3,
        80 => GGML_OP_CROSS_ENTROPY_LOSS,
        81 => GGML_OP_CROSS_ENTROPY_LOSS_BACK,
        82 => GGML_OP_OPT_STEP_ADAMW,
        83 => GGML_OP_COUNT,
        _ => throw ArgumentError('Unknown value for ggml_op: $value'),
      };
}

enum ggml_unary_op {
  GGML_UNARY_OP_ABS(0),
  GGML_UNARY_OP_SGN(1),
  GGML_UNARY_OP_NEG(2),
  GGML_UNARY_OP_STEP(3),
  GGML_UNARY_OP_TANH(4),
  GGML_UNARY_OP_ELU(5),
  GGML_UNARY_OP_RELU(6),
  GGML_UNARY_OP_SIGMOID(7),
  GGML_UNARY_OP_GELU(8),
  GGML_UNARY_OP_GELU_QUICK(9),
  GGML_UNARY_OP_SILU(10),
  GGML_UNARY_OP_HARDSWISH(11),
  GGML_UNARY_OP_HARDSIGMOID(12),
  GGML_UNARY_OP_EXP(13),
  GGML_UNARY_OP_COUNT(14);

  final int value;
  const ggml_unary_op(this.value);

  static ggml_unary_op fromValue(int value) => switch (value) {
        0 => GGML_UNARY_OP_ABS,
        1 => GGML_UNARY_OP_SGN,
        2 => GGML_UNARY_OP_NEG,
        3 => GGML_UNARY_OP_STEP,
        4 => GGML_UNARY_OP_TANH,
        5 => GGML_UNARY_OP_ELU,
        6 => GGML_UNARY_OP_RELU,
        7 => GGML_UNARY_OP_SIGMOID,
        8 => GGML_UNARY_OP_GELU,
        9 => GGML_UNARY_OP_GELU_QUICK,
        10 => GGML_UNARY_OP_SILU,
        11 => GGML_UNARY_OP_HARDSWISH,
        12 => GGML_UNARY_OP_HARDSIGMOID,
        13 => GGML_UNARY_OP_EXP,
        14 => GGML_UNARY_OP_COUNT,
        _ => throw ArgumentError('Unknown value for ggml_unary_op: $value'),
      };
}

enum ggml_object_type {
  GGML_OBJECT_TYPE_TENSOR(0),
  GGML_OBJECT_TYPE_GRAPH(1),
  GGML_OBJECT_TYPE_WORK_BUFFER(2);

  final int value;
  const ggml_object_type(this.value);

  static ggml_object_type fromValue(int value) => switch (value) {
        0 => GGML_OBJECT_TYPE_TENSOR,
        1 => GGML_OBJECT_TYPE_GRAPH,
        2 => GGML_OBJECT_TYPE_WORK_BUFFER,
        _ => throw ArgumentError('Unknown value for ggml_object_type: $value'),
      };
}

enum ggml_log_level {
  GGML_LOG_LEVEL_NONE(0),
  GGML_LOG_LEVEL_DEBUG(1),
  GGML_LOG_LEVEL_INFO(2),
  GGML_LOG_LEVEL_WARN(3),
  GGML_LOG_LEVEL_ERROR(4),

  /// continue previous log
  GGML_LOG_LEVEL_CONT(5);

  final int value;
  const ggml_log_level(this.value);

  static ggml_log_level fromValue(int value) => switch (value) {
        0 => GGML_LOG_LEVEL_NONE,
        1 => GGML_LOG_LEVEL_DEBUG,
        2 => GGML_LOG_LEVEL_INFO,
        3 => GGML_LOG_LEVEL_WARN,
        4 => GGML_LOG_LEVEL_ERROR,
        5 => GGML_LOG_LEVEL_CONT,
        _ => throw ArgumentError('Unknown value for ggml_log_level: $value'),
      };
}

/// this tensor...
enum ggml_tensor_flag {
  /// ...is an input for the GGML compute graph
  GGML_TENSOR_FLAG_INPUT(1),

  /// ...is an output for the GGML compute graph
  GGML_TENSOR_FLAG_OUTPUT(2),

  /// ...contains trainable parameters
  GGML_TENSOR_FLAG_PARAM(4),

  /// ...defines loss for numerical optimization (multiple loss tensors add up)
  GGML_TENSOR_FLAG_LOSS(8);

  final int value;
  const ggml_tensor_flag(this.value);

  static ggml_tensor_flag fromValue(int value) => switch (value) {
        1 => GGML_TENSOR_FLAG_INPUT,
        2 => GGML_TENSOR_FLAG_OUTPUT,
        4 => GGML_TENSOR_FLAG_PARAM,
        8 => GGML_TENSOR_FLAG_LOSS,
        _ => throw ArgumentError('Unknown value for ggml_tensor_flag: $value'),
      };
}

final class ggml_init_params extends ffi.Struct {
  /// bytes
  @ffi.Size()
  external int mem_size;

  /// if NULL, memory will be allocated internally
  external ffi.Pointer<ffi.Void> mem_buffer;

  /// don't allocate memory for the tensor data
  @ffi.Bool()
  external bool no_alloc;
}

final class ggml_backend_buffer extends ffi.Opaque {}

/// n-dimensional tensor
final class ggml_tensor extends ffi.Struct {
  @ffi.UnsignedInt()
  external int typeAsInt;

  ggml_type get type => ggml_type.fromValue(typeAsInt);

  external ffi.Pointer<ggml_backend_buffer> buffer;

  /// number of elements
  @ffi.Array.multi([4])
  external ffi.Array<ffi.Int64> ne;

  /// stride in bytes:
  /// nb[0] = ggml_type_size(type)
  /// nb[1] = nb[0]   * (ne[0] / ggml_blck_size(type)) + padding
  /// nb[i] = nb[i-1] * ne[i-1]
  @ffi.Array.multi([4])
  external ffi.Array<ffi.Size> nb;

  /// compute data
  @ffi.UnsignedInt()
  external int opAsInt;

  ggml_op get op => ggml_op.fromValue(opAsInt);

  /// op params - allocated as int32_t for alignment
  @ffi.Array.multi([16])
  external ffi.Array<ffi.Int32> op_params;

  @ffi.Int32()
  external int flags;

  @ffi.Array.multi([10])
  external ffi.Array<ffi.Pointer<ggml_tensor>> src;

  /// source tensor and offset for views
  external ffi.Pointer<ggml_tensor> view_src;

  @ffi.Size()
  external int view_offs;

  external ffi.Pointer<ffi.Void> data;

  @ffi.Array.multi([64])
  external ffi.Array<ffi.Char> name;

  /// extra things e.g. for ggml-cuda.cu
  external ffi.Pointer<ffi.Void> extra;

  @ffi.Array.multi([8])
  external ffi.Array<ffi.Char> padding;
}

typedef ggml_abort_callbackFunction = ffi.Bool Function(
    ffi.Pointer<ffi.Void> data);
typedef Dartggml_abort_callbackFunction = bool Function(
    ffi.Pointer<ffi.Void> data);

/// Abort callback
/// If not NULL, called before ggml computation
/// If it returns true, the computation is aborted
typedef ggml_abort_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_abort_callbackFunction>>;
typedef ggml_guid_t = ffi.Pointer<ffi.Pointer<ffi.Uint8>>;

enum ggml_op_pool {
  GGML_OP_POOL_MAX(0),
  GGML_OP_POOL_AVG(1),
  GGML_OP_POOL_COUNT(2);

  final int value;
  const ggml_op_pool(this.value);

  static ggml_op_pool fromValue(int value) => switch (value) {
        0 => GGML_OP_POOL_MAX,
        1 => GGML_OP_POOL_AVG,
        2 => GGML_OP_POOL_COUNT,
        _ => throw ArgumentError('Unknown value for ggml_op_pool: $value'),
      };
}

/// sort rows
enum ggml_sort_order {
  GGML_SORT_ORDER_ASC(0),
  GGML_SORT_ORDER_DESC(1);

  final int value;
  const ggml_sort_order(this.value);

  static ggml_sort_order fromValue(int value) => switch (value) {
        0 => GGML_SORT_ORDER_ASC,
        1 => GGML_SORT_ORDER_DESC,
        _ => throw ArgumentError('Unknown value for ggml_sort_order: $value'),
      };
}

typedef ggml_unary_op_f32_tFunction = ffi.Void Function(
    ffi.Int, ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>);
typedef Dartggml_unary_op_f32_tFunction = void Function(
    int, ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>);

/// custom operators
typedef ggml_unary_op_f32_t
    = ffi.Pointer<ffi.NativeFunction<ggml_unary_op_f32_tFunction>>;
typedef ggml_binary_op_f32_tFunction = ffi.Void Function(ffi.Int,
    ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>);
typedef Dartggml_binary_op_f32_tFunction = void Function(int,
    ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>);
typedef ggml_binary_op_f32_t
    = ffi.Pointer<ffi.NativeFunction<ggml_binary_op_f32_tFunction>>;
typedef ggml_custom1_op_f32_tFunction = ffi.Void Function(
    ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>);
typedef Dartggml_custom1_op_f32_tFunction = void Function(
    ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>);
typedef ggml_custom1_op_f32_t
    = ffi.Pointer<ffi.NativeFunction<ggml_custom1_op_f32_tFunction>>;
typedef ggml_custom2_op_f32_tFunction = ffi.Void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>);
typedef Dartggml_custom2_op_f32_tFunction = void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>);
typedef ggml_custom2_op_f32_t
    = ffi.Pointer<ffi.NativeFunction<ggml_custom2_op_f32_tFunction>>;
typedef ggml_custom3_op_f32_tFunction = ffi.Void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>);
typedef Dartggml_custom3_op_f32_tFunction = void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>);
typedef ggml_custom3_op_f32_t
    = ffi.Pointer<ffi.NativeFunction<ggml_custom3_op_f32_tFunction>>;
typedef ggml_custom1_op_tFunction = ffi.Void Function(
    ffi.Pointer<ggml_tensor> dst,
    ffi.Pointer<ggml_tensor> a,
    ffi.Int ith,
    ffi.Int nth,
    ffi.Pointer<ffi.Void> userdata);
typedef Dartggml_custom1_op_tFunction = void Function(
    ffi.Pointer<ggml_tensor> dst,
    ffi.Pointer<ggml_tensor> a,
    int ith,
    int nth,
    ffi.Pointer<ffi.Void> userdata);

/// custom operators v2
typedef ggml_custom1_op_t
    = ffi.Pointer<ffi.NativeFunction<ggml_custom1_op_tFunction>>;
typedef ggml_custom2_op_tFunction = ffi.Void Function(
    ffi.Pointer<ggml_tensor> dst,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Int ith,
    ffi.Int nth,
    ffi.Pointer<ffi.Void> userdata);
typedef Dartggml_custom2_op_tFunction = void Function(
    ffi.Pointer<ggml_tensor> dst,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int ith,
    int nth,
    ffi.Pointer<ffi.Void> userdata);
typedef ggml_custom2_op_t
    = ffi.Pointer<ffi.NativeFunction<ggml_custom2_op_tFunction>>;
typedef ggml_custom3_op_tFunction = ffi.Void Function(
    ffi.Pointer<ggml_tensor> dst,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    ffi.Int ith,
    ffi.Int nth,
    ffi.Pointer<ffi.Void> userdata);
typedef Dartggml_custom3_op_tFunction = void Function(
    ffi.Pointer<ggml_tensor> dst,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    int ith,
    int nth,
    ffi.Pointer<ffi.Void> userdata);
typedef ggml_custom3_op_t
    = ffi.Pointer<ffi.NativeFunction<ggml_custom3_op_tFunction>>;
typedef ggml_log_callbackFunction = ffi.Void Function(ffi.UnsignedInt level,
    ffi.Pointer<ffi.Char> text, ffi.Pointer<ffi.Void> user_data);
typedef Dartggml_log_callbackFunction = void Function(ggml_log_level level,
    ffi.Pointer<ffi.Char> text, ffi.Pointer<ffi.Void> user_data);

/// TODO these functions were sandwiched in the old optimization interface, is there a better place for them?
typedef ggml_log_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_log_callbackFunction>>;
typedef ggml_to_float_tFunction = ffi.Void Function(
    ffi.Pointer<ffi.Void> x, ffi.Pointer<ffi.Float> y, ffi.Int64 k);
typedef Dartggml_to_float_tFunction = void Function(
    ffi.Pointer<ffi.Void> x, ffi.Pointer<ffi.Float> y, int k);
typedef ggml_to_float_t
    = ffi.Pointer<ffi.NativeFunction<ggml_to_float_tFunction>>;
typedef ggml_from_float_tFunction = ffi.Void Function(
    ffi.Pointer<ffi.Float> x, ffi.Pointer<ffi.Void> y, ffi.Int64 k);
typedef Dartggml_from_float_tFunction = void Function(
    ffi.Pointer<ffi.Float> x, ffi.Pointer<ffi.Void> y, int k);
typedef ggml_from_float_t
    = ffi.Pointer<ffi.NativeFunction<ggml_from_float_tFunction>>;

final class ggml_type_traits extends ffi.Struct {
  external ffi.Pointer<ffi.Char> type_name;

  @ffi.Int64()
  external int blck_size;

  /// interleave elements in blocks
  @ffi.Int64()
  external int blck_size_interleave;

  @ffi.Size()
  external int type_size;

  @ffi.Bool()
  external bool is_quantized;

  external ggml_to_float_t to_float;

  external ggml_from_float_t from_float_ref;
}

/// scheduling priorities
enum ggml_sched_priority {
  GGML_SCHED_PRIO_NORMAL(0),
  GGML_SCHED_PRIO_MEDIUM(1),
  GGML_SCHED_PRIO_HIGH(2),
  GGML_SCHED_PRIO_REALTIME(3);

  final int value;
  const ggml_sched_priority(this.value);

  static ggml_sched_priority fromValue(int value) => switch (value) {
        0 => GGML_SCHED_PRIO_NORMAL,
        1 => GGML_SCHED_PRIO_MEDIUM,
        2 => GGML_SCHED_PRIO_HIGH,
        3 => GGML_SCHED_PRIO_REALTIME,
        _ =>
          throw ArgumentError('Unknown value for ggml_sched_priority: $value'),
      };
}

/// threadpool params
/// Use ggml_threadpool_params_default() or ggml_threadpool_params_init() to populate the defaults
final class ggml_threadpool_params extends ffi.Struct {
  /// mask of cpu cores (all-zeros means use default affinity settings)
  @ffi.Array.multi([512])
  external ffi.Array<ffi.Bool> cpumask;

  /// number of threads
  @ffi.Int()
  external int n_threads;

  /// thread priority
  @ffi.UnsignedInt()
  external int prioAsInt;

  ggml_sched_priority get prio => ggml_sched_priority.fromValue(prioAsInt);

  /// polling level (0 - no polling, 100 - aggressive polling)
  @ffi.Uint32()
  external int poll;

  /// strict cpu placement
  @ffi.Bool()
  external bool strict_cpu;

  /// start in paused state
  @ffi.Bool()
  external bool paused;
}

final class ggml_threadpool extends ffi.Opaque {}

typedef ggml_threadpool_t = ffi.Pointer<ggml_threadpool>;

final class ggml_backend_buffer_type extends ffi.Opaque {}

typedef ggml_backend_buffer_type_t = ffi.Pointer<ggml_backend_buffer_type>;
typedef ggml_backend_buffer_t = ffi.Pointer<ggml_backend_buffer>;

final class ggml_backend extends ffi.Opaque {}

typedef ggml_backend_t = ffi.Pointer<ggml_backend>;

/// Tensor allocator
final class ggml_tallocr extends ffi.Struct {
  external ggml_backend_buffer_t buffer;

  external ffi.Pointer<ffi.Void> base;

  @ffi.Size()
  external int alignment;

  @ffi.Size()
  external int offset;
}

final class ggml_gallocr extends ffi.Opaque {}

/// special tensor flags for use with the graph allocator:
/// ggml_set_input(): all input tensors are allocated at the beginning of the graph in non-overlapping addresses
/// ggml_set_output(): output tensors are never freed and never overwritten
typedef ggml_gallocr_t = ffi.Pointer<ggml_gallocr>;

final class ggml_backend_event extends ffi.Opaque {}

typedef ggml_backend_event_t = ffi.Pointer<ggml_backend_event>;
typedef ggml_backend_graph_plan_t = ffi.Pointer<ffi.Void>;

final class ggml_backend_reg extends ffi.Opaque {}

typedef ggml_backend_reg_t = ffi.Pointer<ggml_backend_reg>;

final class ggml_backend_device extends ffi.Opaque {}

typedef ggml_backend_dev_t = ffi.Pointer<ggml_backend_device>;

/// Backend buffer
enum ggml_backend_buffer_usage {
  GGML_BACKEND_BUFFER_USAGE_ANY(0),
  GGML_BACKEND_BUFFER_USAGE_WEIGHTS(1),
  GGML_BACKEND_BUFFER_USAGE_COMPUTE(2);

  final int value;
  const ggml_backend_buffer_usage(this.value);

  static ggml_backend_buffer_usage fromValue(int value) => switch (value) {
        0 => GGML_BACKEND_BUFFER_USAGE_ANY,
        1 => GGML_BACKEND_BUFFER_USAGE_WEIGHTS,
        2 => GGML_BACKEND_BUFFER_USAGE_COMPUTE,
        _ => throw ArgumentError(
            'Unknown value for ggml_backend_buffer_usage: $value'),
      };
}

/// Backend device
enum ggml_backend_dev_type {
  /// CPU device using system memory
  GGML_BACKEND_DEVICE_TYPE_CPU(0),

  /// GPU device using dedicated memory
  GGML_BACKEND_DEVICE_TYPE_GPU(1),

  /// accelerator devices intended to be used together with the CPU backend (e.g. BLAS or AMX)
  GGML_BACKEND_DEVICE_TYPE_ACCEL(2);

  final int value;
  const ggml_backend_dev_type(this.value);

  static ggml_backend_dev_type fromValue(int value) => switch (value) {
        0 => GGML_BACKEND_DEVICE_TYPE_CPU,
        1 => GGML_BACKEND_DEVICE_TYPE_GPU,
        2 => GGML_BACKEND_DEVICE_TYPE_ACCEL,
        _ => throw ArgumentError(
            'Unknown value for ggml_backend_dev_type: $value'),
      };
}

/// functionality supported by the device
final class ggml_backend_dev_caps extends ffi.Struct {
  /// asynchronous operations
  @ffi.Bool()
  external bool async$;

  /// pinned host buffer
  @ffi.Bool()
  external bool host_buffer;

  /// creating buffers from host ptr
  @ffi.Bool()
  external bool buffer_from_host_ptr;

  /// event synchronization
  @ffi.Bool()
  external bool events;
}

/// all the device properties
final class ggml_backend_dev_props extends ffi.Struct {
  external ffi.Pointer<ffi.Char> name;

  external ffi.Pointer<ffi.Char> description;

  @ffi.Size()
  external int memory_free;

  @ffi.Size()
  external int memory_total;

  @ffi.UnsignedInt()
  external int typeAsInt;

  ggml_backend_dev_type get type => ggml_backend_dev_type.fromValue(typeAsInt);

  external ggml_backend_dev_caps caps;
}

typedef ggml_backend_split_buffer_type_tFunction = ggml_backend_buffer_type_t
    Function(ffi.Int main_device, ffi.Pointer<ffi.Float> tensor_split);
typedef Dartggml_backend_split_buffer_type_tFunction
    = ggml_backend_buffer_type_t Function(
        int main_device, ffi.Pointer<ffi.Float> tensor_split);

/// Split buffer type for tensor parallelism
typedef ggml_backend_split_buffer_type_t
    = ffi.Pointer<ffi.NativeFunction<ggml_backend_split_buffer_type_tFunction>>;
typedef ggml_backend_set_n_threads_tFunction = ffi.Void Function(
    ggml_backend_t backend, ffi.Int n_threads);
typedef Dartggml_backend_set_n_threads_tFunction = void Function(
    ggml_backend_t backend, int n_threads);

/// Set the number of threads for the backend
typedef ggml_backend_set_n_threads_t
    = ffi.Pointer<ffi.NativeFunction<ggml_backend_set_n_threads_tFunction>>;
typedef ggml_backend_dev_get_extra_bufts_tFunction
    = ffi.Pointer<ggml_backend_buffer_type_t> Function(
        ggml_backend_dev_t device);

/// Get additional buffer types provided by the device (returns a NULL-terminated array)
typedef ggml_backend_dev_get_extra_bufts_t = ffi
    .Pointer<ffi.NativeFunction<ggml_backend_dev_get_extra_bufts_tFunction>>;
typedef ggml_backend_set_abort_callback_tFunction = ffi.Void Function(
    ggml_backend_t backend,
    ggml_abort_callback abort_callback,
    ffi.Pointer<ffi.Void> abort_callback_data);
typedef Dartggml_backend_set_abort_callback_tFunction = void Function(
    ggml_backend_t backend,
    ggml_abort_callback abort_callback,
    ffi.Pointer<ffi.Void> abort_callback_data);

/// Set the abort callback for the backend
typedef ggml_backend_set_abort_callback_t = ffi
    .Pointer<ffi.NativeFunction<ggml_backend_set_abort_callback_tFunction>>;

/// Get a list of feature flags supported by the backend (returns a NULL-terminated array)
final class ggml_backend_feature extends ffi.Struct {
  external ffi.Pointer<ffi.Char> name;

  external ffi.Pointer<ffi.Char> value;
}

typedef ggml_backend_get_features_tFunction = ffi.Pointer<ggml_backend_feature>
    Function(ggml_backend_reg_t reg);
typedef ggml_backend_get_features_t
    = ffi.Pointer<ffi.NativeFunction<ggml_backend_get_features_tFunction>>;

final class ggml_backend_sched extends ffi.Opaque {}

/// The backend scheduler allows for multiple backend devices to be used together
/// Handles compute buffer allocation, assignment of tensors to backends, and copying of tensors between backends
/// The backends are selected based on:
/// - the backend that supports the operation
/// - the location of the pre-allocated tensors (e.g. the weights)
///     /*
///       Example usage:
///
/// operations that use tensors allocated in a buffer with USAGE_WEIGHTS will be assigned
/// preferrably to run on the same backend as the buffer
///         ggml_backend_buffer_set_usage(buf_weights, GGML_BACKEND_BUFFER_USAGE_WEIGHTS);
///
///         sched = ggml_backend_sched_new({backend_gpu, backend_gpu2, backend_cpu}, NULL, num_backends, GGML_DEFAULT_GRAPH_SIZE, false);
///
/// initialize buffers from a max size graph (optional)
///         reserve_graph = build_graph(sched, max_batch_size);
///
/// manually assign nodes to a backend (optional, should not be needed in most cases)
///         struct ggml_tensor * node = ggml_mul_mat(ctx, ...);
///         ggml_backend_sched_set_tensor_backend(sched, node, backend_gpu);
///
///         ggml_backend_sched_reserve(sched, reserve_graph);
///
/// compute
///         graph = build_graph(sched); // the graph and its tensors are single-use in terms of allocation, multi-use in terms of computation
///         for (int i = 0; i < 10; ++i) {
///             ggml_backend_sched_graph_compute(sched, graph); // on the first iteration the graph is allocated automatically
///         }
///
/// if there are graph inputs:
///         graph = build_graph(sched); // get a new graph that is not allocated (the metadata for the old graph is freed once ggml_free is called)
///         ggml_backend_sched_reset(sched); // clear the allocation of the previous graph
///         ggml_backend_sched_alloc_graph(sched, graph); // explicitly allocate the new graph but do not execute it
///         ggml_backend_tensor_set(input_tensor, ...); // copy data to the newly allocated graph tensors
///         ggml_backend_sched_graph_compute(sched, graph); // execute the graph
///
/// as an alternative to the above it is also possible to assign the inputs to a dedicated context and
/// allocate them statically via ggml_backend_alloc_ctx_tensors
///     }
///     */
typedef ggml_backend_sched_t = ffi.Pointer<ggml_backend_sched>;
typedef ggml_backend_sched_eval_callbackFunction = ffi.Bool Function(
    ffi.Pointer<ggml_tensor> t, ffi.Bool ask, ffi.Pointer<ffi.Void> user_data);
typedef Dartggml_backend_sched_eval_callbackFunction = bool Function(
    ffi.Pointer<ggml_tensor> t, bool ask, ffi.Pointer<ffi.Void> user_data);

/// Evaluation callback for each node in the graph (set with ggml_backend_sched_set_eval_callback)
/// when ask == true, the scheduler wants to know if the user wants to observe this node
/// this allows the scheduler to batch nodes together in order to evaluate them in a single call
///
/// when ask == false, the scheduler is passing the node tensor to the user for observation
/// if the user returns false, the scheduler will cancel the graph compute
typedef ggml_backend_sched_eval_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_backend_sched_eval_callbackFunction>>;

/// Utils
final class ggml_backend_graph_copy extends ffi.Struct {
  external ggml_backend_buffer_t buffer;

  external ffi.Pointer<ggml_context> ctx_allocated;

  external ffi.Pointer<ggml_context> ctx_unallocated;

  external ffi.Pointer<ggml_cgraph> graph;
}

typedef ggml_backend_eval_callbackFunction = ffi.Bool Function(
    ffi.Int node_index,
    ffi.Pointer<ggml_tensor> t1,
    ffi.Pointer<ggml_tensor> t2,
    ffi.Pointer<ffi.Void> user_data);
typedef Dartggml_backend_eval_callbackFunction = bool Function(
    int node_index,
    ffi.Pointer<ggml_tensor> t1,
    ffi.Pointer<ggml_tensor> t2,
    ffi.Pointer<ffi.Void> user_data);
typedef ggml_backend_eval_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_backend_eval_callbackFunction>>;

/// the compute plan that needs to be prepared for ggml_graph_compute()
/// since https://github.com/ggml-org/ggml/issues/287
final class ggml_cplan extends ffi.Struct {
  /// size of work buffer, calculated by `ggml_graph_plan()`
  @ffi.Size()
  external int work_size;

  /// work buffer, to be allocated by caller before calling to `ggml_graph_compute()`
  external ffi.Pointer<ffi.Uint8> work_data;

  @ffi.Int()
  external int n_threads;

  external ffi.Pointer<ggml_threadpool> threadpool;

  /// abort ggml_graph_compute when true
  external ggml_abort_callback abort_callback;

  external ffi.Pointer<ffi.Void> abort_callback_data;
}

/// numa strategies
enum ggml_numa_strategy {
  GGML_NUMA_STRATEGY_DISABLED(0),
  GGML_NUMA_STRATEGY_DISTRIBUTE(1),
  GGML_NUMA_STRATEGY_ISOLATE(2),
  GGML_NUMA_STRATEGY_NUMACTL(3),
  GGML_NUMA_STRATEGY_MIRROR(4),
  GGML_NUMA_STRATEGY_COUNT(5);

  final int value;
  const ggml_numa_strategy(this.value);

  static ggml_numa_strategy fromValue(int value) => switch (value) {
        0 => GGML_NUMA_STRATEGY_DISABLED,
        1 => GGML_NUMA_STRATEGY_DISTRIBUTE,
        2 => GGML_NUMA_STRATEGY_ISOLATE,
        3 => GGML_NUMA_STRATEGY_NUMACTL,
        4 => GGML_NUMA_STRATEGY_MIRROR,
        5 => GGML_NUMA_STRATEGY_COUNT,
        _ =>
          throw ArgumentError('Unknown value for ggml_numa_strategy: $value'),
      };
}

typedef ggml_vec_dot_tFunction = ffi.Void Function(
    ffi.Int n,
    ffi.Pointer<ffi.Float> s,
    ffi.Size bs,
    ffi.Pointer<ffi.Void> x,
    ffi.Size bx,
    ffi.Pointer<ffi.Void> y,
    ffi.Size by,
    ffi.Int nrc);
typedef Dartggml_vec_dot_tFunction = void Function(
    int n,
    ffi.Pointer<ffi.Float> s,
    int bs,
    ffi.Pointer<ffi.Void> x,
    int bx,
    ffi.Pointer<ffi.Void> y,
    int by,
    int nrc);

/// Internal types and functions exposed for tests and benchmarks
typedef ggml_vec_dot_t
    = ffi.Pointer<ffi.NativeFunction<ggml_vec_dot_tFunction>>;

final class ggml_type_traits_cpu extends ffi.Struct {
  external ggml_from_float_t from_float;

  external ggml_vec_dot_t vec_dot;

  @ffi.UnsignedInt()
  external int vec_dot_typeAsInt;

  ggml_type get vec_dot_type => ggml_type.fromValue(vec_dot_typeAsInt);

  /// number of rows to process simultaneously
  @ffi.Int64()
  external int nrows;
}

/// C interface
///
/// TODO: show sample usage
final class llama_vocab extends ffi.Opaque {}

final class llama_model extends ffi.Opaque {}

final class llama_context extends ffi.Opaque {}

typedef llama_token = ffi.Int32;
typedef Dartllama_token = int;

/// TODO: simplify (https://github.com/ggml-org/llama.cpp/pull/9294#pullrequestreview-2286561979)
final class llama_token_data extends ffi.Struct {
  /// token id
  @llama_token()
  external int id;

  /// log-odds of the token
  @ffi.Float()
  external double logit;

  /// probability of the token
  @ffi.Float()
  external double p;
}

final class llama_token_data_array extends ffi.Struct {
  /// TODO: consider SoA
  /// NOTE: this pointer can be modified by the samplers
  external ffi.Pointer<llama_token_data> data;

  @ffi.Size()
  external int size;

  /// this is the index in the data array (i.e. not the token id)
  @ffi.Int64()
  external int selected;

  @ffi.Bool()
  external bool sorted;
}

/// user code can implement the interface below in order to create custom llama_sampler
final class llama_sampler_i extends ffi.Struct {
  /// can be NULL
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_sampler> smpl)>>
      name;

  /// can be NULL
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_sampler> smpl, llama_token token)>> accept;

  /// required
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_sampler> smpl,
              ffi.Pointer<llama_token_data_array> cur_p)>> apply;

  /// can be NULL
  external ffi.Pointer<
          ffi
          .NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler> smpl)>>
      reset;

  /// can be NULL if ctx is NULL
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Pointer<llama_sampler> smpl)>> clone;

  /// can be NULL if ctx is NULL
  external ffi.Pointer<
      ffi
      .NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler> smpl)>> free;
}

/// Sampling API
///
/// Sample usage:
///
/// // prepare the sampling chain at the start
/// auto sparams = llama_sampler_chain_default_params();
///
/// llama_sampler * smpl = llama_sampler_chain_init(sparams);
///
/// llama_sampler_chain_add(smpl, llama_sampler_init_top_k(50));
/// llama_sampler_chain_add(smpl, llama_sampler_init_top_p(0.9, 1));
/// llama_sampler_chain_add(smpl, llama_sampler_init_temp (0.8));
///
/// // typically, the chain should end with a sampler such as "greedy", "dist" or "mirostat"
/// // this sampler will be responsible to select the actual token
/// llama_sampler_chain_add(smpl, llama_sampler_init_dist(seed));
///
/// ...
///
/// // decoding loop:
/// while (...) {
/// ...
///
/// llama_decode(ctx, batch);
///
/// // sample from the logits of the last token in the batch
/// const llama_token id = llama_sampler_sample(smpl, ctx, -1);
///
/// // accepting the token updates the internal state of certain samplers (e.g. grammar, repetition, etc.)
/// llama_sampler_accept(smpl, id);
/// ...
/// }
///
/// llama_sampler_free(smpl);
///
/// TODO: In the future, llama_sampler will be utilized to offload the sampling to the backends (e.g. GPU).
typedef llama_sampler_context_t = ffi.Pointer<ffi.Void>;

final class llama_sampler extends ffi.Struct {
  external ffi.Pointer<llama_sampler_i> iface;

  external llama_sampler_context_t ctx;
}

typedef llama_pos = ffi.Int32;
typedef Dartllama_pos = int;
typedef llama_seq_id = ffi.Int32;
typedef Dartllama_seq_id = int;

enum llama_vocab_type {
  /// For models without vocab
  LLAMA_VOCAB_TYPE_NONE(0),

  /// LLaMA tokenizer based on byte-level BPE with byte fallback
  LLAMA_VOCAB_TYPE_SPM(1),

  /// GPT-2 tokenizer based on byte-level BPE
  LLAMA_VOCAB_TYPE_BPE(2),

  /// BERT tokenizer based on WordPiece
  LLAMA_VOCAB_TYPE_WPM(3),

  /// T5 tokenizer based on Unigram
  LLAMA_VOCAB_TYPE_UGM(4),

  /// RWKV tokenizer based on greedy tokenization
  LLAMA_VOCAB_TYPE_RWKV(5);

  final int value;
  const llama_vocab_type(this.value);

  static llama_vocab_type fromValue(int value) => switch (value) {
        0 => LLAMA_VOCAB_TYPE_NONE,
        1 => LLAMA_VOCAB_TYPE_SPM,
        2 => LLAMA_VOCAB_TYPE_BPE,
        3 => LLAMA_VOCAB_TYPE_WPM,
        4 => LLAMA_VOCAB_TYPE_UGM,
        5 => LLAMA_VOCAB_TYPE_RWKV,
        _ => throw ArgumentError('Unknown value for llama_vocab_type: $value'),
      };
}

/// pre-tokenization types
enum llama_vocab_pre_type {
  LLAMA_VOCAB_PRE_TYPE_DEFAULT(0),
  LLAMA_VOCAB_PRE_TYPE_LLAMA3(1),
  LLAMA_VOCAB_PRE_TYPE_DEEPSEEK_LLM(2),
  LLAMA_VOCAB_PRE_TYPE_DEEPSEEK_CODER(3),
  LLAMA_VOCAB_PRE_TYPE_FALCON(4),
  LLAMA_VOCAB_PRE_TYPE_MPT(5),
  LLAMA_VOCAB_PRE_TYPE_STARCODER(6),
  LLAMA_VOCAB_PRE_TYPE_GPT2(7),
  LLAMA_VOCAB_PRE_TYPE_REFACT(8),
  LLAMA_VOCAB_PRE_TYPE_COMMAND_R(9),
  LLAMA_VOCAB_PRE_TYPE_STABLELM2(10),
  LLAMA_VOCAB_PRE_TYPE_QWEN2(11),
  LLAMA_VOCAB_PRE_TYPE_OLMO(12),
  LLAMA_VOCAB_PRE_TYPE_DBRX(13),
  LLAMA_VOCAB_PRE_TYPE_SMAUG(14),
  LLAMA_VOCAB_PRE_TYPE_PORO(15),
  LLAMA_VOCAB_PRE_TYPE_CHATGLM3(16),
  LLAMA_VOCAB_PRE_TYPE_CHATGLM4(17),
  LLAMA_VOCAB_PRE_TYPE_VIKING(18),
  LLAMA_VOCAB_PRE_TYPE_JAIS(19),
  LLAMA_VOCAB_PRE_TYPE_TEKKEN(20),
  LLAMA_VOCAB_PRE_TYPE_SMOLLM(21),
  LLAMA_VOCAB_PRE_TYPE_CODESHELL(22),
  LLAMA_VOCAB_PRE_TYPE_BLOOM(23),
  LLAMA_VOCAB_PRE_TYPE_GPT3_FINNISH(24),
  LLAMA_VOCAB_PRE_TYPE_EXAONE(25),
  LLAMA_VOCAB_PRE_TYPE_CHAMELEON(26),
  LLAMA_VOCAB_PRE_TYPE_MINERVA(27),
  LLAMA_VOCAB_PRE_TYPE_DEEPSEEK3_LLM(28),
  LLAMA_VOCAB_PRE_TYPE_GPT4O(29);

  final int value;
  const llama_vocab_pre_type(this.value);

  static llama_vocab_pre_type fromValue(int value) => switch (value) {
        0 => LLAMA_VOCAB_PRE_TYPE_DEFAULT,
        1 => LLAMA_VOCAB_PRE_TYPE_LLAMA3,
        2 => LLAMA_VOCAB_PRE_TYPE_DEEPSEEK_LLM,
        3 => LLAMA_VOCAB_PRE_TYPE_DEEPSEEK_CODER,
        4 => LLAMA_VOCAB_PRE_TYPE_FALCON,
        5 => LLAMA_VOCAB_PRE_TYPE_MPT,
        6 => LLAMA_VOCAB_PRE_TYPE_STARCODER,
        7 => LLAMA_VOCAB_PRE_TYPE_GPT2,
        8 => LLAMA_VOCAB_PRE_TYPE_REFACT,
        9 => LLAMA_VOCAB_PRE_TYPE_COMMAND_R,
        10 => LLAMA_VOCAB_PRE_TYPE_STABLELM2,
        11 => LLAMA_VOCAB_PRE_TYPE_QWEN2,
        12 => LLAMA_VOCAB_PRE_TYPE_OLMO,
        13 => LLAMA_VOCAB_PRE_TYPE_DBRX,
        14 => LLAMA_VOCAB_PRE_TYPE_SMAUG,
        15 => LLAMA_VOCAB_PRE_TYPE_PORO,
        16 => LLAMA_VOCAB_PRE_TYPE_CHATGLM3,
        17 => LLAMA_VOCAB_PRE_TYPE_CHATGLM4,
        18 => LLAMA_VOCAB_PRE_TYPE_VIKING,
        19 => LLAMA_VOCAB_PRE_TYPE_JAIS,
        20 => LLAMA_VOCAB_PRE_TYPE_TEKKEN,
        21 => LLAMA_VOCAB_PRE_TYPE_SMOLLM,
        22 => LLAMA_VOCAB_PRE_TYPE_CODESHELL,
        23 => LLAMA_VOCAB_PRE_TYPE_BLOOM,
        24 => LLAMA_VOCAB_PRE_TYPE_GPT3_FINNISH,
        25 => LLAMA_VOCAB_PRE_TYPE_EXAONE,
        26 => LLAMA_VOCAB_PRE_TYPE_CHAMELEON,
        27 => LLAMA_VOCAB_PRE_TYPE_MINERVA,
        28 => LLAMA_VOCAB_PRE_TYPE_DEEPSEEK3_LLM,
        29 => LLAMA_VOCAB_PRE_TYPE_GPT4O,
        _ =>
          throw ArgumentError('Unknown value for llama_vocab_pre_type: $value'),
      };
}

enum llama_rope_type {
  LLAMA_ROPE_TYPE_NONE(-1),
  LLAMA_ROPE_TYPE_NORM(0),
  LLAMA_ROPE_TYPE_NEOX(2),
  LLAMA_ROPE_TYPE_MROPE(8),
  LLAMA_ROPE_TYPE_VISION(24);

  final int value;
  const llama_rope_type(this.value);

  static llama_rope_type fromValue(int value) => switch (value) {
        -1 => LLAMA_ROPE_TYPE_NONE,
        0 => LLAMA_ROPE_TYPE_NORM,
        2 => LLAMA_ROPE_TYPE_NEOX,
        8 => LLAMA_ROPE_TYPE_MROPE,
        24 => LLAMA_ROPE_TYPE_VISION,
        _ => throw ArgumentError('Unknown value for llama_rope_type: $value'),
      };
}

enum llama_token_type {
  LLAMA_TOKEN_TYPE_UNDEFINED(0),
  LLAMA_TOKEN_TYPE_NORMAL(1),
  LLAMA_TOKEN_TYPE_UNKNOWN(2),
  LLAMA_TOKEN_TYPE_CONTROL(3),
  LLAMA_TOKEN_TYPE_USER_DEFINED(4),
  LLAMA_TOKEN_TYPE_UNUSED(5),
  LLAMA_TOKEN_TYPE_BYTE(6);

  final int value;
  const llama_token_type(this.value);

  static llama_token_type fromValue(int value) => switch (value) {
        0 => LLAMA_TOKEN_TYPE_UNDEFINED,
        1 => LLAMA_TOKEN_TYPE_NORMAL,
        2 => LLAMA_TOKEN_TYPE_UNKNOWN,
        3 => LLAMA_TOKEN_TYPE_CONTROL,
        4 => LLAMA_TOKEN_TYPE_USER_DEFINED,
        5 => LLAMA_TOKEN_TYPE_UNUSED,
        6 => LLAMA_TOKEN_TYPE_BYTE,
        _ => throw ArgumentError('Unknown value for llama_token_type: $value'),
      };
}

enum llama_token_attr {
  LLAMA_TOKEN_ATTR_UNDEFINED(0),
  LLAMA_TOKEN_ATTR_UNKNOWN(1),
  LLAMA_TOKEN_ATTR_UNUSED(2),
  LLAMA_TOKEN_ATTR_NORMAL(4),

  /// SPECIAL?
  LLAMA_TOKEN_ATTR_CONTROL(8),
  LLAMA_TOKEN_ATTR_USER_DEFINED(16),
  LLAMA_TOKEN_ATTR_BYTE(32),
  LLAMA_TOKEN_ATTR_NORMALIZED(64),
  LLAMA_TOKEN_ATTR_LSTRIP(128),
  LLAMA_TOKEN_ATTR_RSTRIP(256),
  LLAMA_TOKEN_ATTR_SINGLE_WORD(512);

  final int value;
  const llama_token_attr(this.value);

  static llama_token_attr fromValue(int value) => switch (value) {
        0 => LLAMA_TOKEN_ATTR_UNDEFINED,
        1 => LLAMA_TOKEN_ATTR_UNKNOWN,
        2 => LLAMA_TOKEN_ATTR_UNUSED,
        4 => LLAMA_TOKEN_ATTR_NORMAL,
        8 => LLAMA_TOKEN_ATTR_CONTROL,
        16 => LLAMA_TOKEN_ATTR_USER_DEFINED,
        32 => LLAMA_TOKEN_ATTR_BYTE,
        64 => LLAMA_TOKEN_ATTR_NORMALIZED,
        128 => LLAMA_TOKEN_ATTR_LSTRIP,
        256 => LLAMA_TOKEN_ATTR_RSTRIP,
        512 => LLAMA_TOKEN_ATTR_SINGLE_WORD,
        _ => throw ArgumentError('Unknown value for llama_token_attr: $value'),
      };
}

/// model file types
enum llama_ftype {
  LLAMA_FTYPE_ALL_F32(0),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_F16(1),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q4_0(2),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q4_1(3),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q8_0(7),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q5_0(8),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q5_1(9),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q2_K(10),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q3_K_S(11),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q3_K_M(12),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q3_K_L(13),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q4_K_S(14),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q4_K_M(15),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q5_K_S(16),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q5_K_M(17),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q6_K(18),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ2_XXS(19),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ2_XS(20),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q2_K_S(21),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ3_XS(22),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ3_XXS(23),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ1_S(24),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ4_NL(25),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ3_S(26),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ3_M(27),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ2_S(28),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ2_M(29),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ4_XS(30),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ1_M(31),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_BF16(32),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_TQ1_0(36),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_TQ2_0(37),

  /// not specified in the model file
  LLAMA_FTYPE_GUESSED(1024);

  final int value;
  const llama_ftype(this.value);

  static llama_ftype fromValue(int value) => switch (value) {
        0 => LLAMA_FTYPE_ALL_F32,
        1 => LLAMA_FTYPE_MOSTLY_F16,
        2 => LLAMA_FTYPE_MOSTLY_Q4_0,
        3 => LLAMA_FTYPE_MOSTLY_Q4_1,
        7 => LLAMA_FTYPE_MOSTLY_Q8_0,
        8 => LLAMA_FTYPE_MOSTLY_Q5_0,
        9 => LLAMA_FTYPE_MOSTLY_Q5_1,
        10 => LLAMA_FTYPE_MOSTLY_Q2_K,
        11 => LLAMA_FTYPE_MOSTLY_Q3_K_S,
        12 => LLAMA_FTYPE_MOSTLY_Q3_K_M,
        13 => LLAMA_FTYPE_MOSTLY_Q3_K_L,
        14 => LLAMA_FTYPE_MOSTLY_Q4_K_S,
        15 => LLAMA_FTYPE_MOSTLY_Q4_K_M,
        16 => LLAMA_FTYPE_MOSTLY_Q5_K_S,
        17 => LLAMA_FTYPE_MOSTLY_Q5_K_M,
        18 => LLAMA_FTYPE_MOSTLY_Q6_K,
        19 => LLAMA_FTYPE_MOSTLY_IQ2_XXS,
        20 => LLAMA_FTYPE_MOSTLY_IQ2_XS,
        21 => LLAMA_FTYPE_MOSTLY_Q2_K_S,
        22 => LLAMA_FTYPE_MOSTLY_IQ3_XS,
        23 => LLAMA_FTYPE_MOSTLY_IQ3_XXS,
        24 => LLAMA_FTYPE_MOSTLY_IQ1_S,
        25 => LLAMA_FTYPE_MOSTLY_IQ4_NL,
        26 => LLAMA_FTYPE_MOSTLY_IQ3_S,
        27 => LLAMA_FTYPE_MOSTLY_IQ3_M,
        28 => LLAMA_FTYPE_MOSTLY_IQ2_S,
        29 => LLAMA_FTYPE_MOSTLY_IQ2_M,
        30 => LLAMA_FTYPE_MOSTLY_IQ4_XS,
        31 => LLAMA_FTYPE_MOSTLY_IQ1_M,
        32 => LLAMA_FTYPE_MOSTLY_BF16,
        36 => LLAMA_FTYPE_MOSTLY_TQ1_0,
        37 => LLAMA_FTYPE_MOSTLY_TQ2_0,
        1024 => LLAMA_FTYPE_GUESSED,
        _ => throw ArgumentError('Unknown value for llama_ftype: $value'),
      };
}

enum llama_rope_scaling_type {
  LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED(-1),
  LLAMA_ROPE_SCALING_TYPE_NONE(0),
  LLAMA_ROPE_SCALING_TYPE_LINEAR(1),
  LLAMA_ROPE_SCALING_TYPE_YARN(2),
  LLAMA_ROPE_SCALING_TYPE_LONGROPE(3);

  static const LLAMA_ROPE_SCALING_TYPE_MAX_VALUE =
      LLAMA_ROPE_SCALING_TYPE_LONGROPE;

  final int value;
  const llama_rope_scaling_type(this.value);

  static llama_rope_scaling_type fromValue(int value) => switch (value) {
        -1 => LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED,
        0 => LLAMA_ROPE_SCALING_TYPE_NONE,
        1 => LLAMA_ROPE_SCALING_TYPE_LINEAR,
        2 => LLAMA_ROPE_SCALING_TYPE_YARN,
        3 => LLAMA_ROPE_SCALING_TYPE_LONGROPE,
        _ => throw ArgumentError(
            'Unknown value for llama_rope_scaling_type: $value'),
      };

  @override
  String toString() {
    if (this == LLAMA_ROPE_SCALING_TYPE_LONGROPE)
      return "llama_rope_scaling_type.LLAMA_ROPE_SCALING_TYPE_LONGROPE, llama_rope_scaling_type.LLAMA_ROPE_SCALING_TYPE_MAX_VALUE";
    return super.toString();
  }
}

enum llama_pooling_type {
  LLAMA_POOLING_TYPE_UNSPECIFIED(-1),
  LLAMA_POOLING_TYPE_NONE(0),
  LLAMA_POOLING_TYPE_MEAN(1),
  LLAMA_POOLING_TYPE_CLS(2),
  LLAMA_POOLING_TYPE_LAST(3),

  /// used by reranking models to attach the classification head to the graph
  LLAMA_POOLING_TYPE_RANK(4);

  final int value;
  const llama_pooling_type(this.value);

  static llama_pooling_type fromValue(int value) => switch (value) {
        -1 => LLAMA_POOLING_TYPE_UNSPECIFIED,
        0 => LLAMA_POOLING_TYPE_NONE,
        1 => LLAMA_POOLING_TYPE_MEAN,
        2 => LLAMA_POOLING_TYPE_CLS,
        3 => LLAMA_POOLING_TYPE_LAST,
        4 => LLAMA_POOLING_TYPE_RANK,
        _ =>
          throw ArgumentError('Unknown value for llama_pooling_type: $value'),
      };
}

enum llama_attention_type {
  LLAMA_ATTENTION_TYPE_UNSPECIFIED(-1),
  LLAMA_ATTENTION_TYPE_CAUSAL(0),
  LLAMA_ATTENTION_TYPE_NON_CAUSAL(1);

  final int value;
  const llama_attention_type(this.value);

  static llama_attention_type fromValue(int value) => switch (value) {
        -1 => LLAMA_ATTENTION_TYPE_UNSPECIFIED,
        0 => LLAMA_ATTENTION_TYPE_CAUSAL,
        1 => LLAMA_ATTENTION_TYPE_NON_CAUSAL,
        _ =>
          throw ArgumentError('Unknown value for llama_attention_type: $value'),
      };
}

enum llama_split_mode {
  /// single GPU
  LLAMA_SPLIT_MODE_NONE(0),

  /// split layers and KV across GPUs
  LLAMA_SPLIT_MODE_LAYER(1),

  /// split layers and KV across GPUs, use tensor parallelism if supported
  LLAMA_SPLIT_MODE_ROW(2);

  final int value;
  const llama_split_mode(this.value);

  static llama_split_mode fromValue(int value) => switch (value) {
        0 => LLAMA_SPLIT_MODE_NONE,
        1 => LLAMA_SPLIT_MODE_LAYER,
        2 => LLAMA_SPLIT_MODE_ROW,
        _ => throw ArgumentError('Unknown value for llama_split_mode: $value'),
      };
}

typedef llama_progress_callbackFunction = ffi.Bool Function(
    ffi.Float progress, ffi.Pointer<ffi.Void> user_data);
typedef Dartllama_progress_callbackFunction = bool Function(
    double progress, ffi.Pointer<ffi.Void> user_data);
typedef llama_progress_callback
    = ffi.Pointer<ffi.NativeFunction<llama_progress_callbackFunction>>;

/// Input data for llama_decode
/// A llama_batch object can contain input about one or many sequences
/// The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens
///
/// - token  : the token ids of the input (used when embd is NULL)
/// - embd   : token embeddings (i.e. float vector of size n_embd) (used when token is NULL)
/// - pos    : the positions of the respective token in the sequence
/// (if set to NULL, the token position will be tracked automatically by llama_decode)
/// - seq_id : the sequence to which the respective token belongs
/// (if set to NULL, the sequence ID will be assumed to be 0)
/// - logits : if zero, the logits (and/or the embeddings) for the respective token will not be output
/// (if set to NULL, only the logits for last token will be returned)
final class llama_batch extends ffi.Struct {
  @ffi.Int32()
  external int n_tokens;

  external ffi.Pointer<llama_token> token;

  external ffi.Pointer<ffi.Float> embd;

  external ffi.Pointer<llama_pos> pos;

  external ffi.Pointer<ffi.Int32> n_seq_id;

  external ffi.Pointer<ffi.Pointer<llama_seq_id>> seq_id;

  /// TODO: rename this to "output"
  external ffi.Pointer<ffi.Int8> logits;
}

enum llama_model_kv_override_type {
  LLAMA_KV_OVERRIDE_TYPE_INT(0),
  LLAMA_KV_OVERRIDE_TYPE_FLOAT(1),
  LLAMA_KV_OVERRIDE_TYPE_BOOL(2),
  LLAMA_KV_OVERRIDE_TYPE_STR(3);

  final int value;
  const llama_model_kv_override_type(this.value);

  static llama_model_kv_override_type fromValue(int value) => switch (value) {
        0 => LLAMA_KV_OVERRIDE_TYPE_INT,
        1 => LLAMA_KV_OVERRIDE_TYPE_FLOAT,
        2 => LLAMA_KV_OVERRIDE_TYPE_BOOL,
        3 => LLAMA_KV_OVERRIDE_TYPE_STR,
        _ => throw ArgumentError(
            'Unknown value for llama_model_kv_override_type: $value'),
      };
}

final class UnnamedUnion1 extends ffi.Union {
  @ffi.Int64()
  external int val_i64;

  @ffi.Double()
  external double val_f64;

  @ffi.Bool()
  external bool val_bool;

  @ffi.Array.multi([128])
  external ffi.Array<ffi.Char> val_str;
}

final class llama_model_kv_override extends ffi.Struct {
  @ffi.UnsignedInt()
  external int tagAsInt;

  llama_model_kv_override_type get tag =>
      llama_model_kv_override_type.fromValue(tagAsInt);

  @ffi.Array.multi([128])
  external ffi.Array<ffi.Char> key;

  external UnnamedUnion1 unnamed;
}

final class llama_model_params extends ffi.Struct {
  /// NULL-terminated list of devices to use for offloading (if NULL, all available devices are used)
  external ffi.Pointer<ggml_backend_dev_t> devices;

  /// number of layers to store in VRAM
  @ffi.Int32()
  external int n_gpu_layers;

  /// how to split the model across multiple GPUs
  @ffi.UnsignedInt()
  external int split_modeAsInt;

  llama_split_mode get split_mode =>
      llama_split_mode.fromValue(split_modeAsInt);

  /// the GPU that is used for the entire model when split_mode is LLAMA_SPLIT_MODE_NONE
  @ffi.Int32()
  external int main_gpu;

  /// proportion of the model (layers or rows) to offload to each GPU, size: llama_max_devices()
  external ffi.Pointer<ffi.Float> tensor_split;

  /// Called with a progress value between 0.0 and 1.0. Pass NULL to disable.
  /// If the provided progress_callback returns true, model loading continues.
  /// If it returns false, model loading is immediately aborted.
  external llama_progress_callback progress_callback;

  /// context pointer passed to the progress callback
  external ffi.Pointer<ffi.Void> progress_callback_user_data;

  /// override key-value pairs of the model meta data
  external ffi.Pointer<llama_model_kv_override> kv_overrides;

  /// only load the vocabulary, no weights
  @ffi.Bool()
  external bool vocab_only;

  /// use mmap if possible
  @ffi.Bool()
  external bool use_mmap;

  /// force system to keep model in RAM
  @ffi.Bool()
  external bool use_mlock;

  /// validate model tensor data
  @ffi.Bool()
  external bool check_tensors;
}

/// NOTE: changing the default values of parameters marked as [EXPERIMENTAL] may cause crashes or incorrect results in certain configurations
/// https://github.com/ggml-org/llama.cpp/pull/7544
final class llama_context_params extends ffi.Struct {
  /// text context, 0 = from model
  @ffi.Uint32()
  external int n_ctx;

  /// logical maximum batch size that can be submitted to llama_decode
  @ffi.Uint32()
  external int n_batch;

  /// physical maximum batch size
  @ffi.Uint32()
  external int n_ubatch;

  /// max number of sequences (i.e. distinct states for recurrent models)
  @ffi.Uint32()
  external int n_seq_max;

  /// number of threads to use for generation
  @ffi.Int32()
  external int n_threads;

  /// number of threads to use for batch processing
  @ffi.Int32()
  external int n_threads_batch;

  /// RoPE scaling type, from `enum llama_rope_scaling_type`
  @ffi.Int()
  external int rope_scaling_typeAsInt;

  llama_rope_scaling_type get rope_scaling_type =>
      llama_rope_scaling_type.fromValue(rope_scaling_typeAsInt);

  /// whether to pool (sum) embedding results by sequence id
  @ffi.Int()
  external int pooling_typeAsInt;

  llama_pooling_type get pooling_type =>
      llama_pooling_type.fromValue(pooling_typeAsInt);

  /// attention type to use for embeddings
  @ffi.Int()
  external int attention_typeAsInt;

  llama_attention_type get attention_type =>
      llama_attention_type.fromValue(attention_typeAsInt);

  /// RoPE base frequency, 0 = from model
  @ffi.Float()
  external double rope_freq_base;

  /// RoPE frequency scaling factor, 0 = from model
  @ffi.Float()
  external double rope_freq_scale;

  /// YaRN extrapolation mix factor, negative = from model
  @ffi.Float()
  external double yarn_ext_factor;

  /// YaRN magnitude scaling factor
  @ffi.Float()
  external double yarn_attn_factor;

  /// YaRN low correction dim
  @ffi.Float()
  external double yarn_beta_fast;

  /// YaRN high correction dim
  @ffi.Float()
  external double yarn_beta_slow;

  /// YaRN original context size
  @ffi.Uint32()
  external int yarn_orig_ctx;

  /// defragment the KV cache if holes/size > thold, < 0 disabled (default)
  @ffi.Float()
  external double defrag_thold;

  external ggml_backend_sched_eval_callback cb_eval;

  external ffi.Pointer<ffi.Void> cb_eval_user_data;

  /// data type for K cache [EXPERIMENTAL]
  @ffi.UnsignedInt()
  external int type_kAsInt;

  ggml_type get type_k => ggml_type.fromValue(type_kAsInt);

  /// data type for V cache [EXPERIMENTAL]
  @ffi.UnsignedInt()
  external int type_vAsInt;

  ggml_type get type_v => ggml_type.fromValue(type_vAsInt);

  /// the llama_decode() call computes all logits, not just the last one (DEPRECATED - set llama_batch.logits instead)
  @ffi.Bool()
  external bool logits_all;

  /// if true, extract embeddings (together with logits)
  @ffi.Bool()
  external bool embeddings;

  /// whether to offload the KQV ops (including the KV cache) to GPU
  @ffi.Bool()
  external bool offload_kqv;

  /// whether to use flash attention [EXPERIMENTAL]
  @ffi.Bool()
  external bool flash_attn;

  /// whether to measure performance timings
  @ffi.Bool()
  external bool no_perf;

  /// Abort callback
  /// if it returns true, execution of llama_decode() will be aborted
  /// currently works only with CPU execution
  external ggml_abort_callback abort_callback;

  external ffi.Pointer<ffi.Void> abort_callback_data;
}

/// model quantization parameters
final class llama_model_quantize_params extends ffi.Struct {
  /// number of threads to use for quantizing, if <=0 will use std::thread::hardware_concurrency()
  @ffi.Int32()
  external int nthread;

  /// quantize to this llama_ftype
  @ffi.UnsignedInt()
  external int ftypeAsInt;

  llama_ftype get ftype => llama_ftype.fromValue(ftypeAsInt);

  /// output tensor type
  @ffi.UnsignedInt()
  external int output_tensor_typeAsInt;

  ggml_type get output_tensor_type =>
      ggml_type.fromValue(output_tensor_typeAsInt);

  /// token embeddings tensor type
  @ffi.UnsignedInt()
  external int token_embedding_typeAsInt;

  ggml_type get token_embedding_type =>
      ggml_type.fromValue(token_embedding_typeAsInt);

  /// allow quantizing non-f32/f16 tensors
  @ffi.Bool()
  external bool allow_requantize;

  /// quantize output.weight
  @ffi.Bool()
  external bool quantize_output_tensor;

  /// only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored
  @ffi.Bool()
  external bool only_copy;

  /// quantize all tensors to the default type
  @ffi.Bool()
  external bool pure;

  /// quantize to the same number of shards
  @ffi.Bool()
  external bool keep_split;

  /// pointer to importance matrix data
  external ffi.Pointer<ffi.Void> imatrix;

  /// pointer to vector containing overrides
  external ffi.Pointer<ffi.Void> kv_overrides;
}

final class llama_logit_bias extends ffi.Struct {
  @llama_token()
  external int token;

  @ffi.Float()
  external double bias;
}

final class llama_sampler_chain_params extends ffi.Struct {
  /// whether to measure performance timings
  @ffi.Bool()
  external bool no_perf;
}

/// used in chat template
final class llama_chat_message extends ffi.Struct {
  external ffi.Pointer<ffi.Char> role;

  external ffi.Pointer<ffi.Char> content;
}

/// lora adapter
final class llama_adapter_lora extends ffi.Opaque {}

/// Information associated with an individual cell in the KV cache view.
final class llama_kv_cache_view_cell extends ffi.Struct {
  /// The position for this cell. Takes KV cache shifts into account.
  /// May be negative if the cell is not populated.
  @llama_pos()
  external int pos;
}

/// An updateable view of the KV cache.
final class llama_kv_cache_view extends ffi.Struct {
  /// Number of KV cache cells. This will be the same as the context size.
  @ffi.Int32()
  external int n_cells;

  /// Maximum number of sequences that can exist in a cell. It's not an error
  /// if there are more sequences in a cell than this value, however they will
  /// not be visible in the view cells_sequences.
  @ffi.Int32()
  external int n_seq_max;

  /// Number of tokens in the cache. For example, if there are two populated
  /// cells, the first with 1 sequence id in it and the second with 2 sequence
  /// ids then you'll have 3 tokens.
  @ffi.Int32()
  external int token_count;

  /// Number of populated cache cells.
  @ffi.Int32()
  external int used_cells;

  /// Maximum contiguous empty slots in the cache.
  @ffi.Int32()
  external int max_contiguous;

  /// Index to the start of the max_contiguous slot range. Can be negative
  /// when cache is full.
  @ffi.Int32()
  external int max_contiguous_idx;

  /// Information for an individual cell.
  external ffi.Pointer<llama_kv_cache_view_cell> cells;

  /// The sequences for each cell. There will be n_seq_max items per cell.
  external ffi.Pointer<llama_seq_id> cells_sequences;
}

/// Performance utils
///
/// NOTE: Used by llama.cpp examples, avoid using in third-party apps. Instead, do your own performance measurements.
final class llama_perf_context_data extends ffi.Struct {
  @ffi.Double()
  external double t_start_ms;

  @ffi.Double()
  external double t_load_ms;

  @ffi.Double()
  external double t_p_eval_ms;

  @ffi.Double()
  external double t_eval_ms;

  @ffi.Int32()
  external int n_p_eval;

  @ffi.Int32()
  external int n_eval;
}

final class llama_perf_sampler_data extends ffi.Struct {
  @ffi.Double()
  external double t_sample_ms;

  @ffi.Int32()
  external int n_sample;
}

final class common_chat_msg extends ffi.Opaque {}

typedef common_chat_msg_t = common_chat_msg;

final class lcpp_data_pvalue extends ffi.Struct {
  external ffi.Pointer<ffi.Char> value;

  @ffi.Int32()
  external int length;

  @ffi.Bool()
  external bool found;
}

typedef lcpp_data_pvalue_t = lcpp_data_pvalue;

enum lcpp_mirostat_type {
  /// disabled,
  LCPP_MIROSTAT_NONE(0),

  /// mirostat 1.0
  LCPP_MIROSTAT_V1(1),

  /// mirostat 2.0
  LCPP_MIROSTAT_V2(2);

  final int value;
  const lcpp_mirostat_type(this.value);

  static lcpp_mirostat_type fromValue(int value) => switch (value) {
        0 => LCPP_MIROSTAT_NONE,
        1 => LCPP_MIROSTAT_V1,
        2 => LCPP_MIROSTAT_V2,
        _ =>
          throw ArgumentError('Unknown value for lcpp_mirostat_type: $value'),
      };
}

enum lcpp_model_family {
  LCPP_MODEL_FAMILY_LLAMA(0),
  LCPP_MODEL_FAMILY_QWEN(1),
  LCPP_MODEL_FAMILY_PHI(2),
  LCPP_MODEL_FAMILY_GEMMA(3),
  LCPP_MODEL_FAMILY_GRANITE(4),
  LCPP_MODEL_FAMILY_DEEPSEEK(5),
  LCPP_MODEL_FAMILY_MISTRAL(6),
  LCPP_MODEL_FAMILY_COUNT(7),
  LCPP_MODEL_FAMILY_UNSPECIFIED(30),
  LCPP_MODEL_FAMILY_UNKNOWN(31);

  final int value;
  const lcpp_model_family(this.value);

  static lcpp_model_family fromValue(int value) => switch (value) {
        0 => LCPP_MODEL_FAMILY_LLAMA,
        1 => LCPP_MODEL_FAMILY_QWEN,
        2 => LCPP_MODEL_FAMILY_PHI,
        3 => LCPP_MODEL_FAMILY_GEMMA,
        4 => LCPP_MODEL_FAMILY_GRANITE,
        5 => LCPP_MODEL_FAMILY_DEEPSEEK,
        6 => LCPP_MODEL_FAMILY_MISTRAL,
        7 => LCPP_MODEL_FAMILY_COUNT,
        30 => LCPP_MODEL_FAMILY_UNSPECIFIED,
        31 => LCPP_MODEL_FAMILY_UNKNOWN,
        _ => throw ArgumentError('Unknown value for lcpp_model_family: $value'),
      };
}

/// from common.h
enum lcpp_common_sampler_type {
  LCPP_COMMON_SAMPLER_TYPE_NONE(0),
  LCPP_COMMON_SAMPLER_TYPE_DRY(1),
  LCPP_COMMON_SAMPLER_TYPE_TOP_K(2),
  LCPP_COMMON_SAMPLER_TYPE_TOP_P(3),
  LCPP_COMMON_SAMPLER_TYPE_MIN_P(4),

  /// LCPP_COMMON_SAMPLER_TYPE_TFS_Z       = 5,
  LCPP_COMMON_SAMPLER_TYPE_TYPICAL_P(6),
  LCPP_COMMON_SAMPLER_TYPE_TEMPERATURE(7),
  LCPP_COMMON_SAMPLER_TYPE_XTC(8),
  LCPP_COMMON_SAMPLER_TYPE_INFILL(9),
  LCPP_COMMON_SAMPLER_TYPE_PENALTIES(10);

  final int value;
  const lcpp_common_sampler_type(this.value);

  static lcpp_common_sampler_type fromValue(int value) => switch (value) {
        0 => LCPP_COMMON_SAMPLER_TYPE_NONE,
        1 => LCPP_COMMON_SAMPLER_TYPE_DRY,
        2 => LCPP_COMMON_SAMPLER_TYPE_TOP_K,
        3 => LCPP_COMMON_SAMPLER_TYPE_TOP_P,
        4 => LCPP_COMMON_SAMPLER_TYPE_MIN_P,
        6 => LCPP_COMMON_SAMPLER_TYPE_TYPICAL_P,
        7 => LCPP_COMMON_SAMPLER_TYPE_TEMPERATURE,
        8 => LCPP_COMMON_SAMPLER_TYPE_XTC,
        9 => LCPP_COMMON_SAMPLER_TYPE_INFILL,
        10 => LCPP_COMMON_SAMPLER_TYPE_PENALTIES,
        _ => throw ArgumentError(
            'Unknown value for lcpp_common_sampler_type: $value'),
      };
}

/// sampling parameters
final class lcpp_params extends ffi.Struct {
  /// <= 0.0 to sample greedily, 0.0 to not output probabilities
  @ffi.Float()
  external double temp;

  /// 0.0 = disabled
  @ffi.Float()
  external double dynatemp_range;

  /// controls how entropy maps to temperature in dynamic temperature sampler
  @ffi.Float()
  external double dynatemp_exponent;

  /// 1.0 = disabled
  @ffi.Float()
  external double top_p;

  /// 0.0 = disabled
  @ffi.Float()
  external double min_p;

  /// 0.0 = disabled
  @ffi.Float()
  external double xtc_probability;

  /// > 0.5 disables XTC
  @ffi.Float()
  external double xtc_threshold;

  /// typical_p, 1.0 = disabled
  @ffi.Float()
  external double typ_p;

  /// 1.0 = disabled
  @ffi.Float()
  external double penalty_repeat;

  /// 0.0 = disabled
  @ffi.Float()
  external double penalty_freq;

  /// 0.0 = disabled
  @ffi.Float()
  external double penalty_present;

  /// 0.0 = disabled;      DRY repetition penalty for tokens extending repetition:
  @ffi.Float()
  external double dry_multiplier;

  /// 0.0 = disabled;      multiplier * base ^ (length of sequence before token - allowed length)
  @ffi.Float()
  external double dry_base;

  /// -1.0 = disabled
  @ffi.Float()
  external double top_n_sigma;

  /// target entropy
  @ffi.Float()
  external double mirostat_tau;

  /// learning rate
  @ffi.Float()
  external double mirostat_eta;

  /// the seed used to initialize llama_sampler
  @ffi.Uint32()
  external int seed;

  /// number of previous tokens to remember
  @ffi.Int32()
  external int n_prev;

  /// if greater than 0, output the probabilities of top n_probs tokens.
  @ffi.Int32()
  external int n_probs;

  /// 0 = disabled, otherwise samplers should return at least min_keep tokens
  @ffi.Int32()
  external int min_keep;

  /// <= 0 to use vocab size
  @ffi.Int32()
  external int top_k;

  /// last n tokens to penalize (0 = disable penalty, -1 = context size)
  @ffi.Int32()
  external int penalty_last_n;

  /// tokens extending repetitions beyond this receive penalty
  @ffi.Int32()
  external int dry_allowed_length;

  /// how many tokens to scan for repetitions (0 = disable penalty, -1 = context size)
  @ffi.Int32()
  external int dry_penalty_last_n;

  @ffi.Int32()
  external int n_dry_sequence_breakers;

  @ffi.Int32()
  external int n_samplers;

  @ffi.Int32()
  external int n_grammar_length;

  @ffi.Int32()
  external int n_model_path_length;

  /// 0 = disabled, 1 = mirostat, 2 = mirostat 2.0
  @ffi.Uint8()
  external int mirostatAsInt;

  lcpp_mirostat_type get mirostat =>
      lcpp_mirostat_type.fromValue(mirostatAsInt);

  /// model family e.g. deepseek phi
  @ffi.Uint8()
  external int model_familyAsInt;

  lcpp_model_family get model_family =>
      lcpp_model_family.fromValue(model_familyAsInt);

  @ffi.Bool()
  external bool ignore_eos;

  /// disable performance metrics
  @ffi.Bool()
  external bool no_perf;

  @ffi.Bool()
  external bool timing_per_token;

  @ffi.Bool()
  external bool grammar_lazy;

  /// default sequence breakers for DRY
  external ffi.Pointer<ffi.Pointer<ffi.Char>> dry_sequence_breakers;

  external ffi.Pointer<ffi.Uint8> samplers;

  /// optional BNF-like grammar to constrain sampling
  external ffi.Pointer<ffi.Char> grammar;

  /// path to GGUF model file
  external ffi.Pointer<ffi.Char> model_path;
}

/// sampling parameters
typedef lcpp_params_t = lcpp_params;

final class lcpp_common_chat_tool_call extends ffi.Struct {
  external ffi.Pointer<ffi.Char> name;

  external ffi.Pointer<ffi.Char> arguments;

  external ffi.Pointer<ffi.Char> id;

  @ffi.Uint32()
  external int n_name;

  @ffi.Uint32()
  external int n_arguments;

  @ffi.Uint32()
  external int n_id;
}

typedef lcpp_common_chat_tool_call_t = lcpp_common_chat_tool_call;

final class lcpp_common_chat_msg_content_part extends ffi.Struct {
  external ffi.Pointer<ffi.Char> type;

  external ffi.Pointer<ffi.Char> text;

  @ffi.Uint32()
  external int n_type;

  @ffi.Uint32()
  external int n_text;
}

typedef lcpp_common_chat_msg_content_part_t = lcpp_common_chat_msg_content_part;

final class lcpp_common_chat_msg extends ffi.Struct {
  external ffi.Pointer<ffi.Char> role;

  external ffi.Pointer<ffi.Char> content;

  @ffi.Uint32()
  external int n_role;

  @ffi.Uint32()
  external int n_content;

  external ffi.Pointer<ffi.Pointer<lcpp_common_chat_msg_content_part_t>>
      content_parts;

  @ffi.Int32()
  external int n_content_parts;

  external ffi.Pointer<ffi.Pointer<lcpp_common_chat_tool_call_t>> tool_calls;

  @ffi.Int32()
  external int n_tool_calls;

  external ffi.Pointer<ffi.Char> reasoning_content;

  @ffi.Uint32()
  external int n_reasoning_content;

  external ffi.Pointer<ffi.Char> tool_name;

  @ffi.Uint32()
  external int n_tool_name;

  external ffi.Pointer<ffi.Char> tool_call_id;

  @ffi.Uint32()
  external int n_tool_call_id;
}

typedef lcpp_common_chat_msg_t = lcpp_common_chat_msg;
typedef llama_model_params_t = llama_model_params;
typedef llama_context_params_t = llama_context_params;
typedef LppTokenStreamCallbackFunction = ffi.Void Function(
    ffi.Pointer<ffi.Char>, ffi.Int);
typedef DartLppTokenStreamCallbackFunction = void Function(
    ffi.Pointer<ffi.Char>, int);
typedef LppTokenStreamCallback
    = ffi.Pointer<ffi.NativeFunction<LppTokenStreamCallbackFunction>>;
typedef LppChatMessageCallbackFunction = ffi.Void Function(
    ffi.Pointer<lcpp_common_chat_msg_t>);
typedef DartLppChatMessageCallbackFunction = void Function(
    ffi.Pointer<lcpp_common_chat_msg_t>);
typedef LppChatMessageCallback
    = ffi.Pointer<ffi.NativeFunction<LppChatMessageCallbackFunction>>;

const int _VCRT_COMPILER_PREPROCESSOR = 1;

const int _SAL_VERSION = 20;

const int __SAL_H_VERSION = 180000000;

const int _USE_DECLSPECS_FOR_SAL = 0;

const int _USE_ATTRIBUTES_FOR_SAL = 0;

const int _CRT_PACKING = 8;

const int _VCRUNTIME_DISABLED_WARNINGS = 4514;

const int _HAS_EXCEPTIONS = 1;

const int _WCHAR_T_DEFINED = 1;

const int NULL = 0;

const int _HAS_CXX17 = 0;

const int _HAS_CXX20 = 0;

const int _HAS_CXX23 = 0;

const int _HAS_CXX26 = 0;

const int _HAS_NODISCARD = 1;

const int INT8_MIN = -128;

const int INT16_MIN = -32768;

const int INT32_MIN = -2147483648;

const int INT64_MIN = -9223372036854775808;

const int INT8_MAX = 127;

const int INT16_MAX = 32767;

const int INT32_MAX = 2147483647;

const int INT64_MAX = 9223372036854775807;

const int UINT8_MAX = 255;

const int UINT16_MAX = 65535;

const int UINT32_MAX = 4294967295;

const int UINT64_MAX = -1;

const int INT_LEAST8_MIN = -128;

const int INT_LEAST16_MIN = -32768;

const int INT_LEAST32_MIN = -2147483648;

const int INT_LEAST64_MIN = -9223372036854775808;

const int INT_LEAST8_MAX = 127;

const int INT_LEAST16_MAX = 32767;

const int INT_LEAST32_MAX = 2147483647;

const int INT_LEAST64_MAX = 9223372036854775807;

const int UINT_LEAST8_MAX = 255;

const int UINT_LEAST16_MAX = 65535;

const int UINT_LEAST32_MAX = 4294967295;

const int UINT_LEAST64_MAX = -1;

const int INT_FAST8_MIN = -128;

const int INT_FAST16_MIN = -2147483648;

const int INT_FAST32_MIN = -2147483648;

const int INT_FAST64_MIN = -9223372036854775808;

const int INT_FAST8_MAX = 127;

const int INT_FAST16_MAX = 2147483647;

const int INT_FAST32_MAX = 2147483647;

const int INT_FAST64_MAX = 9223372036854775807;

const int UINT_FAST8_MAX = 255;

const int UINT_FAST16_MAX = 4294967295;

const int UINT_FAST32_MAX = 4294967295;

const int UINT_FAST64_MAX = -1;

const int INTPTR_MIN = -9223372036854775808;

const int INTPTR_MAX = 9223372036854775807;

const int UINTPTR_MAX = -1;

const int INTMAX_MIN = -9223372036854775808;

const int INTMAX_MAX = 9223372036854775807;

const int UINTMAX_MAX = -1;

const int PTRDIFF_MIN = -9223372036854775808;

const int PTRDIFF_MAX = 9223372036854775807;

const int SIZE_MAX = -1;

const int SIG_ATOMIC_MIN = -2147483648;

const int SIG_ATOMIC_MAX = 2147483647;

const int WCHAR_MIN = 0;

const int WCHAR_MAX = 65535;

const int WINT_MIN = 0;

const int WINT_MAX = 65535;

const int __bool_true_false_are_defined = 1;

const int false$ = 0;

const int true$ = 1;

const int _ARM_WINAPI_PARTITION_DESKTOP_SDK_AVAILABLE = 1;

const int _CRT_BUILD_DESKTOP_APP = 1;

const int _UCRT_DISABLED_WARNINGS = 4324;

const int _ARGMAX = 100;

const int _TRUNCATE = -1;

const int _CRT_INT_MAX = 2147483647;

const int _CRT_SIZE_MAX = -1;

const String __FILEW__ = 'C';

const int _CRT_FUNCTIONS_REQUIRED = 1;

const int _CRT_HAS_CXX17 = 0;

const int _CRT_HAS_C11 = 0;

const int _CRT_INTERNAL_NONSTDC_NAMES = 1;

const int __STDC_SECURE_LIB__ = 200411;

const int __GOT_SECURE_LIB__ = 200411;

const int __STDC_WANT_SECURE_LIB__ = 1;

const int _SECURECRT_FILL_BUFFER_PATTERN = 254;

const int _CRT_SECURE_CPP_OVERLOAD_STANDARD_NAMES = 0;

const int _CRT_SECURE_CPP_OVERLOAD_STANDARD_NAMES_COUNT = 0;

const int _CRT_SECURE_CPP_OVERLOAD_SECURE_NAMES = 1;

const int _CRT_SECURE_CPP_OVERLOAD_STANDARD_NAMES_MEMORY = 0;

const int _CRT_SECURE_CPP_OVERLOAD_SECURE_NAMES_MEMORY = 0;

const String _CRT_INTERNAL_STDIO_SYMBOL_PREFIX = '';

const int _CRT_INTERNAL_PRINTF_LEGACY_VSPRINTF_NULL_TERMINATION = 1;

const int _CRT_INTERNAL_PRINTF_STANDARD_SNPRINTF_BEHAVIOR = 2;

const int _CRT_INTERNAL_PRINTF_LEGACY_WIDE_SPECIFIERS = 4;

const int _CRT_INTERNAL_PRINTF_LEGACY_MSVCRT_COMPATIBILITY = 8;

const int _CRT_INTERNAL_PRINTF_LEGACY_THREE_DIGIT_EXPONENTS = 16;

const int _CRT_INTERNAL_PRINTF_STANDARD_ROUNDING = 32;

const int _CRT_INTERNAL_SCANF_SECURECRT = 1;

const int _CRT_INTERNAL_SCANF_LEGACY_WIDE_SPECIFIERS = 2;

const int _CRT_INTERNAL_SCANF_LEGACY_MSVCRT_COMPATIBILITY = 4;

const int WEOF = 65535;

const int BUFSIZ = 512;

const int _NFILE = 512;

const int _NSTREAM_ = 512;

const int _IOB_ENTRIES = 3;

const int EOF = -1;

const int _IOFBF = 0;

const int _IOLBF = 64;

const int _IONBF = 4;

const int L_tmpnam = 260;

const int L_tmpnam_s = 260;

const int SEEK_CUR = 1;

const int SEEK_END = 2;

const int SEEK_SET = 0;

const int FILENAME_MAX = 260;

const int FOPEN_MAX = 20;

const int _SYS_OPEN = 20;

const int TMP_MAX = 2147483647;

const int TMP_MAX_S = 2147483647;

const int _TMP_MAX_S = 2147483647;

const int SYS_OPEN = 20;

const int GGML_FILE_MAGIC = 1734831468;

const int GGML_FILE_VERSION = 2;

const int GGML_QNT_VERSION = 2;

const int GGML_QNT_VERSION_FACTOR = 1000;

const int GGML_MAX_DIMS = 4;

const int GGML_MAX_PARAMS = 2048;

const int GGML_MAX_SRC = 10;

const int GGML_MAX_N_THREADS = 512;

const int GGML_MAX_OP_PARAMS = 64;

const int GGML_MAX_NAME = 64;

const int GGML_DEFAULT_N_THREADS = 4;

const int GGML_DEFAULT_GRAPH_SIZE = 2048;

const int GGML_MEM_ALIGN = 16;

const int GGML_EXIT_SUCCESS = 0;

const int GGML_EXIT_ABORTED = 1;

const int GGML_ROPE_TYPE_NEOX = 2;

const int GGML_ROPE_TYPE_MROPE = 8;

const int GGML_ROPE_TYPE_VISION = 24;

const int GGML_KQ_MASK_PAD = 64;

const int GGML_N_TASKS_MAX = -1;

const int LLAMA_DEFAULT_SEED = 4294967295;

const int LLAMA_TOKEN_NULL = -1;

const int LLAMA_FILE_MAGIC_GGLA = 1734831201;

const int LLAMA_FILE_MAGIC_GGSN = 1734833006;

const int LLAMA_FILE_MAGIC_GGSQ = 1734833009;

const int LLAMA_SESSION_MAGIC = 1734833006;

const int LLAMA_SESSION_VERSION = 9;

const int LLAMA_STATE_SEQ_MAGIC = 1734833009;

const int LLAMA_STATE_SEQ_VERSION = 2;
